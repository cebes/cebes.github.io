{
    "docs": [
        {
            "location": "/",
            "text": "Meet Cebes - your new Data Science buddy\n\u00b6\n\n\nCebes' mission is to \nsimplify\n Data Science, improve Data Scientists' productivity, \nhelp them to focus on modeling and understanding, instead of the dreary, trivial work.\n\n\nIt does so by:\n\n\n\n\n\n\nSimplifying interaction with Apache Spark for \ndata processing\n\n    Cebes provides a client with \npandas\n-like APIs for working on big \nDataframes\n, \n    so you don't need to connect directly to the Spark cluster to run Spark jobs.\n\n\n\n\n\n\nProviding unified APIs for processing data and training models in \npipelines\n\n    ETL stages and Machine Learning algorithms can be connected to form flexible \n    and powerful data processing pipelines. Hyper-parameters can be tuned automatically.\n\n\n\n\n\n\nSimplifying \ndeployment and life-cycle management\n of pipelines\n\n    Pipelines can be exported and published to a \npipeline repository\n. They can be taken\n    up from there to be used in serving.\n\n    Cebes serves \npipelines\n, not \nmodels\n, hence all your ETL logic can be carried \n    over to inference time. The serving component is written using modern technology \n    and can be easily customized to fit your setup.\n\n    Good news is you don't need to do it all by yourself. Cebes can do that in a few \n    lines of code!\n\n\n\n\n\n\nBringing \nSpark\n closer to popular Machine Learning libraries like \ntensorflow\n, \n\nkeras\n, \nscikit-learn\n, ...\n\n    Although still being work-in-progress, we plan to support popular Python Machine \n    Learning libraries. Your model written in Python will be able to consume data \n    processed by \nSpark\n. All you need to do is to construct an appropriate Pipeline, \n    Cebes will handle data transfers and other boring work automatically!\n\n\n\n\n\n\nIf that sounds exciting to you, let's check it out by a few examples!\n\n\n\n\nCebes at a glance\n\u00b6\n\n\nOnce \ninstalled\n, Cebes can be used as any other Python library.\nYou first create a Cebes Session to \nconnect to a Cebes Server\n,\nthen load the sample \nCylinder bands\n dataset like so:\n\n\n>>>\n \nimport\n \npycebes\n \nas\n \ncb\n\n\n\n>>>\n \ns\n \n=\n \ncb\n.\nSession\n()\n\n    \n[\nINFO\n]\n \npycebes\n.\ninternal\n.\ndocker_helpers\n:\n \nStarting\n \nCebes\n \ncontainer\n \ncebes\n-\nserver\n-\nx\n.\nx\n.\nx\n-\n0\n[\nphvu\n/\ncebes\n:\nx\n.\nx\n.\nx\n]\n \n        \nwith\n \ndata\n \npath\n \nat\n \n$\nHOME\n/.\ncebes\n/\nx\n.\nx\n.\nx\n\n    \n[\nINFO\n]\n \npycebes\n.\ninternal\n.\ndocker_helpers\n:\n \nCebes\n \ncontainer\n \nstarted\n,\n \nlistening\n \nat\n \nlocalhost\n:\n32768\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nConnecting\n \nto\n \nCebes\n \ncontainer\n \ncebes\n-\nserver\n-\nx\n.\nx\n.\nx\n-\n0\n[\nphvu\n/\ncebes\n:\nx\n.\nx\n.\nx\n]\n \nat\n \nport\n \n32768\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nSpark\n \nUI\n \ncan\n \nbe\n \naccessed\n \nat\n \nhttp\n:\n//\nlocalhost\n:\n32769\n\n\n\n>>>\n \ndf\n \n=\n \ns\n.\nload_test_datasets\n()[\n'cylinder_bands'\n]\n\n\n\n\n\n\nThe schema of the Dataframe, including \ndata types\n of each column, can\nbe accessed with \ndf.schema\n, a summary of the data can be shown with \ndf.show()\n, and the size of the dataset\nis stored in \ndf.shape\n:\n\n\n>>>\n \ndf\n.\nschema\n\n    \nSchema\n(\nfields\n=\n[\nSchemaField\n(\nname\n=\n'timestamp'\n,\nstorage_type\n=\nLONG\n,\nvariable_type\n=\nDISCRETE\n),\n\n                   \nSchemaField\n(\nname\n=\n'cylinder_number'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n),\n\n                   \nSchemaField\n(\nname\n=\n'customer'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n),\n\n                   \nSchemaField\n(\nname\n=\n'job_number'\n,\nstorage_type\n=\nINTEGER\n,\nvariable_type\n=\nDISCRETE\n),\n\n                   \n...\n,\n\n                   \nSchemaField\n(\nname\n=\n'band_type'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n)])\n\n\n\n>>>\n \ndf\n.\nshow\n()\n\n    \nID\n:\n \n6744406\na\n-\n239\na\n-\n4\na4a\n-\na8c7\n-\n9\na422bf4b477\n\n    \nShape\n:\n \n(\n540\n,\n \n40\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \ntimestamp\n \ncylinder_number\n \ncustomer\n  \njob_number\n \ngrain_screened\n \nink_color\n  \\\n    \n0\n   \n19910108\n            \nX126\n  \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n   \n    \n1\n   \n19910109\n            \nX266\n  \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n   \n    \n2\n   \n19910104\n              \nB7\n   \nMODMAT\n       \n47201\n            \nYES\n       \nKEY\n   \n    \n3\n   \n19910104\n            \nT133\n   \nMASSEY\n       \n39039\n            \nYES\n       \nKEY\n   \n    \n4\n   \n19910111\n             \nJ34\n    \nKMART\n       \n37351\n             \nNO\n       \nKEY\n   \n\n      \nproof_on_ctd_ink\n \nblade_mfg\n \ncylinder_division\n \npaper_type\n    \n...\n      \\\n    \n0\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n    \n...\n       \n    \n1\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n    \n...\n       \n    \n2\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n    \n...\n       \n    \n3\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n    \n...\n       \n    \n4\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n    \n...\n       \n\n      \nsolvent_pct\n \nesa_voltage\n \nesa_amperage\n  \nwax\n \nhardener\n  \nroller_durometer\n  \\\n    \n0\n   \n36.400002\n         \n0.0\n          \n0.0\n  \n2.5\n      \n1.0\n                \n34\n   \n    \n1\n   \n38.500000\n         \n0.0\n          \n0.0\n  \n2.5\n      \n0.7\n                \n34\n   \n    \n2\n   \n39.799999\n         \n0.0\n          \n0.0\n  \n2.8\n      \n0.9\n                \n40\n   \n    \n3\n   \n38.799999\n         \n0.0\n          \n0.0\n  \n2.5\n      \n1.3\n                \n40\n   \n    \n4\n   \n42.500000\n         \n5.0\n          \n0.0\n  \n2.3\n      \n0.6\n                \n35\n   \n\n       \ncurrent_density\n \nanode_space_ratio\n \nchrome_content\n  \nband_type\n  \n    \n0\n               \n40\n        \n105.000000\n          \n100.0\n       \nband\n  \n    \n1\n               \n40\n        \n105.000000\n          \n100.0\n     \nnoband\n  \n    \n2\n               \n40\n        \n103.870003\n          \n100.0\n     \nnoband\n  \n    \n3\n               \n40\n        \n108.059998\n          \n100.0\n     \nnoband\n  \n    \n4\n               \n40\n        \n106.669998\n          \n100.0\n     \nnoband\n  \n\n    \n[\n5\n \nrows\n \nx\n \n40\n \ncolumns\n]\n\n\n\n>>>\n \ndf\n.\nshape\n\n    \n(\n540\n,\n \n40\n)\n\n\n\n\n\n\nThis is a dataset of 540 rows and 40 columns about cylinder bands.\n\n\nWe can already perform some non-trivial \nSQL query\n on this dataset.\nFor example, we can count the number of transactions recorded every week:\n\n\n# convert the `timestamp` column from \"yyyyMMdd\" into Unix timestamp, and cast it to type TIMESTAMP\n\n\n>>>\n \ndf2\n \n=\n \ndf\n.\nwith_column\n(\n'timestamp_unix'\n,\n \n\n...\n:\n    \ncb\n.\nunix_timestamp\n(\ndf\n.\ntimestamp\n.\ncast\n(\ncb\n.\nStorageTypes\n.\nSTRING\n),\n \npattern\n=\n'yyyyMMdd'\n)\n.\n\\\n\n...\n:\n    \ncast\n(\ncb\n.\nStorageTypes\n.\nTIMESTAMP\n))\n\n\n\n\n>>>\n \ndf2\n.\nshow\n()\n\n    \nID\n:\n \nb03c92b0\n-\nf97a\n-\n4\nd21\n-\n94e5\n-\n1\nb039fc2c038\n\n    \nShape\n:\n \n(\n540\n,\n \n41\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \ntimestamp\n \ncylinder_number\n \ncustomer\n  \njob_number\n \ngrain_screened\n \nink_color\n  \\\n    \n0\n   \n19910108\n            \nX126\n  \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n   \n    \n1\n   \n19910109\n            \nX266\n  \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n   \n    \n2\n   \n19910104\n              \nB7\n   \nMODMAT\n       \n47201\n            \nYES\n       \nKEY\n   \n    \n3\n   \n19910104\n            \nT133\n   \nMASSEY\n       \n39039\n            \nYES\n       \nKEY\n   \n    \n4\n   \n19910111\n             \nJ34\n    \nKMART\n       \n37351\n             \nNO\n       \nKEY\n   \n\n      \nproof_on_ctd_ink\n \nblade_mfg\n \ncylinder_division\n \npaper_type\n       \n...\n        \\\n    \n0\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n    \n1\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n    \n2\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n    \n3\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n    \n4\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n\n      \nesa_voltage\n \nesa_amperage\n  \nwax\n \nhardener\n \nroller_durometer\n  \ncurrent_density\n  \\\n    \n0\n         \n0.0\n          \n0.0\n  \n2.5\n      \n1.0\n               \n34\n               \n40\n   \n    \n1\n         \n0.0\n          \n0.0\n  \n2.5\n      \n0.7\n               \n34\n               \n40\n   \n    \n2\n         \n0.0\n          \n0.0\n  \n2.8\n      \n0.9\n               \n40\n               \n40\n   \n    \n3\n         \n0.0\n          \n0.0\n  \n2.5\n      \n1.3\n               \n40\n               \n40\n   \n    \n4\n         \n5.0\n          \n0.0\n  \n2.3\n      \n0.6\n               \n35\n               \n40\n   \n\n       \nanode_space_ratio\n \nchrome_content\n \nband_type\n  \ntimestamp_unix\n  \n    \n0\n         \n105.000000\n          \n100.0\n      \nband\n       \n663292800\n  \n    \n1\n         \n105.000000\n          \n100.0\n    \nnoband\n       \n663379200\n  \n    \n2\n         \n103.870003\n          \n100.0\n    \nnoband\n       \n662947200\n  \n    \n3\n         \n108.059998\n          \n100.0\n    \nnoband\n       \n662947200\n  \n    \n4\n         \n106.669998\n          \n100.0\n    \nnoband\n       \n663552000\n  \n\n    \n[\n5\n \nrows\n \nx\n \n41\n \ncolumns\n]\n\n\n\n>>>\n \ndf3\n \n=\n \ndf2\n.\ngroupby\n(\ncb\n.\nwindow\n(\ndf2\n.\ntimestamp_unix\n,\n \n'7 days'\n,\n \n'7 days'\n))\n.\ncount\n()\n\n\n\n>>>\n \ndf3\n.\nschema\n\n    \nSchema\n(\nfields\n=\n[\nSchemaField\n(\nname\n=\n'window'\n,\nstorage_type\n=\nStruct\n[\nTIMESTAMP\n,\nTIMESTAMP\n],\nvariable_type\n=\nSTRUCT\n),\n\n                   \nSchemaField\n(\nname\n=\n'count'\n,\nstorage_type\n=\nLONG\n,\nvariable_type\n=\nDISCRETE\n)])\n\n\n\n>>>\n \ndf3\n.\nshow\n()\n\n    \nID\n:\n \n22\nc45afe\n-\n1488\n-\n4562\n-\nac4b\n-\n5\nd4767e8a6da\n\n    \nShape\n:\n \n(\n122\n,\n \n2\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n                                           \nwindow\n  \ncount\n\n    \n0\n  \n{\n'end'\n:\n \n676166400.0\n,\n \n'start'\n:\n \n675561600.0\n}\n      \n8\n\n    \n1\n  \n{\n'end'\n:\n \n651974400.0\n,\n \n'start'\n:\n \n651369600.0\n}\n      \n4\n\n    \n2\n  \n{\n'end'\n:\n \n701568000.0\n,\n \n'start'\n:\n \n700963200.0\n}\n      \n1\n\n    \n3\n  \n{\n'end'\n:\n \n707011200.0\n,\n \n'start'\n:\n \n706406400.0\n}\n      \n1\n\n    \n4\n  \n{\n'end'\n:\n \n714873600.0\n,\n \n'start'\n:\n \n714268800.0\n}\n      \n2\n\n\n\n>>>\n \ndf4\n \n=\n \ndf3\n.\nwith_column\n(\n'start'\n,\n \ncb\n.\ndate_format\n(\ndf3\n[\n'window.start'\n],\n \n'dd/MM/yyyy'\n))\n.\n\\\n    \nwith_column\n(\n'end'\n,\n \ncb\n.\ndate_format\n(\ndf3\n[\n'window.end'\n],\n \n'dd/MM/yyyy'\n))\n.\n\\\n    \nwith_column\n(\n'window_start'\n,\n \ndf3\n[\n'window.start'\n])\n.\nsort\n(\n'window_start'\n)\n\n\n\n>>>\n \ndf4\n \n=\n \ndf4\n.\ndrop\n(\ndf4\n.\nwindow_start\n)\n\n\n\n>>>\n \ndf4\n.\nshow\n()\n\n   \nID\n:\n \n85\nf03b43\n-\n7731\n-\n487\na\n-\na01c\n-\ne5d7ae488c4d\n\n   \nShape\n:\n \n(\n122\n,\n \n4\n)\n\n   \nSample\n \n5\n \nrows\n:\n\n                                          \nwindow\n  \ncount\n       \nstart\n         \nend\n\n   \n0\n  \n{\n'end'\n:\n \n639273600.0\n,\n \n'start'\n:\n \n638668800.0\n}\n      \n1\n  \n29\n/\n03\n/\n1990\n  \n05\n/\n04\n/\n1990\n\n   \n1\n  \n{\n'end'\n:\n \n639878400.0\n,\n \n'start'\n:\n \n639273600.0\n}\n      \n2\n  \n05\n/\n04\n/\n1990\n  \n12\n/\n04\n/\n1990\n\n   \n2\n  \n{\n'end'\n:\n \n640483200.0\n,\n \n'start'\n:\n \n639878400.0\n}\n      \n5\n  \n12\n/\n04\n/\n1990\n  \n19\n/\n04\n/\n1990\n\n   \n3\n  \n{\n'end'\n:\n \n641088000.0\n,\n \n'start'\n:\n \n640483200.0\n}\n      \n1\n  \n19\n/\n04\n/\n1990\n  \n26\n/\n04\n/\n1990\n\n   \n4\n  \n{\n'end'\n:\n \n641692800.0\n,\n \n'start'\n:\n \n641088000.0\n}\n      \n2\n  \n26\n/\n04\n/\n1990\n  \n03\n/\n05\n/\n1990\n\n\n\n\n\n\nSee \nthis section\n for more information on Cebes Dataframe APIs,\n\ne.g.\n how to \nload data into Cebes\n, \n\ntag Dataframes\n so that you can reuse them, ...\n\n\nQuery and preprocess data is important, but \nMachine Learning\n is also a focus of Cebes.\nMachine Learning is done using \nPipelines\n. For example, a simple \nPipeline for training a Linear Regression can be done as follows:\n\n\n>>>\n \ndf2\n \n=\n \ndf\n.\ndrop\n(\n*\n(\nset\n(\ndf\n.\ncolumns\n)\n \n-\n \n{\n'viscosity'\n,\n \n'proof_cut'\n,\n \n'caliper'\n}))\n\n\n>>>\n \ndf2\n \n=\n \ndf2\n.\ndropna\n(\ncolumns\n=\n[\n'viscosity'\n,\n \n'proof_cut'\n,\n \n'caliper'\n])\n\n\n\n>>>\n \ndf2\n.\nshow\n()\n\n    \nID\n:\n \n25\na139f8\n-\n4\na27\n-\n442\nf\n-\n8\na95\n-\nd8b0cd528c75\n\n    \nShape\n:\n \n(\n466\n,\n \n3\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \nproof_cut\n  \nviscosity\n  \ncaliper\n\n    \n0\n       \n55.0\n         \n46\n    \n0.200\n\n    \n1\n       \n55.0\n         \n46\n    \n0.300\n\n    \n2\n       \n62.0\n         \n40\n    \n0.433\n\n    \n3\n       \n52.0\n         \n40\n    \n0.300\n\n    \n4\n       \n50.0\n         \n46\n    \n0.300\n\n\n\n>>>\n \nwith\n \ncb\n.\nPipeline\n()\n \nas\n \nppl\n:\n\n\n...\n:\n     \ninp\n \n=\n \ncb\n.\nplaceholder\n(\ncb\n.\nPlaceholderTypes\n.\nDATAFRAME\n)\n\n\n...\n:\n     \nassembler\n \n=\n \ncb\n.\nvector_assembler\n(\ninp\n,\n \ninput_cols\n=\n[\n'viscosity'\n,\n \n'proof_cut'\n],\n \n\n...\n:\n                                     \noutput_col\n=\n'features'\n)\n\n\n...\n:\n     \nlr\n \n=\n \ncb\n.\nlinear_regression\n(\nassembler\n.\noutput_df\n,\n \n\n...\n:\n                               \nfeatures_col\n=\n'features'\n,\n \nlabel_col\n=\n'caliper'\n,\n\n\n...\n:\n                               \nprediction_col\n=\n'caliper_predict'\n,\n \n\n...\n:\n                               \nreg_param\n=\n0.0\n)\n\n\n...\n:\n                                      \n\n\n>>>\n \npredicted_df\n,\n \nlr_model\n,\n \nassembled_df\n \n=\n \nppl\n.\nrun\n(\n\n\n...\n:\n    \n[\nlr\n.\noutput_df\n,\n \nlr\n.\nmodel\n,\n \nassembler\n.\noutput_df\n],\n \nfeeds\n=\n{\ninp\n:\n \ndf2\n})\n\n\n\n>>>\n \npredicted_df\n.\nshow\n()\n\n    \nID\n:\n \n294\nab9a2\n-\n0\nb84\n-\n4\ncf9\n-\n97\nb4\n-\nde4cd4be9901\n\n    \nShape\n:\n \n(\n466\n,\n \n5\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \nproof_cut\n  \nviscosity\n  \ncaliper\n      \nfeatures\n  \ncaliper_predict\n\n    \n0\n       \n55.0\n         \n46\n    \n0.200\n  \n[\n46.0\n,\n \n55.0\n]\n         \n0.273678\n\n    \n1\n       \n55.0\n         \n46\n    \n0.300\n  \n[\n46.0\n,\n \n55.0\n]\n         \n0.273678\n\n    \n2\n       \n62.0\n         \n40\n    \n0.433\n  \n[\n40.0\n,\n \n62.0\n]\n         \n0.266757\n\n    \n3\n       \n52.0\n         \n40\n    \n0.300\n  \n[\n40.0\n,\n \n52.0\n]\n         \n0.261072\n\n    \n4\n       \n50.0\n         \n46\n    \n0.300\n  \n[\n46.0\n,\n \n50.0\n]\n         \n0.270836\n\n\n\n\n\n\nPipelines can then be stored and tagged in the same way as Dataframes. They can also be\npublished to \nPipeline repositories\n and \nserved\n for online \nprediction.\n\n\n\n\nGetting support\n\u00b6\n\n\nCreate an issue on github if you find bugs or have a feature request. Join us on\n\ngitter\n to interact with the team. Alternatively, if you \nprefer the good old way, drop a message to our \nmailing list\n.\n\n\n\n\nWhy this name, Cebes?\n\u00b6\n\n\nCebes\n (\nc.\n 430 \u2013 350 BC) was an Ancient \nGreek philosopher from Thebes, remembered as a disciple of Socrates. He is one of \nthe speakers in the \nPhaedo\n of Plato, in which he is represented as an earnest seeker \nafter virtue and truth, keen in argument and cautious in decision. \n\n\nCebes\n is with you on your journey making faster and better data-driven decisions!",
            "title": "Overview"
        },
        {
            "location": "/#meet-cebes-your-new-data-science-buddy",
            "text": "Cebes' mission is to  simplify  Data Science, improve Data Scientists' productivity, \nhelp them to focus on modeling and understanding, instead of the dreary, trivial work.  It does so by:    Simplifying interaction with Apache Spark for  data processing \n    Cebes provides a client with  pandas -like APIs for working on big  Dataframes , \n    so you don't need to connect directly to the Spark cluster to run Spark jobs.    Providing unified APIs for processing data and training models in  pipelines \n    ETL stages and Machine Learning algorithms can be connected to form flexible \n    and powerful data processing pipelines. Hyper-parameters can be tuned automatically.    Simplifying  deployment and life-cycle management  of pipelines \n    Pipelines can be exported and published to a  pipeline repository . They can be taken\n    up from there to be used in serving. \n    Cebes serves  pipelines , not  models , hence all your ETL logic can be carried \n    over to inference time. The serving component is written using modern technology \n    and can be easily customized to fit your setup. \n    Good news is you don't need to do it all by yourself. Cebes can do that in a few \n    lines of code!    Bringing  Spark  closer to popular Machine Learning libraries like  tensorflow ,  keras ,  scikit-learn , ... \n    Although still being work-in-progress, we plan to support popular Python Machine \n    Learning libraries. Your model written in Python will be able to consume data \n    processed by  Spark . All you need to do is to construct an appropriate Pipeline, \n    Cebes will handle data transfers and other boring work automatically!    If that sounds exciting to you, let's check it out by a few examples!",
            "title": "Meet Cebes - your new Data Science buddy"
        },
        {
            "location": "/#cebes-at-a-glance",
            "text": "Once  installed , Cebes can be used as any other Python library.\nYou first create a Cebes Session to  connect to a Cebes Server ,\nthen load the sample  Cylinder bands  dataset like so:  >>>   import   pycebes   as   cb  >>>   s   =   cb . Session () \n     [ INFO ]   pycebes . internal . docker_helpers :   Starting   Cebes   container   cebes - server - x . x . x - 0 [ phvu / cebes : x . x . x ]  \n         with   data   path   at   $ HOME /. cebes / x . x . x \n     [ INFO ]   pycebes . internal . docker_helpers :   Cebes   container   started ,   listening   at   localhost : 32768 \n     [ INFO ]   pycebes . core . session :   Connecting   to   Cebes   container   cebes - server - x . x . x - 0 [ phvu / cebes : x . x . x ]   at   port   32768 \n     [ INFO ]   pycebes . core . session :   Spark   UI   can   be   accessed   at   http : // localhost : 32769  >>>   df   =   s . load_test_datasets ()[ 'cylinder_bands' ]   The schema of the Dataframe, including  data types  of each column, can\nbe accessed with  df.schema , a summary of the data can be shown with  df.show() , and the size of the dataset\nis stored in  df.shape :  >>>   df . schema \n     Schema ( fields = [ SchemaField ( name = 'timestamp' , storage_type = LONG , variable_type = DISCRETE ), \n                    SchemaField ( name = 'cylinder_number' , storage_type = STRING , variable_type = TEXT ), \n                    SchemaField ( name = 'customer' , storage_type = STRING , variable_type = TEXT ), \n                    SchemaField ( name = 'job_number' , storage_type = INTEGER , variable_type = DISCRETE ), \n                    ... , \n                    SchemaField ( name = 'band_type' , storage_type = STRING , variable_type = TEXT )])  >>>   df . show () \n     ID :   6744406 a - 239 a - 4 a4a - a8c7 - 9 a422bf4b477 \n     Shape :   ( 540 ,   40 ) \n     Sample   5   rows : \n        timestamp   cylinder_number   customer    job_number   grain_screened   ink_color   \\\n     0     19910108              X126    TVGUIDE         25503              YES         KEY    \n     1     19910109              X266    TVGUIDE         25503              YES         KEY    \n     2     19910104                B7     MODMAT         47201              YES         KEY    \n     3     19910104              T133     MASSEY         39039              YES         KEY    \n     4     19910111               J34      KMART         37351               NO         KEY    \n\n       proof_on_ctd_ink   blade_mfg   cylinder_division   paper_type      ...       \\\n     0                YES      BENTON            GALLATIN     UNCOATED      ...        \n     1                YES      BENTON            GALLATIN     UNCOATED      ...        \n     2                YES      BENTON            GALLATIN     UNCOATED      ...        \n     3                YES      BENTON            GALLATIN     UNCOATED      ...        \n     4                YES      BENTON            GALLATIN     UNCOATED      ...        \n\n       solvent_pct   esa_voltage   esa_amperage    wax   hardener    roller_durometer   \\\n     0     36.400002           0.0            0.0    2.5        1.0                  34    \n     1     38.500000           0.0            0.0    2.5        0.7                  34    \n     2     39.799999           0.0            0.0    2.8        0.9                  40    \n     3     38.799999           0.0            0.0    2.5        1.3                  40    \n     4     42.500000           5.0            0.0    2.3        0.6                  35    \n\n        current_density   anode_space_ratio   chrome_content    band_type   \n     0                 40          105.000000            100.0         band   \n     1                 40          105.000000            100.0       noband   \n     2                 40          103.870003            100.0       noband   \n     3                 40          108.059998            100.0       noband   \n     4                 40          106.669998            100.0       noband   \n\n     [ 5   rows   x   40   columns ]  >>>   df . shape \n     ( 540 ,   40 )   This is a dataset of 540 rows and 40 columns about cylinder bands.  We can already perform some non-trivial  SQL query  on this dataset.\nFor example, we can count the number of transactions recorded every week:  # convert the `timestamp` column from \"yyyyMMdd\" into Unix timestamp, and cast it to type TIMESTAMP  >>>   df2   =   df . with_column ( 'timestamp_unix' ,   ... :      cb . unix_timestamp ( df . timestamp . cast ( cb . StorageTypes . STRING ),   pattern = 'yyyyMMdd' ) . \\ ... :      cast ( cb . StorageTypes . TIMESTAMP ))  >>>   df2 . show () \n     ID :   b03c92b0 - f97a - 4 d21 - 94e5 - 1 b039fc2c038 \n     Shape :   ( 540 ,   41 ) \n     Sample   5   rows : \n        timestamp   cylinder_number   customer    job_number   grain_screened   ink_color   \\\n     0     19910108              X126    TVGUIDE         25503              YES         KEY    \n     1     19910109              X266    TVGUIDE         25503              YES         KEY    \n     2     19910104                B7     MODMAT         47201              YES         KEY    \n     3     19910104              T133     MASSEY         39039              YES         KEY    \n     4     19910111               J34      KMART         37351               NO         KEY    \n\n       proof_on_ctd_ink   blade_mfg   cylinder_division   paper_type         ...         \\\n     0                YES      BENTON            GALLATIN     UNCOATED         ...          \n     1                YES      BENTON            GALLATIN     UNCOATED         ...          \n     2                YES      BENTON            GALLATIN     UNCOATED         ...          \n     3                YES      BENTON            GALLATIN     UNCOATED         ...          \n     4                YES      BENTON            GALLATIN     UNCOATED         ...          \n\n       esa_voltage   esa_amperage    wax   hardener   roller_durometer    current_density   \\\n     0           0.0            0.0    2.5        1.0                 34                 40    \n     1           0.0            0.0    2.5        0.7                 34                 40    \n     2           0.0            0.0    2.8        0.9                 40                 40    \n     3           0.0            0.0    2.5        1.3                 40                 40    \n     4           5.0            0.0    2.3        0.6                 35                 40    \n\n        anode_space_ratio   chrome_content   band_type    timestamp_unix   \n     0           105.000000            100.0        band         663292800   \n     1           105.000000            100.0      noband         663379200   \n     2           103.870003            100.0      noband         662947200   \n     3           108.059998            100.0      noband         662947200   \n     4           106.669998            100.0      noband         663552000   \n\n     [ 5   rows   x   41   columns ]  >>>   df3   =   df2 . groupby ( cb . window ( df2 . timestamp_unix ,   '7 days' ,   '7 days' )) . count ()  >>>   df3 . schema \n     Schema ( fields = [ SchemaField ( name = 'window' , storage_type = Struct [ TIMESTAMP , TIMESTAMP ], variable_type = STRUCT ), \n                    SchemaField ( name = 'count' , storage_type = LONG , variable_type = DISCRETE )])  >>>   df3 . show () \n     ID :   22 c45afe - 1488 - 4562 - ac4b - 5 d4767e8a6da \n     Shape :   ( 122 ,   2 ) \n     Sample   5   rows : \n                                            window    count \n     0    { 'end' :   676166400.0 ,   'start' :   675561600.0 }        8 \n     1    { 'end' :   651974400.0 ,   'start' :   651369600.0 }        4 \n     2    { 'end' :   701568000.0 ,   'start' :   700963200.0 }        1 \n     3    { 'end' :   707011200.0 ,   'start' :   706406400.0 }        1 \n     4    { 'end' :   714873600.0 ,   'start' :   714268800.0 }        2  >>>   df4   =   df3 . with_column ( 'start' ,   cb . date_format ( df3 [ 'window.start' ],   'dd/MM/yyyy' )) . \\\n     with_column ( 'end' ,   cb . date_format ( df3 [ 'window.end' ],   'dd/MM/yyyy' )) . \\\n     with_column ( 'window_start' ,   df3 [ 'window.start' ]) . sort ( 'window_start' )  >>>   df4   =   df4 . drop ( df4 . window_start )  >>>   df4 . show () \n    ID :   85 f03b43 - 7731 - 487 a - a01c - e5d7ae488c4d \n    Shape :   ( 122 ,   4 ) \n    Sample   5   rows : \n                                           window    count         start           end \n    0    { 'end' :   639273600.0 ,   'start' :   638668800.0 }        1    29 / 03 / 1990    05 / 04 / 1990 \n    1    { 'end' :   639878400.0 ,   'start' :   639273600.0 }        2    05 / 04 / 1990    12 / 04 / 1990 \n    2    { 'end' :   640483200.0 ,   'start' :   639878400.0 }        5    12 / 04 / 1990    19 / 04 / 1990 \n    3    { 'end' :   641088000.0 ,   'start' :   640483200.0 }        1    19 / 04 / 1990    26 / 04 / 1990 \n    4    { 'end' :   641692800.0 ,   'start' :   641088000.0 }        2    26 / 04 / 1990    03 / 05 / 1990   See  this section  for more information on Cebes Dataframe APIs, e.g.  how to  load data into Cebes ,  tag Dataframes  so that you can reuse them, ...  Query and preprocess data is important, but  Machine Learning  is also a focus of Cebes.\nMachine Learning is done using  Pipelines . For example, a simple \nPipeline for training a Linear Regression can be done as follows:  >>>   df2   =   df . drop ( * ( set ( df . columns )   -   { 'viscosity' ,   'proof_cut' ,   'caliper' }))  >>>   df2   =   df2 . dropna ( columns = [ 'viscosity' ,   'proof_cut' ,   'caliper' ])  >>>   df2 . show () \n     ID :   25 a139f8 - 4 a27 - 442 f - 8 a95 - d8b0cd528c75 \n     Shape :   ( 466 ,   3 ) \n     Sample   5   rows : \n        proof_cut    viscosity    caliper \n     0         55.0           46      0.200 \n     1         55.0           46      0.300 \n     2         62.0           40      0.433 \n     3         52.0           40      0.300 \n     4         50.0           46      0.300  >>>   with   cb . Pipeline ()   as   ppl :  ... :       inp   =   cb . placeholder ( cb . PlaceholderTypes . DATAFRAME )  ... :       assembler   =   cb . vector_assembler ( inp ,   input_cols = [ 'viscosity' ,   'proof_cut' ],   ... :                                       output_col = 'features' )  ... :       lr   =   cb . linear_regression ( assembler . output_df ,   ... :                                 features_col = 'features' ,   label_col = 'caliper' ,  ... :                                 prediction_col = 'caliper_predict' ,   ... :                                 reg_param = 0.0 )  ... :                                        >>>   predicted_df ,   lr_model ,   assembled_df   =   ppl . run (  ... :      [ lr . output_df ,   lr . model ,   assembler . output_df ],   feeds = { inp :   df2 })  >>>   predicted_df . show () \n     ID :   294 ab9a2 - 0 b84 - 4 cf9 - 97 b4 - de4cd4be9901 \n     Shape :   ( 466 ,   5 ) \n     Sample   5   rows : \n        proof_cut    viscosity    caliper        features    caliper_predict \n     0         55.0           46      0.200    [ 46.0 ,   55.0 ]           0.273678 \n     1         55.0           46      0.300    [ 46.0 ,   55.0 ]           0.273678 \n     2         62.0           40      0.433    [ 40.0 ,   62.0 ]           0.266757 \n     3         52.0           40      0.300    [ 40.0 ,   52.0 ]           0.261072 \n     4         50.0           46      0.300    [ 46.0 ,   50.0 ]           0.270836   Pipelines can then be stored and tagged in the same way as Dataframes. They can also be\npublished to  Pipeline repositories  and  served  for online \nprediction.",
            "title": "Cebes at a glance"
        },
        {
            "location": "/#getting-support",
            "text": "Create an issue on github if you find bugs or have a feature request. Join us on gitter  to interact with the team. Alternatively, if you \nprefer the good old way, drop a message to our  mailing list .",
            "title": "Getting support"
        },
        {
            "location": "/#why-this-name-cebes",
            "text": "Cebes  ( c.  430 \u2013 350 BC) was an Ancient \nGreek philosopher from Thebes, remembered as a disciple of Socrates. He is one of \nthe speakers in the  Phaedo  of Plato, in which he is represented as an earnest seeker \nafter virtue and truth, keen in argument and cautious in decision.   Cebes  is with you on your journey making faster and better data-driven decisions!",
            "title": "Why this name, Cebes?"
        },
        {
            "location": "/installation/",
            "text": "The main component of Cebes is a server running as a \n\nSpark application\n. As a Cebes user, you use a \nclient to connect to this server and make it perform tasks of your choice. Cebes provides a Python\nclient called \npycebes\n, but other languages could be supported in the future.\n\n\n\n\n\n\nInstall the Python client \npycebes\n\u00b6\n\n\npycebes\n is the Python client that you use to work with \nCebes server\n. \n\n\nHowever, when you install \npycebes\n, \nCebes server\n is also included, so that you can do everything \nas if it was a full-blown Cebes system. You are not forced to use this server though. The client \nworks with any compatible Cebes server.\n\n\n\n\n\n\n(Optional) Install docker for your workstation: \nMac\n, \n    \nUbuntu\n, \n    \nWindows\n, \n    \nothers\n\n\nYou can skip this step if you don't want to have Cebes server running locally. Whatever you choose, you\ncan always connect to a Cebes server running on a separated Spark cluster.\n\n\nIn the future, we might provide playground Cebes servers for you to play with. Stay tuned!\n\n\n\n\n\n\nInstall \npycebes\n:\n\n\n$ pip install pycebes\n\n\n\n\n\n\n\n\n\nYou can test your installation by connecting the client to a server, as outlined below.\n\n\n\n\n\n\nConnect to a Cebes server\n\u00b6\n\n\nUsing the client, you can connect to a Cebes server by creating a new \nSession\n.\n\n\nConnect to a local Cebes server\n\u00b6\n\n\nIf your docker daemon is running, Cebes client can start a new container containing a suitable version of \nCebes server. In a Python interpreter, just create a \nSession\n object without argument:\n\n\n>>>\n \nimport\n \npycebes\n \nas\n \ncb\n\n\n>>>\n \ns\n \n=\n \ncb\n.\nSession\n()\n\n    \n[\nINFO\n]\n \npycebes\n.\ninternal\n.\ndocker_helpers\n:\n \nStarting\n \nCebes\n \ncontainer\n \ncebes\n-\nserver\n-\nx\n.\nxx\n.\nx\n-\n0\n[\nphvu\n/\ncebes\n:\nx\n.\nxx\n.\nx\n]\n \nwith\n \ndata\n \npath\n \nat\n \n$\nHOME\n/.\ncebes\n/\nx\n.\nxx\n.\nx\n\n    \n[\nINFO\n]\n \npycebes\n.\ninternal\n.\ndocker_helpers\n:\n \nCebes\n \ncontainer\n \nstarted\n,\n \nlistening\n \nat\n \nlocalhost\n:\n32770\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nConnecting\n \nto\n \nCebes\n \ncontainer\n \ncebes\n-\nserver\n-\nx\n.\nxx\n.\nx\n-\n0\n[\nphvu\n/\ncebes\n:\nx\n.\nxx\n.\nx\n]\n \nat\n \nport\n \n32770\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nSpark\n \nUI\n \ncan\n \nbe\n \naccessed\n \nat\n \nhttp\n:\n//\nlocalhost\n:\n32771\n\n\n\n\n\n\nIf you haven't installed Docker, or the docker daemon has not started, the above command will fail:\n\n\n>>>\n \nimport\n \npycebes\n \nas\n \ncb\n \n\n>>>\n \ns\n \n=\n \ncb\n.\nSession\n()\n\n    \n...\n\n    \nValueError\n:\n \nCould\n \nnot\n \ncreate\n \ndocker\n \nclient\n:\n \nError\n \nwhile\n \nfetching\n \nserver\n \nAPI\n \nversion\n:\n \n    \n(\n'Connection aborted.'\n,\n \nConnectionRefusedError\n(\n111\n,\n \n'Connection refused'\n))\n.\n \n    \nHave\n \nyou\n \nstarted\n \nthe\n \ndocker\n \ndaemon\n?\n\n\n\n\n\n\n\n\nConnect to a remote Cebes server\n\u00b6\n\n\nIn any case, you can always connect to a Cebes server by specifying a hostname and a port:\n\n\n>>>\n \nimport\n \npycebes\n \nas\n \ncb\n\n\n>>>\n \ns\n \n=\n \ncb\n.\nSession\n(\n'cebes.server.com'\n,\n \n21000\n)\n\n\n\n\n\n\nWith the built-in server, you can already start exploring Cebes. Whenever you feel serious,\nor your datasets get bigger, you can always install Cebes on a Spark cluster, \nas outlined in the next section.\n\n\n\n\nInstall Cebes server on a Spark cluster\n\u00b6\n\n\nTo use Cebes on big datasets, you need a Spark cluster. The recommended setup is you should\nhave a Spark cluster for your team/company, and the Cebes server runs on this cluster. Users \n(e.g. Data Scientists, DevOps engineers) in your team will then use \npycebes\n to connect \nto the Cebes server.\n\n\nInstalling, configuring and maintaining a Spark cluster is a different topic and will \nnot be covered here. Take a look at the \nSpark website\n \nfor more information.\n\n\n\n\n\n\nConfigure MySQL/MariaDB\n\n\nAssume you have a MySQL/MariaDB database running at \nDATABASE_HOST:DATABASE_PORT\n. You can\nconfigure this database for Cebes using the \n\nsetup-db.sh\n script.\nJust run it in a terminal:\n\n\n$ ./setup-db.sh\n\n\n\n\n\nThe script will ask for the address of the database, the admin username of the database, and create\nthe users and databases needed by Cebes server.\n\n\n\n\nAttention\n\n\nRemember to change the default usernames and passwords in this script to something more secured.\n\n\n\n\n\n\n\n\nDownload the latest version of \ncebes-http-server\n:\n\n\nOn the Spark master node:\n\n\n$ CEBES_VERSION=\"0.9.0\"\n$ wget https://github.com/phvu/cebes-server/releases/download/v${CEBES_VERSION}/cebes-http-server-assembly-${CEBES_VERSION}.jar\n\n\n\n\n\nThe latest releases can be found on \ngithub\n.\n\n\n\n\n\n\nSubmit Cebes server as a Spark application:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n#!/usr/bin/env bash\n\n\nCEBES_MYSQL_SERVER\n=\n\"<MySQL_server_host:my_sql_server_port>\"\n\n\nMYSQL_OPTIONS\n=\n\"?createDatabaseIfNotExist=true&nullNamePatternMatchesAll=true&useSSL=false\"\n\n\n\nexport\n \nCEBES_HIVE_METASTORE_URL\n=\n\"jdbc:mysql://\n${\nCEBES_MYSQL_SERVER\n}\n/cebes_hive_metastore\n${\nMYSQL_OPTIONS\n}\n\"\n\n\nexport\n \nCEBES_HIVE_METASTORE_DRIVER\n=\n\"org.mariadb.jdbc.Driver\"\n\n\nexport\n \nCEBES_HIVE_METASTORE_USERNAME\n=\n\"<mysql_username_for_hive>\"\n\n\nexport\n \nCEBES_HIVE_METASTORE_PASSWORD\n=\n\"<mysql_password_for_hive>\"\n\n\n\nexport\n \nCEBES_MYSQL_URL\n=\n\"jdbc:mysql://\n${\nCEBES_MYSQL_SERVER\n}\n/cebes_store\n${\nMYSQL_OPTIONS\n}\n\"\n\n\nexport\n \nCEBES_MYSQL_DRIVER\n=\n\"org.mariadb.jdbc.Driver\"\n\n\nexport\n \nCEBES_MYSQL_USERNAME\n=\n\"<mysql_username_for_cebes>\"\n\n\nexport\n \nCEBES_MYSQL_PASSWORD\n=\n\"<mysql_password_for_cebes>\"\n\n\n\n# the directory in which ML models will be saved.\n\n\n# should be an HDFS path if Spark runs on Hadoop\n\n\nexport\n \nCEBES_MODEL_STORAGE_DIR\n=\n\"hdfs://tmp\"\n\n\n\nexport\n \nCEBES_SPARK_MODE\n=\n\"yarn\"\n\n\n\n${\nSPARK_HOME\n}\n/bin/spark-submit --class \n\"io.cebes.server.Main\"\n \n\\\n\n     --master \n\"yarn\"\n \n\\\n\n     --conf \n'spark.driver.extraJavaOptions=-Dcebes.logs.dir=/tmp/'\n \n\\\n\n     cebes-http-server-assembly-\n${\nCEBES_VERSION\n}\n.jar\n\n\n\n\n\n\n\n\n\n\nAn example bash script can be found under \ndocker/http-server/start_cebes.sh\n on github.\n\n\n\n\nAutomated deployment\n\u00b6\n\n\nT.B.A\n\n\n\n\nCebes server configuration\n\u00b6\n\n\nHere are a list of configuration variables you need to specify for \ncebes-http-server\n. You can specify these values\nusing environment variables, or the \nextraJavaOptions\n field when running \nspark-submit\n.\n\n\n\n\n\n\n\n\nEnvironment variable name\n\n\nConfiguration key\n\n\nDescription\n\n\nDefault value\n\n\n\n\n\n\n\n\n\n\nCEBES_SPARK_MODE\n\n\ncebes.spark.mode\n\n\nWhich mode to run Spark: local or yarn\n\n\nlocal\n\n\n\n\n\n\nCEBES_HTTP_INTERFACE\n\n\ncebes.http.interface\n\n\nThe interface on which the HTTP service will be listening\n\n\nlocalhost\n\n\n\n\n\n\nCEBES_HTTP_PORT\n\n\ncebes.http.port\n\n\nThe port on which the HTTP service will be listening, to be combined with HTTP_INTERFACE\n\n\n21000\n\n\n\n\n\n\nCEBES_HTTP_SERVER_SECRET\n\n\ncebes.http.server.secret\n\n\nThe secret string to be used in authentication of the HTTP server\n\n\nskipped\n\n\n\n\n\n\nCEBES_UPLOAD_PATH\n\n\ncebes.upload.path\n\n\nThe directory to upload data to\n\n\n/tmp/upload/\n\n\n\n\n\n\nCEBES_DEFAULT_REPOSITORY_HOST\n\n\ncebes.default.repository.host\n\n\nThe default repository to talk to, used in http-server\n\n\nrepo.cebes.io\n\n\n\n\n\n\nCEBES_DEFALT_REPOSITORY_PORT\n\n\ncebes.default.repository.port\n\n\nThe default repository port to talk to, used in http-server\n\n\n80\n\n\n\n\n\n\nCEBES_HIVE_METASTORE_URL\n\n\ncebes.hive.metastore.url\n\n\nURL for the hive metastore\n\n\n\n\n\n\n\n\nCEBES_HIVE_METASTORE_DRIVER\n\n\ncebes.hive.metastore.driver\n\n\nDriver name for the hive metastore\n\n\norg.mariadb.jdbc.Driver\n\n\n\n\n\n\nCEBES_HIVE_METASTORE_USERNAME\n\n\ncebes.hive.metastore.username\n\n\nUsername for the hive metastore\n\n\n\n\n\n\n\n\nCEBES_HIVE_METASTORE_PASSWORD\n\n\ncebes.hive.metastore.password\n\n\nPassword for the hive metastore\n\n\n\n\n\n\n\n\nCEBES_SPARK_WAREHOUSE_DIR\n\n\ncebes.spark.warehouse.dir\n\n\nParent directory to the Spark SQL warehouse\n\n\n/tmp/spark-warehouse\n\n\n\n\n\n\nCEBES_CACHESPEC_DF_STORE\n\n\ncebes.cachespec.df.store\n\n\nSpec for the cache used for dataframe storage in cebes-spark\n\n\nmaximumSize=1000,expireAfterAccess=30m\n\n\n\n\n\n\nCEBES_CACHESPEC_RESULT_STORE\n\n\ncebes.cachespec.result.store\n\n\nSpec for the cache used for result storage in cebes-http-server\n\n\nmaximumSize=1000,expireAfterAccess=30m\n\n\n\n\n\n\nCEBES_CACHESPEC_PIPELINE_STORE\n\n\ncebes.cachespec.pipeline.store\n\n\nSpec for the cache used for pipeline storage in cebes-spark\n\n\nmaximumSize=500,expireAfterAccess=30m\n\n\n\n\n\n\nCEBES_CACHESPEC_MODEL_STORE\n\n\ncebes.cachespec.model.store\n\n\nSpec for the cache used for model storage in cebes-spark\n\n\nmaximumSize=500,expireAfterAccess=30m\n\n\n\n\n\n\nCEBES_PIPELINE_STAGE_NAMESPACES\n\n\ncebes.pipeline.stage.namespaces\n\n\na comma-separated list of namespaces containing definition of stages\n\n\nskipped\n\n\n\n\n\n\nCEBES_MODEL_STORAGE_DIR\n\n\ncebes.model.storage.dir\n\n\nThe directory to which all the models are serialized and saved\n\n\n/tmp\n\n\n\n\n\n\nCEBES_MYSQL_URL\n\n\ncebes.mysql.url\n\n\nURL for MySQL database\n\n\n\n\n\n\n\n\nCEBES_MYSQL_DRIVER\n\n\ncebes.mysql.driver\n\n\nDriver for MySQL database\n\n\norg.mariadb.jdbc.Driver\n\n\n\n\n\n\nCEBES_MYSQL_USERNAME\n\n\ncebes.mysql.username\n\n\nUsername for MySQL database\n\n\n\n\n\n\n\n\nCEBES_MYSQL_PASSWORD\n\n\ncebes.mysql.password\n\n\nPassword for MySQL database",
            "title": "Installation"
        },
        {
            "location": "/installation/#install-the-python-client-pycebes",
            "text": "pycebes  is the Python client that you use to work with  Cebes server .   However, when you install  pycebes ,  Cebes server  is also included, so that you can do everything \nas if it was a full-blown Cebes system. You are not forced to use this server though. The client \nworks with any compatible Cebes server.    (Optional) Install docker for your workstation:  Mac , \n     Ubuntu , \n     Windows , \n     others  You can skip this step if you don't want to have Cebes server running locally. Whatever you choose, you\ncan always connect to a Cebes server running on a separated Spark cluster.  In the future, we might provide playground Cebes servers for you to play with. Stay tuned!    Install  pycebes :  $ pip install pycebes    You can test your installation by connecting the client to a server, as outlined below.",
            "title": "Install the Python client pycebes"
        },
        {
            "location": "/installation/#connect-to-a-cebes-server",
            "text": "Using the client, you can connect to a Cebes server by creating a new  Session .",
            "title": "Connect to a Cebes server"
        },
        {
            "location": "/installation/#connect-to-a-local-cebes-server",
            "text": "If your docker daemon is running, Cebes client can start a new container containing a suitable version of \nCebes server. In a Python interpreter, just create a  Session  object without argument:  >>>   import   pycebes   as   cb  >>>   s   =   cb . Session () \n     [ INFO ]   pycebes . internal . docker_helpers :   Starting   Cebes   container   cebes - server - x . xx . x - 0 [ phvu / cebes : x . xx . x ]   with   data   path   at   $ HOME /. cebes / x . xx . x \n     [ INFO ]   pycebes . internal . docker_helpers :   Cebes   container   started ,   listening   at   localhost : 32770 \n     [ INFO ]   pycebes . core . session :   Connecting   to   Cebes   container   cebes - server - x . xx . x - 0 [ phvu / cebes : x . xx . x ]   at   port   32770 \n     [ INFO ]   pycebes . core . session :   Spark   UI   can   be   accessed   at   http : // localhost : 32771   If you haven't installed Docker, or the docker daemon has not started, the above command will fail:  >>>   import   pycebes   as   cb   >>>   s   =   cb . Session () \n     ... \n     ValueError :   Could   not   create   docker   client :   Error   while   fetching   server   API   version :  \n     ( 'Connection aborted.' ,   ConnectionRefusedError ( 111 ,   'Connection refused' )) .  \n     Have   you   started   the   docker   daemon ?",
            "title": "Connect to a local Cebes server"
        },
        {
            "location": "/installation/#connect-to-a-remote-cebes-server",
            "text": "In any case, you can always connect to a Cebes server by specifying a hostname and a port:  >>>   import   pycebes   as   cb  >>>   s   =   cb . Session ( 'cebes.server.com' ,   21000 )   With the built-in server, you can already start exploring Cebes. Whenever you feel serious,\nor your datasets get bigger, you can always install Cebes on a Spark cluster, \nas outlined in the next section.",
            "title": "Connect to a remote Cebes server"
        },
        {
            "location": "/installation/#install-cebes-server-on-a-spark-cluster",
            "text": "To use Cebes on big datasets, you need a Spark cluster. The recommended setup is you should\nhave a Spark cluster for your team/company, and the Cebes server runs on this cluster. Users \n(e.g. Data Scientists, DevOps engineers) in your team will then use  pycebes  to connect \nto the Cebes server.  Installing, configuring and maintaining a Spark cluster is a different topic and will \nnot be covered here. Take a look at the  Spark website  \nfor more information.    Configure MySQL/MariaDB  Assume you have a MySQL/MariaDB database running at  DATABASE_HOST:DATABASE_PORT . You can\nconfigure this database for Cebes using the  setup-db.sh  script.\nJust run it in a terminal:  $ ./setup-db.sh  The script will ask for the address of the database, the admin username of the database, and create\nthe users and databases needed by Cebes server.   Attention  Remember to change the default usernames and passwords in this script to something more secured.     Download the latest version of  cebes-http-server :  On the Spark master node:  $ CEBES_VERSION=\"0.9.0\"\n$ wget https://github.com/phvu/cebes-server/releases/download/v${CEBES_VERSION}/cebes-http-server-assembly-${CEBES_VERSION}.jar  The latest releases can be found on  github .    Submit Cebes server as a Spark application:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 #!/usr/bin/env bash  CEBES_MYSQL_SERVER = \"<MySQL_server_host:my_sql_server_port>\"  MYSQL_OPTIONS = \"?createDatabaseIfNotExist=true&nullNamePatternMatchesAll=true&useSSL=false\"  export   CEBES_HIVE_METASTORE_URL = \"jdbc:mysql:// ${ CEBES_MYSQL_SERVER } /cebes_hive_metastore ${ MYSQL_OPTIONS } \"  export   CEBES_HIVE_METASTORE_DRIVER = \"org.mariadb.jdbc.Driver\"  export   CEBES_HIVE_METASTORE_USERNAME = \"<mysql_username_for_hive>\"  export   CEBES_HIVE_METASTORE_PASSWORD = \"<mysql_password_for_hive>\"  export   CEBES_MYSQL_URL = \"jdbc:mysql:// ${ CEBES_MYSQL_SERVER } /cebes_store ${ MYSQL_OPTIONS } \"  export   CEBES_MYSQL_DRIVER = \"org.mariadb.jdbc.Driver\"  export   CEBES_MYSQL_USERNAME = \"<mysql_username_for_cebes>\"  export   CEBES_MYSQL_PASSWORD = \"<mysql_password_for_cebes>\"  # the directory in which ML models will be saved.  # should be an HDFS path if Spark runs on Hadoop  export   CEBES_MODEL_STORAGE_DIR = \"hdfs://tmp\"  export   CEBES_SPARK_MODE = \"yarn\"  ${ SPARK_HOME } /bin/spark-submit --class  \"io.cebes.server.Main\"   \\ \n     --master  \"yarn\"   \\ \n     --conf  'spark.driver.extraJavaOptions=-Dcebes.logs.dir=/tmp/'   \\ \n     cebes-http-server-assembly- ${ CEBES_VERSION } .jar     An example bash script can be found under  docker/http-server/start_cebes.sh  on github.",
            "title": "Install Cebes server on a Spark cluster"
        },
        {
            "location": "/installation/#automated-deployment",
            "text": "T.B.A",
            "title": "Automated deployment"
        },
        {
            "location": "/installation/#cebes-server-configuration",
            "text": "Here are a list of configuration variables you need to specify for  cebes-http-server . You can specify these values\nusing environment variables, or the  extraJavaOptions  field when running  spark-submit .     Environment variable name  Configuration key  Description  Default value      CEBES_SPARK_MODE  cebes.spark.mode  Which mode to run Spark: local or yarn  local    CEBES_HTTP_INTERFACE  cebes.http.interface  The interface on which the HTTP service will be listening  localhost    CEBES_HTTP_PORT  cebes.http.port  The port on which the HTTP service will be listening, to be combined with HTTP_INTERFACE  21000    CEBES_HTTP_SERVER_SECRET  cebes.http.server.secret  The secret string to be used in authentication of the HTTP server  skipped    CEBES_UPLOAD_PATH  cebes.upload.path  The directory to upload data to  /tmp/upload/    CEBES_DEFAULT_REPOSITORY_HOST  cebes.default.repository.host  The default repository to talk to, used in http-server  repo.cebes.io    CEBES_DEFALT_REPOSITORY_PORT  cebes.default.repository.port  The default repository port to talk to, used in http-server  80    CEBES_HIVE_METASTORE_URL  cebes.hive.metastore.url  URL for the hive metastore     CEBES_HIVE_METASTORE_DRIVER  cebes.hive.metastore.driver  Driver name for the hive metastore  org.mariadb.jdbc.Driver    CEBES_HIVE_METASTORE_USERNAME  cebes.hive.metastore.username  Username for the hive metastore     CEBES_HIVE_METASTORE_PASSWORD  cebes.hive.metastore.password  Password for the hive metastore     CEBES_SPARK_WAREHOUSE_DIR  cebes.spark.warehouse.dir  Parent directory to the Spark SQL warehouse  /tmp/spark-warehouse    CEBES_CACHESPEC_DF_STORE  cebes.cachespec.df.store  Spec for the cache used for dataframe storage in cebes-spark  maximumSize=1000,expireAfterAccess=30m    CEBES_CACHESPEC_RESULT_STORE  cebes.cachespec.result.store  Spec for the cache used for result storage in cebes-http-server  maximumSize=1000,expireAfterAccess=30m    CEBES_CACHESPEC_PIPELINE_STORE  cebes.cachespec.pipeline.store  Spec for the cache used for pipeline storage in cebes-spark  maximumSize=500,expireAfterAccess=30m    CEBES_CACHESPEC_MODEL_STORE  cebes.cachespec.model.store  Spec for the cache used for model storage in cebes-spark  maximumSize=500,expireAfterAccess=30m    CEBES_PIPELINE_STAGE_NAMESPACES  cebes.pipeline.stage.namespaces  a comma-separated list of namespaces containing definition of stages  skipped    CEBES_MODEL_STORAGE_DIR  cebes.model.storage.dir  The directory to which all the models are serialized and saved  /tmp    CEBES_MYSQL_URL  cebes.mysql.url  URL for MySQL database     CEBES_MYSQL_DRIVER  cebes.mysql.driver  Driver for MySQL database  org.mariadb.jdbc.Driver    CEBES_MYSQL_USERNAME  cebes.mysql.username  Username for MySQL database     CEBES_MYSQL_PASSWORD  cebes.mysql.password  Password for MySQL database",
            "title": "Cebes server configuration"
        },
        {
            "location": "/roadmap/",
            "text": "Roadmap\n\u00b6\n\n\nHere are the features that Cebes is missing, and will be improved, in no particular order. \nTalk to us if you are interested in making some of them (or all of them) happen!\n\n\n\n\nPipelines: more Machine Learning and ETL stages, polish the workflow and API.\n\n\nAuthentication and Authorization for \ncebes-server\n and \ncebes-pipeline-serving\n.\n\n\nFrontend with user management and for pipeline repository.\n\n\nAuto-deployment for \ncebes-pipeline-serving\n\n\n\n\nSome of these are captured in Cebes github issues. Feel free to create new issues for \nfeatures that you care about.",
            "title": "Roadmap"
        },
        {
            "location": "/roadmap/#roadmap",
            "text": "Here are the features that Cebes is missing, and will be improved, in no particular order. \nTalk to us if you are interested in making some of them (or all of them) happen!   Pipelines: more Machine Learning and ETL stages, polish the workflow and API.  Authentication and Authorization for  cebes-server  and  cebes-pipeline-serving .  Frontend with user management and for pipeline repository.  Auto-deployment for  cebes-pipeline-serving   Some of these are captured in Cebes github issues. Feel free to create new issues for \nfeatures that you care about.",
            "title": "Roadmap"
        },
        {
            "location": "/session/",
            "text": "Connect to Cebes server\n\u00b6\n\n\nUsing \npycebes\n, you need to connect to Cebes server by creating a \nSession\n:\n\n\n>>>\n \nimport\n \npycebes\n \nas\n \ncb\n\n\n>>>\n \nsession\n \n=\n \ncb\n.\nSession\n()\n\n    \n[\nINFO\n]\n \npycebes\n.\ninternal\n.\ndocker_helpers\n:\n \nStarting\n \nCebes\n \ncontainer\n \n        \ncebes\n-\nserver\n-\nx\n.\nxx\n.\nx\n-\n0\n[\nphvu\n/\ncebes\n:\nx\n.\nxx\n.\nx\n]\n \nwith\n \ndata\n \npath\n \nat\n \n$\nHOME\n/.\ncebes\n/\nx\n.\nxx\n.\nx\n\n    \n[\nINFO\n]\n \npycebes\n.\ninternal\n.\ndocker_helpers\n:\n \nCebes\n \ncontainer\n \nstarted\n,\n \nlistening\n \nat\n \nlocalhost\n:\n32770\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nConnecting\n \nto\n \nCebes\n \ncontainer\n \ncebes\n-\nserver\n-\nx\n.\nxx\n.\nx\n-\n0\n[\nphvu\n/\ncebes\n:\nx\n.\nxx\n.\nx\n]\n \nat\n \nport\n \n32770\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nSpark\n \nUI\n \ncan\n \nbe\n \naccessed\n \nat\n \nhttp\n:\n//\nlocalhost\n:\n32771\n\n\n\n\n\n\nWithout arguments, \nSession\n will try to establish a connection to a local Cebes server running as \na Docker container on your workstation. If that succeed, the log will show the details as above.\n\n\nIf you don't stop the docker container, next time Cebes client will automatically connect to the \nrunning container without starting a new one:\n\n\n>>>\n \nanother_session\n \n=\n \ncb\n.\nSession\n()\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nConnecting\n \nto\n \nCebes\n \ncontainer\n \n        \ncebes\n-\nserver\n-\nx\n.\nxx\n.\nx\n-\n0\n[\nphvu\n/\ncebes\n:\nx\n.\nxx\n.\nx\n]\n \nat\n \nport\n \n32770\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nSpark\n \nUI\n \ncan\n \nbe\n \naccessed\n \nat\n \nhttp\n:\n//\nlocalhost\n:\n32771\n\n\n\n\n\n\nIf you haven't installed Docker, or the docker daemon has not started, the above command will fail:\n\n\n>>>\n \nsession\n \n=\n \ncb\n.\nSession\n()\n\n    \n...\n\n    \nValueError\n:\n \nCould\n \nnot\n \ncreate\n \ndocker\n \nclient\n:\n \nError\n \nwhile\n \nfetching\n \nserver\n \nAPI\n \nversion\n:\n \n    \n(\n'Connection aborted.'\n,\n \nConnectionRefusedError\n(\n111\n,\n \n'Connection refused'\n))\n.\n \n    \nHave\n \nyou\n \nstarted\n \nthe\n \ndocker\n \ndaemon\n?\n\n\n\n\n\n\nIn any case, you can always connect to a Cebes server by specifying a hostname and a port:\n\n\n>>>\n \nsession\n \n=\n \ncb\n.\nSession\n(\n'cebes.server.com'\n,\n \n21000\n)\n\n\n\n\n\n\n\n\nDefault Session and the session context\n\u00b6\n\n\nUsually you only need one \nSession\n to work with a Cebes server. Within a Python thread,\nthe first \nSession\n you created will become the default Session. At any point in time,\nyou can call \nget_default_session()\n to get the current default Session, which will be \nused for all commands issued in the thread:\n\n\nimport\n \npycebes\n \nas\n \ncb\n\n\n\nsession\n \n=\n \ncb\n.\nSession\n()\n\n\nassert\n \ncb\n.\nget_default_session\n()\n \nis\n \nsession\n\n\n\n\n\n\nIn the same Python thread, you can create additional Sessions, but then you need \nto specify which Session you want to use by the \nas_default()\n context manager:\n\n\nwith\n \ncb\n.\nSession\n()\n.\nas_default\n()\n \nas\n \nanother_session\n:\n\n    \n# all commands in this block use `another_session`\n\n    \nanother_session\n.\ndataframe\n.\nlist\n()\n\n\n\n# all commands now use `session`\n\n\n\nyet_another_session\n \n=\n \ncb\n.\nSession\n()\n\n\nwith\n \nyet_another_session\n.\nas_default\n():\n\n    \n# all commands in this block use `yet_another_session`\n\n    \npass\n\n\n\n\n\n\nIn these examples, we use the \nSession\n object explicitly, hence it is easy to say which\none is being used. However, Cebes has some other APIs that involve implicit default session, \nin which case the context manager will come handy.\n\n\n\n\nWorking with Session\n\u00b6\n\n\nSession\n objects are equipped with 2 main groups of functions: \n\n\n\n\nloading data into Cebes\n \n\n\nmanaging dataframes, models and pipelines",
            "title": "Key concepts"
        },
        {
            "location": "/session/#connect-to-cebes-server",
            "text": "Using  pycebes , you need to connect to Cebes server by creating a  Session :  >>>   import   pycebes   as   cb  >>>   session   =   cb . Session () \n     [ INFO ]   pycebes . internal . docker_helpers :   Starting   Cebes   container  \n         cebes - server - x . xx . x - 0 [ phvu / cebes : x . xx . x ]   with   data   path   at   $ HOME /. cebes / x . xx . x \n     [ INFO ]   pycebes . internal . docker_helpers :   Cebes   container   started ,   listening   at   localhost : 32770 \n     [ INFO ]   pycebes . core . session :   Connecting   to   Cebes   container   cebes - server - x . xx . x - 0 [ phvu / cebes : x . xx . x ]   at   port   32770 \n     [ INFO ]   pycebes . core . session :   Spark   UI   can   be   accessed   at   http : // localhost : 32771   Without arguments,  Session  will try to establish a connection to a local Cebes server running as \na Docker container on your workstation. If that succeed, the log will show the details as above.  If you don't stop the docker container, next time Cebes client will automatically connect to the \nrunning container without starting a new one:  >>>   another_session   =   cb . Session () \n     [ INFO ]   pycebes . core . session :   Connecting   to   Cebes   container  \n         cebes - server - x . xx . x - 0 [ phvu / cebes : x . xx . x ]   at   port   32770 \n     [ INFO ]   pycebes . core . session :   Spark   UI   can   be   accessed   at   http : // localhost : 32771   If you haven't installed Docker, or the docker daemon has not started, the above command will fail:  >>>   session   =   cb . Session () \n     ... \n     ValueError :   Could   not   create   docker   client :   Error   while   fetching   server   API   version :  \n     ( 'Connection aborted.' ,   ConnectionRefusedError ( 111 ,   'Connection refused' )) .  \n     Have   you   started   the   docker   daemon ?   In any case, you can always connect to a Cebes server by specifying a hostname and a port:  >>>   session   =   cb . Session ( 'cebes.server.com' ,   21000 )",
            "title": "Connect to Cebes server"
        },
        {
            "location": "/session/#default-session-and-the-session-context",
            "text": "Usually you only need one  Session  to work with a Cebes server. Within a Python thread,\nthe first  Session  you created will become the default Session. At any point in time,\nyou can call  get_default_session()  to get the current default Session, which will be \nused for all commands issued in the thread:  import   pycebes   as   cb  session   =   cb . Session ()  assert   cb . get_default_session ()   is   session   In the same Python thread, you can create additional Sessions, but then you need \nto specify which Session you want to use by the  as_default()  context manager:  with   cb . Session () . as_default ()   as   another_session : \n     # all commands in this block use `another_session` \n     another_session . dataframe . list ()  # all commands now use `session`  yet_another_session   =   cb . Session ()  with   yet_another_session . as_default (): \n     # all commands in this block use `yet_another_session` \n     pass   In these examples, we use the  Session  object explicitly, hence it is easy to say which\none is being used. However, Cebes has some other APIs that involve implicit default session, \nin which case the context manager will come handy.",
            "title": "Default Session and the session context"
        },
        {
            "location": "/session/#working-with-session",
            "text": "Session  objects are equipped with 2 main groups of functions:    loading data into Cebes    managing dataframes, models and pipelines",
            "title": "Working with Session"
        },
        {
            "location": "/session_load_data/",
            "text": "Depending on where your dataset is, Cebes can load most of them using the following APIs.\n\n\nLocal datasets\n\u00b6\n\n\nThe most straightforward case is when your data is a \npandas DataFrame\n. You can create a \nCebes Dataframe out of it using \nSession.from_pandas\n:\n\n\n>>>\n \nimport\n \npandas\n \nas\n \npd\n\n\n>>>\n \npandas_df\n \n=\n \npd\n.\nDataFrame\n([[\n1\n,\n \n'a'\n],\n \n[\n2\n,\n \n'b'\n]],\n \ncolumns\n=\n[\n'number'\n,\n \n'string'\n])\n\n\n>>>\n \ncebes_df\n \n=\n \nsession\n.\nfrom_pandas\n(\npandas_df\n)\n\n\n    \nUploading\n:\n \n....................\n \n100\n%\n\n\n\n\n\n\nUnder the hood, this function will serialize and upload the pandas DataFrame to Cebes server,\nthen create a Cebes Dataframe out of it, which you can use as a normal Cebes Dataframe:\n\n\n>>>\n \ncebes_df\n.\nshow\n()\n\n    \nShape\n:\n \n(\n2\n,\n \n2\n)\n\n    \nSample\n \n2\n \nrows\n:\n\n       \nnumber\n \nstring\n\n    \n0\n       \n1\n      \na\n\n    \n1\n       \n2\n      \nb\n\n\n\n>>>\n \ncebes_df\n.\nschema\n\n    \nSchema\n(\nfields\n=\n[\nSchemaField\n(\nname\n=\n'number'\n,\nstorage_type\n=\nINTEGER\n,\nvariable_type\n=\nDISCRETE\n),\n\n                   \nSchemaField\n(\nname\n=\n'string'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n)])\n\n\n\n\n\n\nA \nJSON file\n can be uploaded with \nread_json\n:\n\n\n>>>\n \njson_options\n \n=\n \ncb\n.\nJsonReadOptions\n(\ndate_format\n=\n'yyyy-MM-dd'\n)\n\n\n>>>\n \ndf\n \n=\n \nsession\n.\nread_json\n(\npath\n=\n'/path/to/data.json'\n,\n \noptions\n=\njson_options\n)\n\n\n\n\n\n\nSimilarly, there is \nread_csv\n for \nCSV files\n:\n\n\n>>>\n \ncsv_options\n \n=\n \ncb\n.\nCsvReadOptions\n(\nsep\n=\n','\n,\n \nencoding\n=\n'UTF-8'\n,\n \nquote\n=\n'\"'\n)\n\n\n>>>\n \ndf\n \n=\n \nsession\n.\nread_csv\n(\npath\n=\n'/path/to/data.csv'\n,\n \noptions\n=\ncsv_options\n)\n\n\n\n\n\n\nMore generally, any \nJSON, CSV, Parquet, ORC file\n on your machine can be loaded with \n\nread_local\n:\n\n\n>>>\n \nparquet_options\n \n=\n \ncb\n.\nParquetReadOptions\n(\nmerge_schema\n=\nTrue\n)\n\n\n>>>\n \ndf\n \n=\n \nsession\n.\nread_local\n(\npath\n=\n'/path/to/data.parquet'\n,\n \nfmt\n=\n'parquet'\n,\n \noptions\n=\nparquet_options\n)\n\n\n\n\n\n\nFinally, when everything above fails, you can still use \nread_local()\n with \nfmt='text'\n, which will \ngive you a Dataframe of text lines. Then you can use Cebes \npowerful Dataframe APIs\n \nto extract relevant information.\n\n\nLocal datasets are convenient when you want to quickly test something, or your datasets are small.\n\n\n\n\nRemote datasets\n\u00b6\n\n\nMore often, if you have a separated Spark cluster, it might already contain interesting business data.\nIn that case you can load it into Cebes using the following APIs.\n\n\nA table in an RDBMS can be read with \nread_jdbc\n:\n\n\n>>>\n \ndf\n \n=\n \nsession\n.\nread_jdbc\n(\nurl\n=\n'mysql.server.com:3306'\n,\n \ntable_name\n=\n'sales'\n,\n \n                           \nuser_name\n=\n'admin'\n,\n \npassword\n=\n'abc'\n)\n\n\n\n\n\n\nSimilarly, a table in Hive can be read with \nread_hive\n:\n\n\n>>>\n \ndf\n \n=\n \nsession\n.\nread_hive\n(\ntable_name\n=\n'my_data'\n)\n\n\n\n\n\n\nAny JSON, CSV, Parquet, ORC or text files stored in Amazon S3 can be read with \n\nread_s3\n:\n\n\n>>>\n \ndf\n \n=\n \nsession\n.\nread_s3\n(\nbucket\n=\n'data'\n,\n \nkey\n=\n'sales/2017/'\n,\n \n                         \naccess_key\n=\n'YOUR_S3_ACCESS_KEY'\n,\n \nsecret_key\n=\n'YOUR_S3_SECRET_KEY'\n,\n \n                         \nfmt\n=\n'csv'\n,\n \noptions\n=\ncb\n.\nCsvReadOptions\n())\n\n\n\n\n\n\nMore often, you might already have data files in Hadoop, in which case \n\nread_hdfs\n will come handy:\n\n\n>>>\n \ndf\n \n=\n \nsession\n.\nread_hdfs\n(\npath\n=\n'/data/path'\n,\n \nserver\n=\nNone\n,\n \nfmt\n=\n'parquet'\n,\n \n                           \noptions\n=\ncb\n.\nParquetReadOptions\n())\n\n\n\n\n\n\nread_s3\n and \nread_hdfs\n support data files in JSON, CSV, Parquet, ORC or text, just like \nread_local\n.\n\n\nOnce you read your datasets into Cebes, it is a good idea to tag them, so they won't get evicted. \nKeep reading to learn about \ntags\n and why they matter in Cebes.",
            "title": "Load data into Cebes"
        },
        {
            "location": "/session_load_data/#local-datasets",
            "text": "The most straightforward case is when your data is a  pandas DataFrame . You can create a \nCebes Dataframe out of it using  Session.from_pandas :  >>>   import   pandas   as   pd  >>>   pandas_df   =   pd . DataFrame ([[ 1 ,   'a' ],   [ 2 ,   'b' ]],   columns = [ 'number' ,   'string' ])  >>>   cebes_df   =   session . from_pandas ( pandas_df )       Uploading :   ....................   100 %   Under the hood, this function will serialize and upload the pandas DataFrame to Cebes server,\nthen create a Cebes Dataframe out of it, which you can use as a normal Cebes Dataframe:  >>>   cebes_df . show () \n     Shape :   ( 2 ,   2 ) \n     Sample   2   rows : \n        number   string \n     0         1        a \n     1         2        b  >>>   cebes_df . schema \n     Schema ( fields = [ SchemaField ( name = 'number' , storage_type = INTEGER , variable_type = DISCRETE ), \n                    SchemaField ( name = 'string' , storage_type = STRING , variable_type = TEXT )])   A  JSON file  can be uploaded with  read_json :  >>>   json_options   =   cb . JsonReadOptions ( date_format = 'yyyy-MM-dd' )  >>>   df   =   session . read_json ( path = '/path/to/data.json' ,   options = json_options )   Similarly, there is  read_csv  for  CSV files :  >>>   csv_options   =   cb . CsvReadOptions ( sep = ',' ,   encoding = 'UTF-8' ,   quote = '\"' )  >>>   df   =   session . read_csv ( path = '/path/to/data.csv' ,   options = csv_options )   More generally, any  JSON, CSV, Parquet, ORC file  on your machine can be loaded with  read_local :  >>>   parquet_options   =   cb . ParquetReadOptions ( merge_schema = True )  >>>   df   =   session . read_local ( path = '/path/to/data.parquet' ,   fmt = 'parquet' ,   options = parquet_options )   Finally, when everything above fails, you can still use  read_local()  with  fmt='text' , which will \ngive you a Dataframe of text lines. Then you can use Cebes  powerful Dataframe APIs  \nto extract relevant information.  Local datasets are convenient when you want to quickly test something, or your datasets are small.",
            "title": "Local datasets"
        },
        {
            "location": "/session_load_data/#remote-datasets",
            "text": "More often, if you have a separated Spark cluster, it might already contain interesting business data.\nIn that case you can load it into Cebes using the following APIs.  A table in an RDBMS can be read with  read_jdbc :  >>>   df   =   session . read_jdbc ( url = 'mysql.server.com:3306' ,   table_name = 'sales' ,  \n                            user_name = 'admin' ,   password = 'abc' )   Similarly, a table in Hive can be read with  read_hive :  >>>   df   =   session . read_hive ( table_name = 'my_data' )   Any JSON, CSV, Parquet, ORC or text files stored in Amazon S3 can be read with  read_s3 :  >>>   df   =   session . read_s3 ( bucket = 'data' ,   key = 'sales/2017/' ,  \n                          access_key = 'YOUR_S3_ACCESS_KEY' ,   secret_key = 'YOUR_S3_SECRET_KEY' ,  \n                          fmt = 'csv' ,   options = cb . CsvReadOptions ())   More often, you might already have data files in Hadoop, in which case  read_hdfs  will come handy:  >>>   df   =   session . read_hdfs ( path = '/data/path' ,   server = None ,   fmt = 'parquet' ,  \n                            options = cb . ParquetReadOptions ())   read_s3  and  read_hdfs  support data files in JSON, CSV, Parquet, ORC or text, just like  read_local .  Once you read your datasets into Cebes, it is a good idea to tag them, so they won't get evicted. \nKeep reading to learn about  tags  and why they matter in Cebes.",
            "title": "Remote datasets"
        },
        {
            "location": "/session_df/",
            "text": "Cebes server keeps a list of all your dataframes, models and pipelines. Those are 3 types of first-class \ncitizens that you work with in Cebes.\n\n\nManaging Dataframes\n\u00b6\n\n\nIn Cebes, each Dataframe is uniquely identified by an ID. All Dataframes are kept in memory until they \nget evicted. The \nCEBES_CACHESPEC_DF_STORE\n flag dictates \nhow and when Dataframes are evicted.\n\n\nDuring the exploration phase however, many temporary Dataframes might be created, while you are only \ninterested in a few of them. You might want to keep them around, or simply prevent them from eviction.\nTo do that, you can give them a tag - an easy-to-remember string:\n\n\n>>>\n \ncebes_df\n.\nid\n\n    \n'8a5d3a28-c3c2-4b58-bd1a-008fa4a33d54'\n\n\n>>>\n \nsession\n.\ndataframe\n.\ntag\n(\ncebes_df\n,\n \n'preprocessed_data'\n)\n\n\n\n\n\n\nA tagged Dataframe is guaranteed to be checkpointed and can be reloaded later, across Session, using its tag:\n\n\n>>>\n \ndf\n \n=\n \nsession\n.\ndataframe\n.\nget\n(\n'preprocessed_data'\n)\n\n\n>>>\n \nassert\n \ndf\n.\nid\n \n==\n \ncebes_df\n.\nid\n\n\n\n\n\n\nYou can get the list of all tagged Dataframes using \ndataframe.list()\n:\n\n\n>>>\n \nsession\n.\ndataframe\n.\nlist\n()\n\n    \nUUID\n                                  \nTag\n                        \nSchema\n                         \nCreated\n\n    \n------------------------------------\n  \n-------------------------\n  \n-----------------------------\n  \n--------------------------\n\n    \n8\na5d3a28\n-\nc3c2\n-\n4\nb58\n-\nbd1a\n-\n00\n8\nfa4a33d54\n  \npreprocessed_data\n:\ndefault\n  \nnumber\n \ninteger\n,\n \nstring\n \nstring\n  \n2017\n-\n12\n-\n30\n \n02\n:\n19\n:\n00.533000\n\n\n\n\n\n\nTechnically, a tag is a string with format \nNAME:VERSION\n. If you don't specify \nVERSION\n, the default version\nis \ndefault\n. This allows you to use the same tag name for multiple Dataframes with different versions.\n\n\n>>>\n \nsession\n.\ndataframe\n.\ntag\n(\nanother_df\n,\n \n'preprocessed_data:v1'\n)\n\n    \nDataframe\n(\nid\n=\n'102d8c93-138a-4c83-99f2-746a033891b9'\n)\n\n\n\n>>>\n \nsession\n.\ndataframe\n.\nlist\n()\n\n    \nUUID\n                                  \nTag\n                        \nSchema\n                         \nCreated\n\n    \n------------------------------------\n  \n-------------------------\n  \n-----------------------------\n  \n--------------------------\n\n    \n8\na5d3a28\n-\nc3c2\n-\n4\nb58\n-\nbd1a\n-\n00\n8\nfa4a33d54\n  \npreprocessed_data\n:\ndefault\n  \nnumber\n \ninteger\n,\n \nstring\n \nstring\n  \n2017\n-\n12\n-\n30\n \n02\n:\n19\n:\n00.533000\n\n    \n102\nd8c93\n-\n138\na\n-\n4\nc83\n-\n99\nf2\n-\n746\na033891b9\n  \npreprocessed_data\n:\nv1\n       \nnumber\n \ninteger\n,\n \nstring\n \nstring\n  \n2017\n-\n12\n-\n30\n \n02\n:\n27\n:\n05.747000\n\n\n\n\n\n\nMultiple tags for the same Dataframe is allowed, but tags are unique: the same tag cannot be used for different\nDataframes. Trying to do so will raise an exception.\n\n\nIf you want to reuse a tag, you need to \nuntag\n it first:\n\n\n>>>\n \nsession\n.\ndataframe\n.\nuntag\n(\n'preprocessed_data'\n)\n\n    \nDataframe\n(\nid\n=\n'8a5d3a28-c3c2-4b58-bd1a-008fa4a33d54'\n)\n\n\n\n>>>\n \nsession\n.\ndataframe\n.\nlist\n()\n\n    \nUUID\n                                  \nTag\n                   \nSchema\n                         \nCreated\n\n    \n------------------------------------\n  \n--------------------\n  \n-----------------------------\n  \n--------------------------\n\n    \n102\nd8c93\n-\n138\na\n-\n4\nc83\n-\n99\nf2\n-\n746\na033891b9\n  \npreprocessed_data\n:\nv1\n  \nnumber\n \ninteger\n,\n \nstring\n \nstring\n  \n2017\n-\n12\n-\n30\n \n02\n:\n27\n:\n05.747000\n\n\n\n\n\n\nNote how Cebes only untag \npreprocessed_data:default\n, while keeping \npreporcessed_data:v1\n unchanged. \nThis is because we did \nsession.dataframe.untag('preprocessed_data')\n, and Cebes looks for the full \ntag \npreprocessed_data:default\n to delete. \n\n\nTo delete \npreprocessed_data:v1\n, we need to be specific:\n\n\n>>>\n \nsession\n.\ndataframe\n.\nuntag\n(\n'preprocessed_data:v1'\n)\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\nTag\n is a string given to a Dataframe, so that users can retrieve them later. \n\n\nTagged Dataframes will be persisted in Cebes.\n\n\nTag has format \nNAME:VERSION\n. When unspecified, the default VERSION is \ndefault\n.\n\n\n\n\n\n\n\n\nManaging Models and Pipelines\n\u00b6\n\n\nAll \npipelines and models\n share the same management principles as Dataframes.\n\n\n\n\n\n\nFor pipelines:\n\n\n\n\nuse \nSession.pipeline.list\n, \nSession.pipeline.get\n, \nSession.pipeline.tag\n and \nSession.pipeline.untag\n\n\nthe \nCEBES_CACHESPEC_PIPELINE_STORE\n flag dictates how\nand when in-memory pipelines are evicted.\n\n\n\n\n\n\n\n\nFor models:\n\n\n\n\nuse \nSession.model.list\n, \nSession.model.get\n, \nSession.model.tag\n and \nSession.model.untag\n\n\nthe \nCEBES_CACHESPEC_MODEL_STORE\n flag dictates how\nand when in-memory models are evicted.\n\n\n\n\n\n\n\n\nSince they can be tagged, Cebes can handle pipelines and models as they evolve with your business logic.\nJust keep the same tag name but give them a different version, all your pipelines and models can be managed \nin a coherent way.",
            "title": "Manage Dataframes, models and pipelines"
        },
        {
            "location": "/session_df/#managing-dataframes",
            "text": "In Cebes, each Dataframe is uniquely identified by an ID. All Dataframes are kept in memory until they \nget evicted. The  CEBES_CACHESPEC_DF_STORE  flag dictates \nhow and when Dataframes are evicted.  During the exploration phase however, many temporary Dataframes might be created, while you are only \ninterested in a few of them. You might want to keep them around, or simply prevent them from eviction.\nTo do that, you can give them a tag - an easy-to-remember string:  >>>   cebes_df . id \n     '8a5d3a28-c3c2-4b58-bd1a-008fa4a33d54'  >>>   session . dataframe . tag ( cebes_df ,   'preprocessed_data' )   A tagged Dataframe is guaranteed to be checkpointed and can be reloaded later, across Session, using its tag:  >>>   df   =   session . dataframe . get ( 'preprocessed_data' )  >>>   assert   df . id   ==   cebes_df . id   You can get the list of all tagged Dataframes using  dataframe.list() :  >>>   session . dataframe . list () \n     UUID                                    Tag                          Schema                           Created \n     ------------------------------------    -------------------------    -----------------------------    -------------------------- \n     8 a5d3a28 - c3c2 - 4 b58 - bd1a - 00 8 fa4a33d54    preprocessed_data : default    number   integer ,   string   string    2017 - 12 - 30   02 : 19 : 00.533000   Technically, a tag is a string with format  NAME:VERSION . If you don't specify  VERSION , the default version\nis  default . This allows you to use the same tag name for multiple Dataframes with different versions.  >>>   session . dataframe . tag ( another_df ,   'preprocessed_data:v1' ) \n     Dataframe ( id = '102d8c93-138a-4c83-99f2-746a033891b9' )  >>>   session . dataframe . list () \n     UUID                                    Tag                          Schema                           Created \n     ------------------------------------    -------------------------    -----------------------------    -------------------------- \n     8 a5d3a28 - c3c2 - 4 b58 - bd1a - 00 8 fa4a33d54    preprocessed_data : default    number   integer ,   string   string    2017 - 12 - 30   02 : 19 : 00.533000 \n     102 d8c93 - 138 a - 4 c83 - 99 f2 - 746 a033891b9    preprocessed_data : v1         number   integer ,   string   string    2017 - 12 - 30   02 : 27 : 05.747000   Multiple tags for the same Dataframe is allowed, but tags are unique: the same tag cannot be used for different\nDataframes. Trying to do so will raise an exception.  If you want to reuse a tag, you need to  untag  it first:  >>>   session . dataframe . untag ( 'preprocessed_data' ) \n     Dataframe ( id = '8a5d3a28-c3c2-4b58-bd1a-008fa4a33d54' )  >>>   session . dataframe . list () \n     UUID                                    Tag                     Schema                           Created \n     ------------------------------------    --------------------    -----------------------------    -------------------------- \n     102 d8c93 - 138 a - 4 c83 - 99 f2 - 746 a033891b9    preprocessed_data : v1    number   integer ,   string   string    2017 - 12 - 30   02 : 27 : 05.747000   Note how Cebes only untag  preprocessed_data:default , while keeping  preporcessed_data:v1  unchanged. \nThis is because we did  session.dataframe.untag('preprocessed_data') , and Cebes looks for the full \ntag  preprocessed_data:default  to delete.   To delete  preprocessed_data:v1 , we need to be specific:  >>>   session . dataframe . untag ( 'preprocessed_data:v1' )    Summary   Tag  is a string given to a Dataframe, so that users can retrieve them later.   Tagged Dataframes will be persisted in Cebes.  Tag has format  NAME:VERSION . When unspecified, the default VERSION is  default .",
            "title": "Managing Dataframes"
        },
        {
            "location": "/session_df/#managing-models-and-pipelines",
            "text": "All  pipelines and models  share the same management principles as Dataframes.    For pipelines:   use  Session.pipeline.list ,  Session.pipeline.get ,  Session.pipeline.tag  and  Session.pipeline.untag  the  CEBES_CACHESPEC_PIPELINE_STORE  flag dictates how\nand when in-memory pipelines are evicted.     For models:   use  Session.model.list ,  Session.model.get ,  Session.model.tag  and  Session.model.untag  the  CEBES_CACHESPEC_MODEL_STORE  flag dictates how\nand when in-memory models are evicted.     Since they can be tagged, Cebes can handle pipelines and models as they evolve with your business logic.\nJust keep the same tag name but give them a different version, all your pipelines and models can be managed \nin a coherent way.",
            "title": "Managing Models and Pipelines"
        },
        {
            "location": "/session_api/",
            "text": "Session\n\u00b6\n\n\nSession\n(\nself\n,\n \nhost\n=\nNone\n,\n \nport\n=\n21000\n,\n \nuser_name\n=\n''\n,\n \npassword\n=\n''\n,\n \ninteractive\n=\nTrue\n)\n\n\n\n\n\n\nConstruct a new \nSession\n to the server at the given host and port, with the given user name and password.\n\n\nArguments\n\n\n\n\nhost (str)\n: Hostname of the Cebes server.\n    If \nNone\n (default), cebes will try to launch a new\n    docker container with a suitable version of Cebes server in it. Note that it requires you have\n    a working docker daemon on your machine.\n    Otherwise a string containing the host name or IP address of the Cebes server you want to connect to.\n\n\nport (int)\n: The port on which Cebes server is listening. Ignored when \nhost=None\n.\n\n\nuser_name (str)\n: Username to log in to Cebes server\n\n\npassword (str)\n: Password of the user to log in to Cebes server\n\n\ninteractive (bool)\n: whether this is an interactive session,\n    in which case some diagnosis logs will be printed to stdout.\n\n\n\n\nload_test_datasets\n\u00b6\n\n\nSession\n.\nload_test_datasets\n(\nself\n)\n\n\n\n\n\n\nLoad test datasets that come by default in Cebes server.\n\n\nReturns\n\n\ndict: a dict of datasets, currently has only one element\n:\n\n`{'cylinder_bands'\n: Dataframe}`\n\n\nread_csv\n\u00b6\n\n\nSession\n.\nread_csv\n(\nself\n,\n \npath\n,\n \noptions\n=\nNone\n)\n\n\n\n\n\n\nUpload a local CSV file to the server, and create a Dataframe out of it.\n\n\nArguments\n\n\n\n\npath (str)\n: path to the local CSV file\n\n\noptions (CsvReadOptions)\n: Additional options that dictate how the files are going to be read.\n\n\nMust be either None or a :class\n:\nCsvReadOptions\n object\n\n\n\n\nReturns\n\n\nDataframe\n: the Cebes Dataframe object created from the data source\n\n\nstop_repository_container\n\u00b6\n\n\nSession\n.\nstop_repository_container\n(\nself\n)\n\n\n\n\n\n\nStop the local pipeline repository if it is running.\n        Do nothing if there is no local repository associated with this Session.\n\n\nclose\n\u00b6\n\n\nSession\n.\nclose\n(\nself\n)\n\n\n\n\n\n\nClose this session. Will stop the Cebes container if this session was\ncreated against a local Cebes container. Otherwise it is a no-op.\n\n\nclient\n\u00b6\n\n\nReturn the client which can be used to send requests to server\n\n\nReturns\n\n\nClient\n: the client object\n\n\nmodel\n\u00b6\n\n\nReturn a helper for working with tagged and cached \nModel\n\n\nReturns\n\n\n_TagHelper\n:\n\n\nfrom_pandas\n\u00b6\n\n\nSession\n.\nfrom_pandas\n(\nself\n,\n \ndf\n)\n\n\n\n\n\n\nUpload the given \npandas\n DataFrame to the server and create a Cebes Dataframe out of it.\nTypes are preserved on a best-efforts basis.\n\n\nArguments\n\n\n\n\ndf (pd.DataFrame)\n: a pandas DataFrame object\n\n\n\n\nReturns\n\n\nDataframe\n: the Cebes Dataframe created from the data source\n\n\nread_jdbc\n\u00b6\n\n\nSession\n.\nread_jdbc\n(\nself\n,\n \nurl\n,\n \ntable_name\n,\n \nuser_name\n=\n''\n,\n \npassword\n=\n''\n)\n\n\n\n\n\n\nRead a Dataframe from a JDBC table\n\n\nArguments\n\n\n\n\nurl (str)\n: URL to the JDBC server\n\n\ntable_name (str)\n: name of the table\n\n\nuser_name (str)\n: JDBC user name\n\n\npassword (str)\n: JDBC password\n\n\n\n\nReturns\n\n\nDataframe\n: the Cebes Dataframe object created from the data source\n\n\nread_local\n\u00b6\n\n\nSession\n.\nread_local\n(\nself\n,\n \npath\n,\n \nfmt\n=\n'csv'\n,\n \noptions\n=\nNone\n)\n\n\n\n\n\n\nUpload a file from the local machine to the server, and create a :class:\nDataframe\n out of it.\n\n\nArguments\n\n\n\n\npath (str)\n: path to the local file\n\n\nfmt (str)\n: format of the file, can be \ncsv\n, \njson\n, \norc\n, \nparquet\n, \ntext\n\n\noptions\n: Additional options that dictate how the files are going to be read.\n\n\n\n\nIf specified, this can be\n:\n\n\n\n\nCsvReadOptions\n when \nfmt='csv'\n,\n\n\nJsonReadOptions\n when \nfmt='json'\n, or\n\n\nParquetReadOptions\n when \nfmt='parquet'\n\n\n\n\n\n\n\n\nOther formats do not need additional options\n\n\nReturns\n\n\nDataframe\n: the Cebes Dataframe object created from the data source\n\n\nstart_repository_container\n\u00b6\n\n\nSession\n.\nstart_repository_container\n(\nself\n,\n \nhost_port\n=\nNone\n)\n\n\n\n\n\n\nStart a local docker container running Cebes pipeline repository,\nwith the repository listening on the host at the given port.\n\n\nIf \nhost_port\n is None, a new port will be automatically allocated\nby Docker. Therefore, to maintain consistency with your pipeline tags,\nit is recommended to specify a high port, e.g. 35000 or 36000.\n\n\nIf one repository was started for this Session already, it will be returned.\n\n\nas_default\n\u00b6\n\n\nSession\n.\nas_default\n(\nself\n)\n\n\n\n\n\n\nReturns a context manager that makes this object the default session.\n\n\nUse with the \nwith\n keyword to specify that all remote calls to server\nshould be executed in this session.\n\n\n    \nsess\n \n=\n \ncb\n.\nSession\n()\n\n    \nwith\n \nsess\n.\nas_default\n():\n\n        \n....\n\n\n\n\n\n\nTo get the current default session, use \nget_default_session\n.\n\n\n\n\nThe default session is a property of the current thread. If you\ncreate a new thread, and wish to use the default session in that\nthread, you must explicitly add a \nwith sess.as_default():\n in that\nthread's function.\n\n\n\n\nReturns\n\n\nA context manager using this session as the default session.\n\n\nread_hive\n\u00b6\n\n\nSession\n.\nread_hive\n(\nself\n,\n \ntable_name\n=\n''\n)\n\n\n\n\n\n\nRead a Dataframe from Hive table of the given name\n\n\nArguments\n\n\n\n\ntable_name (str)\n: name of the Hive table to read data from\n\n\n\n\nReturns\n\n\nDataframe\n: The Cebes Dataframe object created from Hive table\n\n\nread_hdfs\n\u00b6\n\n\nSession\n.\nread_hdfs\n(\nself\n,\n \npath\n,\n \nserver\n=\nNone\n,\n \nfmt\n=\n'csv'\n,\n \noptions\n=\nNone\n)\n\n\n\n\n\n\nLoad a dataset from HDFS.\n\n\nArguments\n\n\n\n\npath (str)\n: path to the files on HDFS, e.g. \n/data/dataset1\n\n\n__server (str): Host name and port, e.g. \nhdfs://server__:9000\n\n\nfmt (str)\n: format of the files, can be \ncsv\n, \njson\n, \norc\n, \nparquet\n, \ntext\n\n\noptions\n: Additional options that dictate how the files are going to be read.\n\n\n\n\nIf specified, this can be\n:\n\n\n\n\nCsvReadOptions\n when \nfmt='csv'\n,\n\n\nJsonReadOptions\n when \nfmt='json'\n, or\n\n\nParquetReadOptions\n when \nfmt='parquet'\n\n\n\n\n\n\n\n\nOther formats do not need additional options\n\n\nReturns\n\n\nDataframe\n: the Cebes Dataframe object created from the data source\n\n\npipeline\n\u00b6\n\n\nReturn a helper for working with tagged and cached \nPipeline\n\n\nReturns\n\n\n_PipelineHelper\n:\n\n\ndataframe\n\u00b6\n\n\nReturn a helper for working with tagged and cached \nDataframe\n\n\nReturns\n\n\n_TagHelper\n:\n\n\nread_s3\n\u00b6\n\n\nSession\n.\nread_s3\n(\nself\n,\n \nbucket\n,\n \nkey\n,\n \naccess_key\n,\n \nsecret_key\n,\n \nregion\n=\nNone\n,\n \nfmt\n=\n'csv'\n,\n \noptions\n=\nNone\n)\n\n\n\n\n\n\nRead a Dataframe from files stored in Amazon S3.\n\n\nArguments\n\n\n\n\nbucket (str)\n: name of the S3 bucket\n\n\nkey (str)\n: path to the file(s) on S3\n\n\naccess_key (str)\n: Amazon S3 access key\n\n\nsecret_key (str)\n: Amazon S3 secret key\n\n\nregion (str)\n: S3 region, if needed.\n\n\nfmt (str)\n: format of the files, can be \ncsv\n, \njson\n, \norc\n, \nparquet\n, \ntext\n\n\noptions\n: Additional options that dictate how the files are going to be read.\n\n\n\n\nIf specified, this can be\n:\n\n\n\n\nCsvReadOptions\n when \nfmt='csv'\n,\n\n\nJsonReadOptions\n when \nfmt='json'\n, or\n\n\nParquetReadOptions\n when \nfmt='parquet'\n\n\n\n\n\n\n\n\nOther formats do not need additional options\n\n\nReturns\n\n\nDataframe\n: the Cebes Dataframe object created from the data source\n\n\nread_json\n\u00b6\n\n\nSession\n.\nread_json\n(\nself\n,\n \npath\n,\n \noptions\n=\nNone\n)\n\n\n\n\n\n\nUpload a local JSON file to the server, and create a Dataframe out of it.\n\n\nArguments\n\n\n\n\npath (str)\n: path to the local JSON file\n\n\noptions (JsonReadOptions)\n: Additional options that dictate how the files are going to be read.\n\n\nMust be either None or a :class\n:\nJsonReadOptions\n object\n\n\n\n\nReturns\n\n\nDataframe\n: the Cebes Dataframe object created from the data source\n\n\nCsvReadOptions\n\u00b6\n\n\nCsvReadOptions\n(\nself\n,\n \nsep\n=\n','\n,\n \nencoding\n=\n'UTF-8'\n,\n \nquote\n=\n'\"'\n,\n \nescape\n=\n'\n\\\\\n'\n,\n \ncomment\n=\nNone\n,\n \nheader\n=\nFalse\n,\n \ninfer_schema\n=\nFalse\n,\n \nignore_leading_white_space\n=\nFalse\n,\n \nnull_value\n=\nNone\n,\n \nnan_value\n=\n'NaN'\n,\n \npositive_inf\n=\n'Inf'\n,\n \nnegative_inf\n=\n'-Inf'\n,\n \ndate_format\n=\n'yyyy-MM-dd'\n,\n \ntimestamp_format\n=\n\"yyyy-MM-dd'T'HH:mm:ss.SSSZZ\"\n,\n \nmax_columns\n=\n20480\n,\n \nmax_chars_per_column\n=-\n1\n,\n \nmax_malformed_log_per_partition\n=\n10\n,\n \nmode\n=\n'PERMISSIVE'\n)\n\n\n\n\n\n\nOptions for reading CSV files.\n\n\nArguments\n\n\n\n\nsep\n: sets the single character as a separator for each field and value.\n\n\nencoding\n: decodes the CSV files by the given encoding type\n\n\nquote\n: sets the single character used for escaping quoted values where\n    the separator can be part of the value. If you would like to turn off quotations, you need to\n    set not \nnull\n but an empty string.\n\n\nescape\n: sets the single character used for escaping quotes inside an already quoted value.\n\n\ncomment\n: sets the single character used for skipping lines beginning with this character.\n    By default, it is disabled\n\n\nheader\n: uses the first line as names of columns.\n\n\ninfer_schema\n: infers the input schema automatically from data. It requires one extra pass over the data.\n\n\nignore_leading_white_space\n: defines whether or not leading whitespaces\n    from values being read should be skipped.\n\n\nnull_value\n: sets the string representation of a null value.\n    This applies to all supported types including the string type.\n\n\nnan_value\n: sets the string representation of a \"non-number\" value\n\n\npositive_inf\n: sets the string representation of a positive infinity value\n\n\nnegative_inf\n: sets the string representation of a negative infinity value\n\n\ndate_format\n: sets the string that indicates a date format.\n    Custom date formats follow the formats at \njava.text.SimpleDateFormat\n.\n    This applies to date type.\n\n\ntimestamp_format\n: sets the string that indicates a timestamp format.\n    Custom date formats follow the formats at \njava.text.SimpleDateFormat\n. This applies to timestamp type.\n\n\nmax_columns\n: defines a hard limit of how many columns a record can have\n\n\nmax_chars_per_column\n: defines the maximum number of characters allowed\n    for any given value being read. By default, it is -1 meaning unlimited length\n\n\nmax_malformed_log_per_partition\n: sets the maximum number of malformed rows\n    will be logged for each partition. Malformed records beyond this number will be ignored.\n\n\nmode\n: allows a mode for dealing with corrupt records during parsing.\n\n\nReadOptions.PERMISSIVE\n - sets other fields to \nnull\n when it meets a corrupted record.\n        When a schema is set by user, it sets \nnull\n for extra fields\n\n\nReadOptions.DROPMALFORMED\n - ignores the whole corrupted records\n\n\nReadOptions.FAILFAST\n - throws an exception when it meets corrupted records\n\n\n\n\n\n\n\n\nJsonReadOptions\n\u00b6\n\n\nJsonReadOptions\n(\nself\n,\n \nprimitives_as_string\n=\nFalse\n,\n \nprefers_decimal\n=\nFalse\n,\n \nallow_comments\n=\nFalse\n,\n \nallow_unquoted_field_names\n=\nFalse\n,\n \nallow_single_quotes\n=\nTrue\n,\n \nallow_numeric_leading_zeros\n=\nFalse\n,\n \nallow_backslash_escaping_any_character\n=\nFalse\n,\n \nmode\n=\n'PERMISSIVE'\n,\n \ncolumn_name_of_corrupt_record\n=\nNone\n,\n \ndate_format\n=\n'yyyy-MM-dd'\n,\n \ntimestamp_format\n=\n\"yyyy-MM-dd'T'HH:mm:ss.SSSZZ\"\n)\n\n\n\n\n\n\nOptions for reading Json files\n\n\nArguments\n\n\n\n\nprimitives_as_string\n: infers all primitive values as a string type\n\n\nprefers_decimal\n: infers all floating-point values as a decimal type.\n    If the values do not fit in decimal, then it infers them as doubles\n\n\nallow_comments\n: ignores Java/C++ style comment in JSON records\n\n\nallow_unquoted_field_names\n: allows unquoted JSON field names\n\n\nallow_single_quotes\n: allows single quotes in addition to double quotes\n\n\nallow_numeric_leading_zeros\n: allows leading zeros in numbers (e.g. 00012)\n\n\nallow_backslash_escaping_any_character\n: allows accepting quoting of all\n    character using backslash quoting mechanism\n\n\n\n\nmode\n: allows a mode for dealing with corrupt records during parsing.\n\n\n\n\nReadOptions.PERMISSIVE\n - sets other fields to \nnull\n when it meets a corrupted record.\n        When a schema is set by user, it sets \nnull\n for extra fields\n\n\nReadOptions.DROPMALFORMED\n - ignores the whole corrupted records\n\n\nReadOptions.FAILFAST\n - throws an exception when it meets corrupted records\n\n\n\n\n\n\n\n\ncolumn_name_of_corrupt_record\n: allows renaming the new field having malformed string\n    created by \nReadOptions.PERMISSIVE\n mode. This overrides \nspark.sql.columnNameOfCorruptRecord\n.\n\n\n\n\ndate_format\n: sets the string that indicates a date format.\n    Custom date formats follow the formats at \njava.text.SimpleDateFormat\n. This applies to date type\n\n\ntimestamp_format\n: sets the string that indicates a timestamp format.\n    Custom date formats follow the formats at \njava.text.SimpleDateFormat\n. This applies to timestamp type\n\n\n\n\nParquetReadOptions\n\u00b6\n\n\nParquetReadOptions\n(\nself\n,\n \nmerge_schema\n=\nTrue\n)\n\n\n\n\n\n\nOptions for reading Parquet files\n\n\nArguments\n\n\n\n\nmerge_schema (bool)\n: sets whether we should merge schemas collected from all Parquet part-files.",
            "title": "Session API Reference"
        },
        {
            "location": "/session_api/#session",
            "text": "",
            "title": "Session"
        },
        {
            "location": "/session_api/#load_test_datasets",
            "text": "",
            "title": "load_test_datasets"
        },
        {
            "location": "/session_api/#read_csv",
            "text": "",
            "title": "read_csv"
        },
        {
            "location": "/session_api/#stop_repository_container",
            "text": "",
            "title": "stop_repository_container"
        },
        {
            "location": "/session_api/#close",
            "text": "",
            "title": "close"
        },
        {
            "location": "/session_api/#client",
            "text": "",
            "title": "client"
        },
        {
            "location": "/session_api/#model",
            "text": "",
            "title": "model"
        },
        {
            "location": "/session_api/#from_pandas",
            "text": "",
            "title": "from_pandas"
        },
        {
            "location": "/session_api/#read_jdbc",
            "text": "",
            "title": "read_jdbc"
        },
        {
            "location": "/session_api/#read_local",
            "text": "",
            "title": "read_local"
        },
        {
            "location": "/session_api/#start_repository_container",
            "text": "",
            "title": "start_repository_container"
        },
        {
            "location": "/session_api/#as_default",
            "text": "",
            "title": "as_default"
        },
        {
            "location": "/session_api/#read_hive",
            "text": "",
            "title": "read_hive"
        },
        {
            "location": "/session_api/#read_hdfs",
            "text": "",
            "title": "read_hdfs"
        },
        {
            "location": "/session_api/#pipeline",
            "text": "",
            "title": "pipeline"
        },
        {
            "location": "/session_api/#dataframe",
            "text": "",
            "title": "dataframe"
        },
        {
            "location": "/session_api/#read_s3",
            "text": "",
            "title": "read_s3"
        },
        {
            "location": "/session_api/#read_json",
            "text": "",
            "title": "read_json"
        },
        {
            "location": "/session_api/#csvreadoptions",
            "text": "",
            "title": "CsvReadOptions"
        },
        {
            "location": "/session_api/#jsonreadoptions",
            "text": "",
            "title": "JsonReadOptions"
        },
        {
            "location": "/session_api/#parquetreadoptions",
            "text": "",
            "title": "ParquetReadOptions"
        },
        {
            "location": "/dataframe_concepts/",
            "text": "Built on top of Spark, Cebes provides a rich set of APIs for loading, cleaning, transforming data at scale.\nThis document describes the basic concepts of Cebes Dataframe.\n\n\n\n\nStorage types, Variable types and Schema\n\u00b6\n\n\nA Cebes Dataframe can be thought of as a big data table, or a collection of columns, \nwhere each column has a name and a \ndata type\n. Elements in the same column have the same data type.\n\n\nHowever, data type is a super confusing concept, for both human and machine. Cebes, therefore, makes \ndistinction between \nStorage types\n and \nVariable types\n:\n\n\n\n\nStorage type\n describes how the data is stored under the hood. This ranges from atomic types \n like \nBooleanType\n, \nByteType\n, \nLongType\n to complicated types like \nArray[T]\n, \nMap[K, V]\n, \n \nVector\n, \nStruct\n, ...\n\n\nVariable type\n describes the role of the variable in analysis. This is closer to the semantic\n that human can assign to variables. For example, \"age\" is a \nDiscrete\n variable, but can be stored\n as either \nIntegerType\n or \nLongType\n.\n\n\n\n\nThe following table shows variable types supported in Cebes, and their corresponding Storage type:\n\n\n\n\n\n\n\n\nVariable types\n\n\nCorresponding storage types\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDiscrete\n\n\nByteType, ShortType, IntegerType, LongType\n\n\nDiscrete numerical variables, e.g. Age, Year\n\n\n\n\n\n\nContinuous\n\n\nFloatType, DoubleType\n\n\nContinuous numerical variables, e.g. Weight, Height\n\n\n\n\n\n\nNomial\n\n\nStringType, BooleanType, ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType, DateType, TimestampType\n\n\nCategorical variables without ranks, e.g. Gender, Job\n\n\n\n\n\n\nOrdinal\n\n\nStringType, BooleanType, ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType, DateType, TimestampType\n\n\nCategorical variables with a rank/order, e.g. Educational level\n\n\n\n\n\n\nText\n\n\nStringType\n\n\nLong pieces of text, e.g. Biography\n\n\n\n\n\n\nDateTime\n\n\nDateType, TimestampType, CalendarIntervalType\n\n\n\n\n\n\n\n\nArray\n\n\nBinaryType, VectorType, Array[_]Type\n\n\n\n\n\n\n\n\nMap\n\n\nMap[_]Type\n\n\n\n\n\n\n\n\nStruct\n\n\nStruct[_]Type\n\n\n\n\n\n\n\n\n\n\nThose are all the types in the Cebes' type system. Most of the time, when you use \npycebes\n, it \nwill try its best to convert those types into corresponding Python types.\n\n\nVariable types give Cebes hints about how to handle the variables when using them to train \nMachine Learning models, while Storage types instruct Cebes how to store the data and do type\nconversions when needed. \n\n\nFinally, each Dataframe has a \nSchema\n, which is a list of schema definition of the columns\nin the Dataframe. For each column, there is a name, a storage type and a variable type. The \nschema can be accessed by the \nschema\n field on the \nDataframe\n object:\n\n\n>>>\n \ndf\n.\nschema\n\n    \nSchema\n(\nfields\n=\n[\nSchemaField\n(\nname\n=\n'timestamp'\n,\nstorage_type\n=\nLONG\n,\nvariable_type\n=\nDISCRETE\n),\n\n                   \nSchemaField\n(\nname\n=\n'cylinder_number'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n),\n\n                   \nSchemaField\n(\nname\n=\n'customer'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n),\n\n                   \nSchemaField\n(\nname\n=\n'job_number'\n,\nstorage_type\n=\nINTEGER\n,\nvariable_type\n=\nDISCRETE\n),\n\n                   \n...\n,\n\n                   \nSchemaField\n(\nname\n=\n'band_type'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n)])\n\n\n\n\n\n\nSee \nthis section\n for APIs to work with Dataframe \nstorage types and variable types.\n\n\n\n\n\n\nDataframe tags and IDs\n\u00b6\n\n\nEach Dataframe in Cebes is assigned a unique identifier. Most operations on Dataframe will \ngive a new Dataframe object with a new ID. The number of Dataframes, therefore, can grow \nquite fast during the course of your data exploration.\n\n\nBy default, Cebes keeps all Dataframes in memory, and evict the ones that are old or have \nnot been used for a while. Evicted Dataframes are lost forever, unless you choose to persist \nthem, by assigning them a tag. Tagged Dataframes will be persisted and can be retrieved later\nusing either the tag or the Dataframe ID.\n\n\nSee \nthis section\n for information on how to work with Dataframe tags.",
            "title": "Key concepts"
        },
        {
            "location": "/dataframe_concepts/#storage-types-variable-types-and-schema",
            "text": "A Cebes Dataframe can be thought of as a big data table, or a collection of columns, \nwhere each column has a name and a  data type . Elements in the same column have the same data type.  However, data type is a super confusing concept, for both human and machine. Cebes, therefore, makes \ndistinction between  Storage types  and  Variable types :   Storage type  describes how the data is stored under the hood. This ranges from atomic types \n like  BooleanType ,  ByteType ,  LongType  to complicated types like  Array[T] ,  Map[K, V] , \n  Vector ,  Struct , ...  Variable type  describes the role of the variable in analysis. This is closer to the semantic\n that human can assign to variables. For example, \"age\" is a  Discrete  variable, but can be stored\n as either  IntegerType  or  LongType .   The following table shows variable types supported in Cebes, and their corresponding Storage type:     Variable types  Corresponding storage types  Description      Discrete  ByteType, ShortType, IntegerType, LongType  Discrete numerical variables, e.g. Age, Year    Continuous  FloatType, DoubleType  Continuous numerical variables, e.g. Weight, Height    Nomial  StringType, BooleanType, ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType, DateType, TimestampType  Categorical variables without ranks, e.g. Gender, Job    Ordinal  StringType, BooleanType, ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType, DateType, TimestampType  Categorical variables with a rank/order, e.g. Educational level    Text  StringType  Long pieces of text, e.g. Biography    DateTime  DateType, TimestampType, CalendarIntervalType     Array  BinaryType, VectorType, Array[_]Type     Map  Map[_]Type     Struct  Struct[_]Type      Those are all the types in the Cebes' type system. Most of the time, when you use  pycebes , it \nwill try its best to convert those types into corresponding Python types.  Variable types give Cebes hints about how to handle the variables when using them to train \nMachine Learning models, while Storage types instruct Cebes how to store the data and do type\nconversions when needed.   Finally, each Dataframe has a  Schema , which is a list of schema definition of the columns\nin the Dataframe. For each column, there is a name, a storage type and a variable type. The \nschema can be accessed by the  schema  field on the  Dataframe  object:  >>>   df . schema \n     Schema ( fields = [ SchemaField ( name = 'timestamp' , storage_type = LONG , variable_type = DISCRETE ), \n                    SchemaField ( name = 'cylinder_number' , storage_type = STRING , variable_type = TEXT ), \n                    SchemaField ( name = 'customer' , storage_type = STRING , variable_type = TEXT ), \n                    SchemaField ( name = 'job_number' , storage_type = INTEGER , variable_type = DISCRETE ), \n                    ... , \n                    SchemaField ( name = 'band_type' , storage_type = STRING , variable_type = TEXT )])   See  this section  for APIs to work with Dataframe \nstorage types and variable types.",
            "title": "Storage types, Variable types and Schema"
        },
        {
            "location": "/dataframe_concepts/#dataframe-tags-and-ids",
            "text": "Each Dataframe in Cebes is assigned a unique identifier. Most operations on Dataframe will \ngive a new Dataframe object with a new ID. The number of Dataframes, therefore, can grow \nquite fast during the course of your data exploration.  By default, Cebes keeps all Dataframes in memory, and evict the ones that are old or have \nnot been used for a while. Evicted Dataframes are lost forever, unless you choose to persist \nthem, by assigning them a tag. Tagged Dataframes will be persisted and can be retrieved later\nusing either the tag or the Dataframe ID.  See  this section  for information on how to work with Dataframe tags.",
            "title": "Dataframe tags and IDs"
        },
        {
            "location": "/dataframe_guide/",
            "text": "Assume that you already \nloaded data into Cebes\n. This document describes\n functions for doing analytics on Dataframes.\n\n\nWorking with types\n\u00b6\n\n\nSee \nthis section\n for an overview\nof types and schema in Cebes.\n\n\nIn \npycebes\n, you can access a column using the \n[]\n notation or \n.<column name>\n notation. The\nschema of the column can be accessed by \nschema[<column name>]\n:\n\n\n>>>\n \ndf\n[\n'job_number'\n]\n\n    \nColumn\n(\nexpr\n=\nSparkPrimitiveExpression\n(\ncol_name\n=\n'job_number'\n,\ndf_id\n=\n'...'\n))\n\n\n\n>>>\n \ndf\n.\njob_number\n\n    \nColumn\n(\nexpr\n=\nSparkPrimitiveExpression\n(\ncol_name\n=\n'job_number'\n,\ndf_id\n=\n'...'\n))\n\n\n\n>>>\n \ndf\n.\nschema\n[\n'job_number'\n]\n\n    \nSchemaField\n(\nname\n=\n'job_number'\n,\nstorage_type\n=\nINTEGER\n,\nvariable_type\n=\nDISCRETE\n)\n\n\n\n\n\n\nThis says \njob_number\n is a column with storage type \nINTEGER\n and variable type \nDISCRETE\n.\n\n\nStorage type casting\n: You can cast a column into a different Storage type like this:\n\n\n>>>\n \ndf2\n \n=\n \ndf\n.\nwith_storage_type\n(\ndf\n.\njob_number\n,\n \ncb\n.\nStorageTypes\n.\nLONG\n)\n\n\n\n>>>\n \ndf2\n.\nschema\n[\n'job_number'\n]\n\n    \nSchemaField\n(\nname\n=\n'job_number'\n,\nstorage_type\n=\nLONG\n,\nvariable_type\n=\nDISCRETE\n)\n\n\n\n>>>\n \ndf\n.\nschema\n[\n'job_number'\n]\n\n    \nSchemaField\n(\nname\n=\n'job_number'\n,\nstorage_type\n=\nINTEGER\n,\nvariable_type\n=\nDISCRETE\n)\n\n\n\n\n\n\nNote that \ndf2\n is a different Dataframe from \ndf\n. In fact they have different IDs.\n\n\nAnother way to do the same thing is to use Cebes SQL APIs:\n\n\n>>>\n \ndf2\n \n=\n \ndf\n.\nwith_column\n(\n'job_number'\n,\n \ndf\n.\njob_number\n.\ncast\n(\ncb\n.\nStorageTypes\n.\nLONG\n))\n\n\n\n\n\n\nHere we used \nwith_column\n and \ncast\n to convert the \njob_number\n column from INTEGER to LONG.\nWe will explore other Cebes SQL APIs later.\n\n\nVariable type casting\n: Similarly, you can set the variable type of a column:\n\n\n>>>\n \ndf2\n \n=\n \ndf\n.\nwith_variable_type\n(\ndf\n.\njob_number\n,\n \ncb\n.\nVariableTypes\n.\nORDINAL\n)\n\n\n\n>>>\n \ndf2\n.\nschema\n[\n'job_number'\n]\n\n    \nSchemaField\n(\nname\n=\n'job_number'\n,\nstorage_type\n=\nINTEGER\n,\nvariable_type\n=\nORDINAL\n)\n\n\n\n>>>\n \ndf\n.\nschema\n[\n'job_number'\n]\n\n    \nSchemaField\n(\nname\n=\n'job_number'\n,\nstorage_type\n=\nINTEGER\n,\nvariable_type\n=\nDISCRETE\n)\n\n\n\n\n\n\n\n\nBasic operations with Dataframes\n\u00b6\n\n\nOne of the most frequently used functions of Dataframe is \n\nshow\n, \nwhich shows essential information of a Dataframe like its ID, shape as well as several rows taken from it:\n\n\n>>>\n \ndf\n.\nshow\n()\n\n    \nID\n:\n \n67\nb827d7\n-\n869\nb\n-\n48\nd9\n-\n9364\n-\n5\na6d3bc2e99c\n\n    \nShape\n:\n \n(\n540\n,\n \n40\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \ntimestamp\n \ncylinder_number\n \ncustomer\n  \njob_number\n \ngrain_screened\n \nink_color\n   \n...\n\n    \n0\n   \n19910108\n            \nX126\n  \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n   \n...\n\n    \n1\n   \n19910109\n            \nX266\n  \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n   \n...\n\n    \n2\n   \n19910104\n              \nB7\n   \nMODMAT\n       \n47201\n            \nYES\n       \nKEY\n   \n...\n\n    \n3\n   \n19910104\n            \nT133\n   \nMASSEY\n       \n39039\n            \nYES\n       \nKEY\n   \n...\n\n    \n4\n   \n19910111\n             \nJ34\n    \nKMART\n       \n37351\n             \nNO\n       \nKEY\n   \n...\n\n\n\n\n\n\nOther information of a Dataframe can be inspected quite intuitively as in the following sample code:\n\n\n# The unique ID of the Dataframe\n\n\n>>>\n \ndf\n.\nid\n\n    \n'67b827d7-869b-48d9-9364-5a6d3bc2e99c'\n\n\n\n# Number of rows\n\n\n>>>\n \nlen\n(\ndf\n)\n\n    \n540\n\n\n\n# Number of columns\n\n\n>>>\n \nlen\n(\ndf\n.\ncolumns\n)\n\n    \n40\n\n\n\n>>>\n \ndf\n.\nshape\n\n    \n(\n540\n,\n \n40\n)\n\n\n\n# list of column names\n\n\n>>>\n \ndf\n.\ncolumns\n\n    \n[\n'timestamp'\n,\n\n     \n'cylinder_number'\n,\n\n     \n...\n,\n\n     \n'band_type'\n]\n\n\n\n# the schema of this Dataframe\n\n\n>>>\n \ndf\n.\nschema\n\n    \nSchema\n(\nfields\n=\n[\nSchemaField\n(\nname\n=\n'timestamp'\n,\nstorage_type\n=\nLONG\n,\nvariable_type\n=\nDISCRETE\n),\n\n                   \nSchemaField\n(\nname\n=\n'cylinder_number'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n),\n\n                   \n...\n\n                   \n,\nSchemaField\n(\nname\n=\n'band_type'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n)])\n\n\n\n# Schema information of a column can be accessed by the [] notation\n\n\n>>>\n \ndf\n.\nschema\n[\n'cylinder_number'\n]\n\n    \nSchemaField\n(\nname\n=\n'cylinder_number'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n)\n\n\n\n# A column in the Dataframe can be accessed using the `dot` notation\n\n\n>>>\n \ndf\n.\ntimestamp\n\n    \nColumn\n(\nexpr\n=\nSparkPrimitiveExpression\n(\ncol_name\n=\n'timestamp'\n,\ndf_id\n=\n'67b827d7-869b-48d9-9364-5a6d3bc2e99c'\n))\n\n\n\n# or the [] notation\n\n\n>>>\n \ndf\n[\n'timestamp'\n]\n\n    \nColumn\n(\nexpr\n=\nSparkPrimitiveExpression\n(\ncol_name\n=\n'timestamp'\n,\ndf_id\n=\n'67b827d7-869b-48d9-9364-5a6d3bc2e99c'\n))\n\n\n\n\n\n\n\n\nSample a Dataframe\n\u00b6\n\n\nThere are two ways to take a sample from a Dataframe, depending on how you want the result:\n\n\n\n\ntake\n returns a sample as a \nDataSample\n \nobject, which is downloaded to the client, and can be converted into, for example, pandas DataFrame.\n\n\nsample\n returns a sample as another \n\nDataframe\n object. The data remains on the Cebes server.\n\n\n\n\nFurthermore, \nsample\n is designed to return a random subset of the Dataframe, while \ntake\n is not necessarily \nrandom.\n\n\n>>>\n \nlocal_sample\n \n=\n \ndf\n.\ntake\n(\n10\n)\n\n\n\n>>>\n \nlocal_sample\n\n    \nDataSample\n(\nschema\n=\nSchema\n(\nfields\n=\n[\nSchemaField\n(\nname\n=\n'timestamp'\n,\nstorage_type\n=\nLONG\n,\nvariable_type\n=\nDISCRETE\n),\n\n                                     \nSchemaField\n(\nname\n=\n'cylinder_number'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n),\n\n                                     \n...\n,\n\n                                     \nSchemaField\n(\nname\n=\n'band_type'\n,\nstorage_type\n=\nSTRING\n,\nvariable_type\n=\nTEXT\n)]))\n\n\n>>>\n \nlocal_sample\n.\nto_pandas\n()\n\n       \ntimestamp\n \ncylinder_number\n    \ncustomer\n  \njob_number\n \ngrain_screened\n \nink_color\n  \n...\n\n    \n0\n   \n19910108\n            \nX126\n     \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n  \n...\n\n    \n1\n   \n19910109\n            \nX266\n     \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n  \n...\n\n    \n2\n   \n19910104\n              \nB7\n      \nMODMAT\n       \n47201\n            \nYES\n       \nKEY\n  \n...\n\n\n    \n[\n10\n \nrows\n \nx\n \n40\n \ncolumns\n]\n\n\n\n>>>\n \ndf2\n \n=\n \ndf\n.\nsample\n(\nprob\n=\n0.1\n,\n \nreplacement\n=\nTrue\n,\n \nseed\n=\n42\n)\n\n\n\n>>>\n \ndf2\n\n    \nDataframe\n(\nid\n=\n'ac712897-c72a-44d3-816f-7e19fa008fcb'\n)\n\n\n\n>>>\n \nlen\n(\ndf2\n)\n\n    \n55\n\n\n\n\n\n\n\n\nOther basic operations may include:\n\n\n\n\nSort a Dataframe with \nsort\n\n\nDrop a column with \ndrop\n\n\nDrop duplicated rows with \ndrop_duplicates\n\n\n\n\nIt hardly gets more intuitive.\n\n\n\n\nNA handling\n\u00b6\n\n\n\n\nDrop rows having empty cells with \ndropna\n\n\nFill empty cells with \nfillna\n\n\n\n\n\n\nStatistical functions\n\u00b6\n\n\n\n\nQuantile\n\n\nCorrelation\n\n\nCovariance\n\n\nCross-table\n\n\nFrequent Itemsets\n\n\nAdvanced sampling\n\n\n\n\n\n\nBasic SQL operations\n\u00b6\n\n\nMost of the operations above could have been expressed in SQL. Thanks to Spark, SQL expressions are a core\npart in Cebes' data exploration APIs. We review here a few main functions, and give pointers to others.\n\n\nThe basic filter and projection operations can be done using \n\nselect\n and\n\nwhere\n. For example,\nhere is a not-so-trivial projection that involves creating an array of values:\n\n\n>>>\n \ndf\n.\ncolumns\n\n    \n[\n...\n,\n\n     \n'customer'\n,\n\n     \n...\n,\n\n     \n'esa_voltage'\n,\n\n     \n'esa_amperage'\n]\n\n\n\n>>>\n \ndf2\n \n=\n \ndf\n.\nselect\n(\ndf\n.\ncustomer\n,\n \ncb\n.\narray\n(\ndf\n.\nesa_voltage\n,\n \ndf\n.\nesa_amperage\n)\n.\nalias\n(\n'esa_array'\n),\n \n                    \ndf\n.\nesa_voltage\n,\n \ndf\n.\nesa_amperage\n)\n.\nwhere\n(\ndf\n.\ncustomer\n.\nisin\n([\n'TVGUIDE'\n,\n \n'MODMAT'\n]))\n\n\n\n>>>\n \ndf2\n.\nshow\n()\n\n    \nID\n:\n \ncd1433ff\n-\n0\nc18\n-\n4920\n-\n9699\n-\nbd7ec7eacd88\n\n    \nShape\n:\n \n(\n99\n,\n \n4\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n      \ncustomer\n   \nesa_array\n  \nesa_voltage\n  \nesa_amperage\n\n    \n0\n  \nTVGUIDE\n  \n[\n0.0\n,\n \n0.0\n]\n          \n0.0\n           \n0.0\n\n    \n1\n  \nTVGUIDE\n  \n[\n0.0\n,\n \n0.0\n]\n          \n0.0\n           \n0.0\n\n    \n2\n   \nMODMAT\n  \n[\n0.0\n,\n \n0.0\n]\n          \n0.0\n           \n0.0\n\n    \n3\n   \nMODMAT\n  \n[\n1.5\n,\n \n0.0\n]\n          \n1.5\n           \n0.0\n\n    \n4\n   \nMODMAT\n  \n[\n0.0\n,\n \n0.0\n]\n          \n0.0\n           \n0.0\n\n\n\n\n\n\nCombine expressions\n\u00b6\n\n\nTo combine multiple boolean expressions, use boolean operators \n&\n, \n|\n and \n~\n on the expressions.\n\n\nNote that Python's built-in boolean operators like \nand\n, \nor\n, \nnot\n are not usable because they \ncannot be overridden to provide the functionality we want. Trying to use \nand\n, \nor\n, \nnot\n on \nCebes expressions will throw an exception.\n\n\n>>>\n \ndf\n.\nselect\n(\ndf\n.\nwax\n,\n \ndf\n.\nhardener\n)\n.\nwhere\n((\ndf\n.\nwax\n \n<\n \n2.8\n)\n \n&\n \n~\n(\ndf\n.\nhardener\n \n>\n \n0.8\n))\n.\nshow\n()\n\n    \nID\n:\n \n0\nba144ef\n-\n22e4\n-\n48\nd6\n-\n8774\n-\n7\neef95b57a40\n\n    \nShape\n:\n \n(\n128\n,\n \n2\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \nwax\n  \nhardener\n\n    \n0\n  \n2.5\n       \n0.7\n\n    \n1\n  \n2.3\n       \n0.6\n\n    \n2\n  \n2.5\n       \n0.8\n\n    \n3\n  \n2.5\n       \n0.6\n\n    \n4\n  \n2.5\n       \n0.8\n\n\n\n>>>\n \ndf\n.\nselect\n(\ndf\n.\nwax\n,\n \ndf\n.\nhardener\n)\n.\nwhere\n((\ndf\n.\nwax\n \n<\n \n2.8\n)\n \nand\n \nnot\n(\ndf\n.\nhardener\n \n>\n \n0.8\n))\n.\nshow\n()\n\n    \n...\n\n    \nValueError\n:\n \nCannot\n \nconvert\n \ncolumn\n \ninto\n \nbool\n:\n \nplease\n \nuse\n \n'&'\n \nfor\n \n'and'\n,\n \n'|'\n \nfor\n \n'or'\n,\n \n    \n'~'\n \nfor\n \n'not'\n \nwhen\n \nbuilding\n \nDataframe\n \nboolean\n \nexpressions\n.\n\n\n\n\n\n\nBitwise operations can be constructed as follows:\n\n\n>>>\n \ndf\n.\nselect\n(\ndf\n.\nroller_durometer\n,\n \ndf\n.\ncurrent_density\n,\n \n              \ndf\n.\nroller_durometer\n.\nbitwise_or\n(\ndf\n.\ncurrent_density\n),\n \n              \ndf\n.\nroller_durometer\n.\nbitwise_and\n(\ncb\n.\nbitwise_not\n(\ndf\n.\ncurrent_density\n))\n.\nalias\n(\n'complicated_expr'\n))\n.\nshow\n()\n\n\n    \nID\n:\n \n2\nccc186d\n-\n6\na7c\n-\n45\nf6\n-\n8572\n-\nbf0e753442eb\n\n    \nShape\n:\n \n(\n540\n,\n \n4\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \nroller_durometer\n  \ncurrent_density\n  \n(\nroller_durometer\n \n|\n \ncurrent_density\n)\n  \\\n    \n0\n                \n34\n               \n40\n                                    \n42\n   \n    \n1\n                \n34\n               \n40\n                                    \n42\n   \n    \n2\n                \n40\n               \n40\n                                    \n40\n   \n    \n3\n                \n40\n               \n40\n                                    \n40\n   \n    \n4\n                \n35\n               \n40\n                                    \n43\n   \n\n       \ncomplicated_expr\n\n    \n0\n                 \n2\n  \n    \n1\n                 \n2\n  \n    \n2\n                 \n0\n  \n    \n3\n                 \n0\n  \n    \n4\n                 \n3\n\n\n\n\n\n\nHere we use some functions on the expressions like \ndf.roller_durometer.bitwise_or()\n, but Cebes also provides\nmany functions in the functional form \ncb.bitwise_not()\n. See \nthis page\n \nfor the full list of functions provided in Cebes.\n\n\nOther basic SQL operations include \nlimit\n,\n \nintersect\n,\n \nunion\n, \n \nsubtract\n, \n \nalias\n, ...\n\n\n\n\nJoins\n\u00b6\n\n\nJoins of two Dataframes can be done with \njoin\n,\nwhich supports generic join conditions and different types of joins. \n\n\nIf one Dataframe is significantly smaller than the other in a join, you can mark it \n\nbroadcast\n, in which \ncase the join might be executed more efficient.\n\n\n\n\nGrouping\n\u00b6\n\n\nGrouping can be done with \ngroupby\n, \n\nrollup\n and \n\ncube\n are also supported.\nAfter \ngroupby()\n, various way to aggregate the results are provided, including functions \nin \nGroupedDataframe\n and \n\naggregation functions\n.\n\n\nWhen grouping on multiple columns, rollup and cube are different in the way they construct the tuples.\nFor example, when grouping on 3 columns, \nROLLUP (YEAR, MONTH, DAY)\n will give the following outputs:\n\n\nYEAR, MONTH, DAY\nYEAR, MONTH\nYEAR\n()\n\n\n\n\n\nwhile \nCUBE (YEAR, MONTH, DAY)\n gives the following:\n\n\nYEAR, MONTH, DAY\nYEAR, MONTH\nYEAR, DAY\nYEAR\nMONTH, DAY\nMONTH\nDAY\n()\n\n\n\n\n\nWhen ... Otherwise ...\n\u00b6\n\n\nA particularly useful API is to compute a column based on values of other columns, much like \nconditional expressions in programming languages. \n\n\nFor example, here is how to encode the \ngender\n string column into numeric:\n\n\n>>>\n \npeople\n.\nselect\n(\ncb\n.\nwhen\n(\npeople\n.\ngender\n \n==\n \n'male'\n,\n \n0\n)\n\n            \n.\nwhen\n(\npeople\n.\ngender\n \n==\n \n'female'\n,\n \n1\n)\n\n            \n.\notherwise\n(\n2\n)\n.\nalias\n(\n'gender_int'\n))\n\n\n\n\n\n\nIf \notherwise()\n is not specified, null value will be used. See \n\nwhen\n and \n\notherwise\n for more information.\n\n\nWindowing on time-series\n\u00b6\n\n\nCebes has the \nwindow\n function for windowing on time-series.\n\n\nIn the example below, the \ntimestamp\n column contains the date in format \nyyyyMMdd\n. We convert \nit into Unix timestamp using the \nunix_timestamp\n function:\n\n\n# convert the `timestamp` column from \"yyyyMMdd\" into Unix timestamp, and cast it to type TIMESTAMP\n\n\n\n>>>\n \ndf2\n \n=\n \ndf\n.\nwith_column\n(\n'timestamp_unix'\n,\n \n\n...\n:\n    \ncb\n.\nunix_timestamp\n(\ndf\n.\ntimestamp\n.\ncast\n(\ncb\n.\nStorageTypes\n.\nSTRING\n),\n \npattern\n=\n'yyyyMMdd'\n)\n.\n\\\n\n...\n:\n    \ncast\n(\ncb\n.\nStorageTypes\n.\nTIMESTAMP\n))\n\n\n\n\n>>>\n \ndf2\n.\nshow\n()\n\n    \nID\n:\n \nb03c92b0\n-\nf97a\n-\n4\nd21\n-\n94e5\n-\n1\nb039fc2c038\n\n    \nShape\n:\n \n(\n540\n,\n \n41\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \ntimestamp\n \ncylinder_number\n \ncustomer\n  \njob_number\n \ngrain_screened\n \nink_color\n  \\\n    \n0\n   \n19910108\n            \nX126\n  \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n   \n    \n1\n   \n19910109\n            \nX266\n  \nTVGUIDE\n       \n25503\n            \nYES\n       \nKEY\n   \n    \n2\n   \n19910104\n              \nB7\n   \nMODMAT\n       \n47201\n            \nYES\n       \nKEY\n   \n    \n3\n   \n19910104\n            \nT133\n   \nMASSEY\n       \n39039\n            \nYES\n       \nKEY\n   \n    \n4\n   \n19910111\n             \nJ34\n    \nKMART\n       \n37351\n             \nNO\n       \nKEY\n   \n\n      \nproof_on_ctd_ink\n \nblade_mfg\n \ncylinder_division\n \npaper_type\n       \n...\n        \\\n    \n0\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n    \n1\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n    \n2\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n    \n3\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n    \n4\n              \nYES\n    \nBENTON\n          \nGALLATIN\n   \nUNCOATED\n       \n...\n         \n\n      \nesa_voltage\n \nesa_amperage\n  \nwax\n \nhardener\n \nroller_durometer\n  \ncurrent_density\n  \\\n    \n0\n         \n0.0\n          \n0.0\n  \n2.5\n      \n1.0\n               \n34\n               \n40\n   \n    \n1\n         \n0.0\n          \n0.0\n  \n2.5\n      \n0.7\n               \n34\n               \n40\n   \n    \n2\n         \n0.0\n          \n0.0\n  \n2.8\n      \n0.9\n               \n40\n               \n40\n   \n    \n3\n         \n0.0\n          \n0.0\n  \n2.5\n      \n1.3\n               \n40\n               \n40\n   \n    \n4\n         \n5.0\n          \n0.0\n  \n2.3\n      \n0.6\n               \n35\n               \n40\n   \n\n       \nanode_space_ratio\n \nchrome_content\n \nband_type\n  \ntimestamp_unix\n  \n    \n0\n         \n105.000000\n          \n100.0\n      \nband\n       \n663292800\n  \n    \n1\n         \n105.000000\n          \n100.0\n    \nnoband\n       \n663379200\n  \n    \n2\n         \n103.870003\n          \n100.0\n    \nnoband\n       \n662947200\n  \n    \n3\n         \n108.059998\n          \n100.0\n    \nnoband\n       \n662947200\n  \n    \n4\n         \n106.669998\n          \n100.0\n    \nnoband\n       \n663552000\n  \n\n    \n[\n5\n \nrows\n \nx\n \n41\n \ncolumns\n]\n\n\n\n\n\n\nWe then use \nwindow\n on the timestamp column, and \ngroupby\n on those groups to find the \nnumber of transactions happening every week:\n\n\n>>>\n \ndf3\n \n=\n \ndf2\n.\ngroupby\n(\ncb\n.\nwindow\n(\ndf2\n.\ntimestamp_unix\n,\n \n'7 days'\n,\n \n'7 days'\n))\n.\ncount\n()\n\n\n\n>>>\n \ndf3\n.\nschema\n\n    \nSchema\n(\nfields\n=\n[\nSchemaField\n(\nname\n=\n'window'\n,\nstorage_type\n=\nStruct\n[\nTIMESTAMP\n,\nTIMESTAMP\n],\nvariable_type\n=\nSTRUCT\n),\n\n                   \nSchemaField\n(\nname\n=\n'count'\n,\nstorage_type\n=\nLONG\n,\nvariable_type\n=\nDISCRETE\n)])\n\n\n\n>>>\n \ndf3\n.\nshow\n()\n\n    \nID\n:\n \n22\nc45afe\n-\n1488\n-\n4562\n-\nac4b\n-\n5\nd4767e8a6da\n\n    \nShape\n:\n \n(\n122\n,\n \n2\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n                                           \nwindow\n  \ncount\n\n    \n0\n  \n{\n'end'\n:\n \n676166400.0\n,\n \n'start'\n:\n \n675561600.0\n}\n      \n8\n\n    \n1\n  \n{\n'end'\n:\n \n651974400.0\n,\n \n'start'\n:\n \n651369600.0\n}\n      \n4\n\n    \n2\n  \n{\n'end'\n:\n \n701568000.0\n,\n \n'start'\n:\n \n700963200.0\n}\n      \n1\n\n    \n3\n  \n{\n'end'\n:\n \n707011200.0\n,\n \n'start'\n:\n \n706406400.0\n}\n      \n1\n\n    \n4\n  \n{\n'end'\n:\n \n714873600.0\n,\n \n'start'\n:\n \n714268800.0\n}\n      \n2\n\n\n\n\n\n\nThe \nwindow\n column now has type \nStruct\n, containing the start and end timestamp. We have \nthe correct results now, but to make it looks a bit better, wee can \nconvert those timestamps into the \ndd/MM/yyyy\n format, and sort the data:\n\n\n>>>\n \ndf4\n \n=\n \ndf3\n.\nwith_column\n(\n'start'\n,\n \ncb\n.\ndate_format\n(\ndf3\n[\n'window.start'\n],\n \n'dd/MM/yyyy'\n))\n.\n\\\n    \nwith_column\n(\n'end'\n,\n \ncb\n.\ndate_format\n(\ndf3\n[\n'window.end'\n],\n \n'dd/MM/yyyy'\n))\n.\n\\\n    \nwith_column\n(\n'window_start'\n,\n \ndf3\n[\n'window.start'\n])\n.\nsort\n(\n'window_start'\n)\n\n\n\n>>>\n \ndf4\n \n=\n \ndf4\n.\ndrop\n(\ndf4\n.\nwindow_start\n)\n\n\n\n>>>\n \ndf4\n.\nshow\n()\n\n   \nID\n:\n \n85\nf03b43\n-\n7731\n-\n487\na\n-\na01c\n-\ne5d7ae488c4d\n\n   \nShape\n:\n \n(\n122\n,\n \n4\n)\n\n   \nSample\n \n5\n \nrows\n:\n\n                                          \nwindow\n  \ncount\n       \nstart\n         \nend\n\n   \n0\n  \n{\n'end'\n:\n \n639273600.0\n,\n \n'start'\n:\n \n638668800.0\n}\n      \n1\n  \n29\n/\n03\n/\n1990\n  \n05\n/\n04\n/\n1990\n\n   \n1\n  \n{\n'end'\n:\n \n639878400.0\n,\n \n'start'\n:\n \n639273600.0\n}\n      \n2\n  \n05\n/\n04\n/\n1990\n  \n12\n/\n04\n/\n1990\n\n   \n2\n  \n{\n'end'\n:\n \n640483200.0\n,\n \n'start'\n:\n \n639878400.0\n}\n      \n5\n  \n12\n/\n04\n/\n1990\n  \n19\n/\n04\n/\n1990\n\n   \n3\n  \n{\n'end'\n:\n \n641088000.0\n,\n \n'start'\n:\n \n640483200.0\n}\n      \n1\n  \n19\n/\n04\n/\n1990\n  \n26\n/\n04\n/\n1990\n\n   \n4\n  \n{\n'end'\n:\n \n641692800.0\n,\n \n'start'\n:\n \n641088000.0\n}\n      \n2\n  \n26\n/\n04\n/\n1990\n  \n03\n/\n05\n/\n1990",
            "title": "User guide"
        },
        {
            "location": "/dataframe_guide/#working-with-types",
            "text": "See  this section  for an overview\nof types and schema in Cebes.  In  pycebes , you can access a column using the  []  notation or  .<column name>  notation. The\nschema of the column can be accessed by  schema[<column name>] :  >>>   df [ 'job_number' ] \n     Column ( expr = SparkPrimitiveExpression ( col_name = 'job_number' , df_id = '...' ))  >>>   df . job_number \n     Column ( expr = SparkPrimitiveExpression ( col_name = 'job_number' , df_id = '...' ))  >>>   df . schema [ 'job_number' ] \n     SchemaField ( name = 'job_number' , storage_type = INTEGER , variable_type = DISCRETE )   This says  job_number  is a column with storage type  INTEGER  and variable type  DISCRETE .  Storage type casting : You can cast a column into a different Storage type like this:  >>>   df2   =   df . with_storage_type ( df . job_number ,   cb . StorageTypes . LONG )  >>>   df2 . schema [ 'job_number' ] \n     SchemaField ( name = 'job_number' , storage_type = LONG , variable_type = DISCRETE )  >>>   df . schema [ 'job_number' ] \n     SchemaField ( name = 'job_number' , storage_type = INTEGER , variable_type = DISCRETE )   Note that  df2  is a different Dataframe from  df . In fact they have different IDs.  Another way to do the same thing is to use Cebes SQL APIs:  >>>   df2   =   df . with_column ( 'job_number' ,   df . job_number . cast ( cb . StorageTypes . LONG ))   Here we used  with_column  and  cast  to convert the  job_number  column from INTEGER to LONG.\nWe will explore other Cebes SQL APIs later.  Variable type casting : Similarly, you can set the variable type of a column:  >>>   df2   =   df . with_variable_type ( df . job_number ,   cb . VariableTypes . ORDINAL )  >>>   df2 . schema [ 'job_number' ] \n     SchemaField ( name = 'job_number' , storage_type = INTEGER , variable_type = ORDINAL )  >>>   df . schema [ 'job_number' ] \n     SchemaField ( name = 'job_number' , storage_type = INTEGER , variable_type = DISCRETE )",
            "title": "Working with types"
        },
        {
            "location": "/dataframe_guide/#basic-operations-with-dataframes",
            "text": "One of the most frequently used functions of Dataframe is  show , \nwhich shows essential information of a Dataframe like its ID, shape as well as several rows taken from it:  >>>   df . show () \n     ID :   67 b827d7 - 869 b - 48 d9 - 9364 - 5 a6d3bc2e99c \n     Shape :   ( 540 ,   40 ) \n     Sample   5   rows : \n        timestamp   cylinder_number   customer    job_number   grain_screened   ink_color     ... \n     0     19910108              X126    TVGUIDE         25503              YES         KEY     ... \n     1     19910109              X266    TVGUIDE         25503              YES         KEY     ... \n     2     19910104                B7     MODMAT         47201              YES         KEY     ... \n     3     19910104              T133     MASSEY         39039              YES         KEY     ... \n     4     19910111               J34      KMART         37351               NO         KEY     ...   Other information of a Dataframe can be inspected quite intuitively as in the following sample code:  # The unique ID of the Dataframe  >>>   df . id \n     '67b827d7-869b-48d9-9364-5a6d3bc2e99c'  # Number of rows  >>>   len ( df ) \n     540  # Number of columns  >>>   len ( df . columns ) \n     40  >>>   df . shape \n     ( 540 ,   40 )  # list of column names  >>>   df . columns \n     [ 'timestamp' , \n      'cylinder_number' , \n      ... , \n      'band_type' ]  # the schema of this Dataframe  >>>   df . schema \n     Schema ( fields = [ SchemaField ( name = 'timestamp' , storage_type = LONG , variable_type = DISCRETE ), \n                    SchemaField ( name = 'cylinder_number' , storage_type = STRING , variable_type = TEXT ), \n                    ... \n                    , SchemaField ( name = 'band_type' , storage_type = STRING , variable_type = TEXT )])  # Schema information of a column can be accessed by the [] notation  >>>   df . schema [ 'cylinder_number' ] \n     SchemaField ( name = 'cylinder_number' , storage_type = STRING , variable_type = TEXT )  # A column in the Dataframe can be accessed using the `dot` notation  >>>   df . timestamp \n     Column ( expr = SparkPrimitiveExpression ( col_name = 'timestamp' , df_id = '67b827d7-869b-48d9-9364-5a6d3bc2e99c' ))  # or the [] notation  >>>   df [ 'timestamp' ] \n     Column ( expr = SparkPrimitiveExpression ( col_name = 'timestamp' , df_id = '67b827d7-869b-48d9-9364-5a6d3bc2e99c' ))",
            "title": "Basic operations with Dataframes"
        },
        {
            "location": "/dataframe_guide/#sample-a-dataframe",
            "text": "There are two ways to take a sample from a Dataframe, depending on how you want the result:   take  returns a sample as a  DataSample  \nobject, which is downloaded to the client, and can be converted into, for example, pandas DataFrame.  sample  returns a sample as another  Dataframe  object. The data remains on the Cebes server.   Furthermore,  sample  is designed to return a random subset of the Dataframe, while  take  is not necessarily \nrandom.  >>>   local_sample   =   df . take ( 10 )  >>>   local_sample \n     DataSample ( schema = Schema ( fields = [ SchemaField ( name = 'timestamp' , storage_type = LONG , variable_type = DISCRETE ), \n                                      SchemaField ( name = 'cylinder_number' , storage_type = STRING , variable_type = TEXT ), \n                                      ... , \n                                      SchemaField ( name = 'band_type' , storage_type = STRING , variable_type = TEXT )]))  >>>   local_sample . to_pandas () \n        timestamp   cylinder_number      customer    job_number   grain_screened   ink_color    ... \n     0     19910108              X126       TVGUIDE         25503              YES         KEY    ... \n     1     19910109              X266       TVGUIDE         25503              YES         KEY    ... \n     2     19910104                B7        MODMAT         47201              YES         KEY    ... \n\n     [ 10   rows   x   40   columns ]  >>>   df2   =   df . sample ( prob = 0.1 ,   replacement = True ,   seed = 42 )  >>>   df2 \n     Dataframe ( id = 'ac712897-c72a-44d3-816f-7e19fa008fcb' )  >>>   len ( df2 ) \n     55    Other basic operations may include:   Sort a Dataframe with  sort  Drop a column with  drop  Drop duplicated rows with  drop_duplicates   It hardly gets more intuitive.",
            "title": "Sample a Dataframe"
        },
        {
            "location": "/dataframe_guide/#na-handling",
            "text": "Drop rows having empty cells with  dropna  Fill empty cells with  fillna",
            "title": "NA handling"
        },
        {
            "location": "/dataframe_guide/#statistical-functions",
            "text": "Quantile  Correlation  Covariance  Cross-table  Frequent Itemsets  Advanced sampling",
            "title": "Statistical functions"
        },
        {
            "location": "/dataframe_guide/#basic-sql-operations",
            "text": "Most of the operations above could have been expressed in SQL. Thanks to Spark, SQL expressions are a core\npart in Cebes' data exploration APIs. We review here a few main functions, and give pointers to others.  The basic filter and projection operations can be done using  select  and where . For example,\nhere is a not-so-trivial projection that involves creating an array of values:  >>>   df . columns \n     [ ... , \n      'customer' , \n      ... , \n      'esa_voltage' , \n      'esa_amperage' ]  >>>   df2   =   df . select ( df . customer ,   cb . array ( df . esa_voltage ,   df . esa_amperage ) . alias ( 'esa_array' ),  \n                     df . esa_voltage ,   df . esa_amperage ) . where ( df . customer . isin ([ 'TVGUIDE' ,   'MODMAT' ]))  >>>   df2 . show () \n     ID :   cd1433ff - 0 c18 - 4920 - 9699 - bd7ec7eacd88 \n     Shape :   ( 99 ,   4 ) \n     Sample   5   rows : \n       customer     esa_array    esa_voltage    esa_amperage \n     0    TVGUIDE    [ 0.0 ,   0.0 ]            0.0             0.0 \n     1    TVGUIDE    [ 0.0 ,   0.0 ]            0.0             0.0 \n     2     MODMAT    [ 0.0 ,   0.0 ]            0.0             0.0 \n     3     MODMAT    [ 1.5 ,   0.0 ]            1.5             0.0 \n     4     MODMAT    [ 0.0 ,   0.0 ]            0.0             0.0",
            "title": "Basic SQL operations"
        },
        {
            "location": "/dataframe_guide/#combine-expressions",
            "text": "To combine multiple boolean expressions, use boolean operators  & ,  |  and  ~  on the expressions.  Note that Python's built-in boolean operators like  and ,  or ,  not  are not usable because they \ncannot be overridden to provide the functionality we want. Trying to use  and ,  or ,  not  on \nCebes expressions will throw an exception.  >>>   df . select ( df . wax ,   df . hardener ) . where (( df . wax   <   2.8 )   &   ~ ( df . hardener   >   0.8 )) . show () \n     ID :   0 ba144ef - 22e4 - 48 d6 - 8774 - 7 eef95b57a40 \n     Shape :   ( 128 ,   2 ) \n     Sample   5   rows : \n        wax    hardener \n     0    2.5         0.7 \n     1    2.3         0.6 \n     2    2.5         0.8 \n     3    2.5         0.6 \n     4    2.5         0.8  >>>   df . select ( df . wax ,   df . hardener ) . where (( df . wax   <   2.8 )   and   not ( df . hardener   >   0.8 )) . show () \n     ... \n     ValueError :   Cannot   convert   column   into   bool :   please   use   '&'   for   'and' ,   '|'   for   'or' ,  \n     '~'   for   'not'   when   building   Dataframe   boolean   expressions .   Bitwise operations can be constructed as follows:  >>>   df . select ( df . roller_durometer ,   df . current_density ,  \n               df . roller_durometer . bitwise_or ( df . current_density ),  \n               df . roller_durometer . bitwise_and ( cb . bitwise_not ( df . current_density )) . alias ( 'complicated_expr' )) . show () \n\n     ID :   2 ccc186d - 6 a7c - 45 f6 - 8572 - bf0e753442eb \n     Shape :   ( 540 ,   4 ) \n     Sample   5   rows : \n        roller_durometer    current_density    ( roller_durometer   |   current_density )   \\\n     0                  34                 40                                      42    \n     1                  34                 40                                      42    \n     2                  40                 40                                      40    \n     3                  40                 40                                      40    \n     4                  35                 40                                      43    \n\n        complicated_expr \n     0                   2   \n     1                   2   \n     2                   0   \n     3                   0   \n     4                   3   Here we use some functions on the expressions like  df.roller_durometer.bitwise_or() , but Cebes also provides\nmany functions in the functional form  cb.bitwise_not() . See  this page  \nfor the full list of functions provided in Cebes.  Other basic SQL operations include  limit ,\n  intersect ,\n  union , \n  subtract , \n  alias , ...",
            "title": "Combine expressions"
        },
        {
            "location": "/dataframe_guide/#joins",
            "text": "Joins of two Dataframes can be done with  join ,\nwhich supports generic join conditions and different types of joins.   If one Dataframe is significantly smaller than the other in a join, you can mark it  broadcast , in which \ncase the join might be executed more efficient.",
            "title": "Joins"
        },
        {
            "location": "/dataframe_guide/#grouping",
            "text": "Grouping can be done with  groupby ,  rollup  and  cube  are also supported.\nAfter  groupby() , various way to aggregate the results are provided, including functions \nin  GroupedDataframe  and  aggregation functions .  When grouping on multiple columns, rollup and cube are different in the way they construct the tuples.\nFor example, when grouping on 3 columns,  ROLLUP (YEAR, MONTH, DAY)  will give the following outputs:  YEAR, MONTH, DAY\nYEAR, MONTH\nYEAR\n()  while  CUBE (YEAR, MONTH, DAY)  gives the following:  YEAR, MONTH, DAY\nYEAR, MONTH\nYEAR, DAY\nYEAR\nMONTH, DAY\nMONTH\nDAY\n()",
            "title": "Grouping"
        },
        {
            "location": "/dataframe_guide/#when-otherwise",
            "text": "A particularly useful API is to compute a column based on values of other columns, much like \nconditional expressions in programming languages.   For example, here is how to encode the  gender  string column into numeric:  >>>   people . select ( cb . when ( people . gender   ==   'male' ,   0 ) \n             . when ( people . gender   ==   'female' ,   1 ) \n             . otherwise ( 2 ) . alias ( 'gender_int' ))   If  otherwise()  is not specified, null value will be used. See  when  and  otherwise  for more information.",
            "title": "When ... Otherwise ..."
        },
        {
            "location": "/dataframe_guide/#windowing-on-time-series",
            "text": "Cebes has the  window  function for windowing on time-series.  In the example below, the  timestamp  column contains the date in format  yyyyMMdd . We convert \nit into Unix timestamp using the  unix_timestamp  function:  # convert the `timestamp` column from \"yyyyMMdd\" into Unix timestamp, and cast it to type TIMESTAMP  >>>   df2   =   df . with_column ( 'timestamp_unix' ,   ... :      cb . unix_timestamp ( df . timestamp . cast ( cb . StorageTypes . STRING ),   pattern = 'yyyyMMdd' ) . \\ ... :      cast ( cb . StorageTypes . TIMESTAMP ))  >>>   df2 . show () \n     ID :   b03c92b0 - f97a - 4 d21 - 94e5 - 1 b039fc2c038 \n     Shape :   ( 540 ,   41 ) \n     Sample   5   rows : \n        timestamp   cylinder_number   customer    job_number   grain_screened   ink_color   \\\n     0     19910108              X126    TVGUIDE         25503              YES         KEY    \n     1     19910109              X266    TVGUIDE         25503              YES         KEY    \n     2     19910104                B7     MODMAT         47201              YES         KEY    \n     3     19910104              T133     MASSEY         39039              YES         KEY    \n     4     19910111               J34      KMART         37351               NO         KEY    \n\n       proof_on_ctd_ink   blade_mfg   cylinder_division   paper_type         ...         \\\n     0                YES      BENTON            GALLATIN     UNCOATED         ...          \n     1                YES      BENTON            GALLATIN     UNCOATED         ...          \n     2                YES      BENTON            GALLATIN     UNCOATED         ...          \n     3                YES      BENTON            GALLATIN     UNCOATED         ...          \n     4                YES      BENTON            GALLATIN     UNCOATED         ...          \n\n       esa_voltage   esa_amperage    wax   hardener   roller_durometer    current_density   \\\n     0           0.0            0.0    2.5        1.0                 34                 40    \n     1           0.0            0.0    2.5        0.7                 34                 40    \n     2           0.0            0.0    2.8        0.9                 40                 40    \n     3           0.0            0.0    2.5        1.3                 40                 40    \n     4           5.0            0.0    2.3        0.6                 35                 40    \n\n        anode_space_ratio   chrome_content   band_type    timestamp_unix   \n     0           105.000000            100.0        band         663292800   \n     1           105.000000            100.0      noband         663379200   \n     2           103.870003            100.0      noband         662947200   \n     3           108.059998            100.0      noband         662947200   \n     4           106.669998            100.0      noband         663552000   \n\n     [ 5   rows   x   41   columns ]   We then use  window  on the timestamp column, and  groupby  on those groups to find the \nnumber of transactions happening every week:  >>>   df3   =   df2 . groupby ( cb . window ( df2 . timestamp_unix ,   '7 days' ,   '7 days' )) . count ()  >>>   df3 . schema \n     Schema ( fields = [ SchemaField ( name = 'window' , storage_type = Struct [ TIMESTAMP , TIMESTAMP ], variable_type = STRUCT ), \n                    SchemaField ( name = 'count' , storage_type = LONG , variable_type = DISCRETE )])  >>>   df3 . show () \n     ID :   22 c45afe - 1488 - 4562 - ac4b - 5 d4767e8a6da \n     Shape :   ( 122 ,   2 ) \n     Sample   5   rows : \n                                            window    count \n     0    { 'end' :   676166400.0 ,   'start' :   675561600.0 }        8 \n     1    { 'end' :   651974400.0 ,   'start' :   651369600.0 }        4 \n     2    { 'end' :   701568000.0 ,   'start' :   700963200.0 }        1 \n     3    { 'end' :   707011200.0 ,   'start' :   706406400.0 }        1 \n     4    { 'end' :   714873600.0 ,   'start' :   714268800.0 }        2   The  window  column now has type  Struct , containing the start and end timestamp. We have \nthe correct results now, but to make it looks a bit better, wee can \nconvert those timestamps into the  dd/MM/yyyy  format, and sort the data:  >>>   df4   =   df3 . with_column ( 'start' ,   cb . date_format ( df3 [ 'window.start' ],   'dd/MM/yyyy' )) . \\\n     with_column ( 'end' ,   cb . date_format ( df3 [ 'window.end' ],   'dd/MM/yyyy' )) . \\\n     with_column ( 'window_start' ,   df3 [ 'window.start' ]) . sort ( 'window_start' )  >>>   df4   =   df4 . drop ( df4 . window_start )  >>>   df4 . show () \n    ID :   85 f03b43 - 7731 - 487 a - a01c - e5d7ae488c4d \n    Shape :   ( 122 ,   4 ) \n    Sample   5   rows : \n                                           window    count         start           end \n    0    { 'end' :   639273600.0 ,   'start' :   638668800.0 }        1    29 / 03 / 1990    05 / 04 / 1990 \n    1    { 'end' :   639878400.0 ,   'start' :   639273600.0 }        2    05 / 04 / 1990    12 / 04 / 1990 \n    2    { 'end' :   640483200.0 ,   'start' :   639878400.0 }        5    12 / 04 / 1990    19 / 04 / 1990 \n    3    { 'end' :   641088000.0 ,   'start' :   640483200.0 }        1    19 / 04 / 1990    26 / 04 / 1990 \n    4    { 'end' :   641692800.0 ,   'start' :   641088000.0 }        2    26 / 04 / 1990    03 / 05 / 1990",
            "title": "Windowing on time-series"
        },
        {
            "location": "/dataframe_reference/",
            "text": "Dataframe\n\u00b6\n\n\nDataframe\n(\nself\n,\n \n_id\n,\n \n_schema\n)\n\n\n\n\n\n\nRepresentation of a Cebes Dataframe on the client side. All functions in this\nclass result in remote call to the Cebes server to perform corresponding actions.\n\n\nUsers should \nNOT\n manually construct this class.\n\n\nunion\n\u00b6\n\n\nDataframe\n.\nunion\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nReturns a new Dataframe containing union of rows in this Dataframe and another Dataframe\n(without deduplication)\n\n\nArguments\n\n\n\n\n\n\nother (Dataframe)\n: another Dataframe to compute the union\n\n\n\n\n\n\n__\nExample__\n:\n\n\n\n\n\n\ndf\n.\nwhere\n(\ndf\n.\nwax\n \n<\n \n2\n)\n.\nunion\n(\ndf\n.\nwhere\n(\ndf\n.\nwax\n \n>\n \n2.8\n))\n.\nselect\n(\n'customer'\n,\n \n'wax'\n)\n.\nshow\n()\n\n    \n...\n\n        \ncustomer\n  \nwax\n\n    \n0\n      \nUSCAV\n  \n1.1\n\n    \n1\n   \nCOLORTIL\n  \n1.7\n\n    \n2\n   \nCOLORTIL\n  \n1.0\n\n    \n3\n  \nABBYPRESS\n  \n1.0\n\n    \n4\n  \nABBYPRESS\n  \n1.0\n\n\n\n\n\n\nschema\n\u00b6\n\n\nThe Schema of this data frame\n\n\ndrop_duplicates\n\u00b6\n\n\nDataframe\n.\ndrop_duplicates\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nReturns a new Dataframe that contains only the unique rows from this Dataframe, only considered\nthe given list of columns.\n\n\nIf no columns are given (the default), all columns will be considered\n\n\nReturns\n\n\nDataframe\n: a new \nDataframe\n with duplicated rows removed\n\n\nExample\n\n\n# original Dataframe has 540 rows\n\n\ndf\n.\nshape\n\n    \n(\n540\n,\n \n40\n)\n\n\n\n# No duplicated rows in this Dataframe, therefore the shape stays the same\n\n\ndf\n.\ndrop_duplicates\n()\n.\nshape\n\n    \n(\n540\n,\n \n40\n)\n\n\n\n# only consider the `customer` column\n\n\ndf\n.\ndrop_duplicates\n(\ndf\n.\ncustomer\n)\n.\nshape\n\n    \n(\n83\n,\n \n40\n)\n\n\n\n# we can check by computing the number of distinct values in column `customer`\n\n\ndf\n.\nselect\n(\ncb\n.\ncount_distinct\n(\ndf\n.\ncustomer\n))\n.\ntake\n()\n.\nto_pandas\n()\n\n       \ncount\n(\nDISTINCT\n \ncustomer\n)\n\n    \n0\n                        \n83\n\n\n\n# only consider 2 columns\n\n\ndf\n.\ndrop_duplicates\n(\ndf\n.\ncustomer\n,\n \n'cylinder_number'\n)\n.\nshape\n\n    \n(\n490\n,\n \n40\n)\n\n\n\n\n\n\nfrom_json\n\u00b6\n\n\nDataframe\n.\nfrom_json\n(\njs_data\n)\n\n\n\n\n\n\nReturn a \nDataframe\n instance from its JSON representation\n\n\nArguments\n\n\n\n\njs_data (dict)\n: a dict with \nid\n and \nschema\n\n\n\n\nReturns\n\n\nDataframe\n: the result Dataframe object\n\n\nselect\n\u00b6\n\n\nDataframe\n.\nselect\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nSelects a set of columns based on expressions.\n\n\nArguments\n\n\n\n\ncolumns\n: list of columns, or column names\n\n\n\n\nReturns\n\n\nDataframe\n: result of the select operation\n\n\nExample\n\n\nimport\n \npycebes\n \nas\n \ncb\n\n\n\n# different ways to provide arguments to `select`\n\n\ndf\n.\nselect\n(\ndf\n.\ncustomer\n,\n \ncb\n.\nsubstring\n(\n'cylinder_number'\n,\n \n0\n,\n \n1\n)\n.\nalias\n(\n'cylinder_t'\n),\n\n          \ncb\n.\ncol\n(\n'hardener'\n),\n \n'wax'\n)\n.\nshow\n()\n\n    \n...\n\n      \ncustomer\n \ncylinder_t\n  \nhardener\n  \nwax\n\n    \n0\n  \nTVGUIDE\n          \nX\n       \n1.0\n  \n2.5\n\n    \n1\n  \nTVGUIDE\n          \nX\n       \n0.7\n  \n2.5\n\n    \n2\n   \nMODMAT\n          \nB\n       \n0.9\n  \n2.8\n\n    \n3\n   \nMASSEY\n          \nT\n       \n1.3\n  \n2.5\n\n    \n4\n    \nKMART\n          \nJ\n       \n0.6\n  \n2.3\n\n\n\n# Select all columns in a dataframe can be done in several ways\n\n\ndf\n.\nselect\n(\n'*'\n)\n\n\ndf\n.\nselect\n(\ndf\n[\n'*'\n])\n\n\ndf\n.\nselect\n(\ncb\n.\ncol\n(\n'*'\n))\n\n\ndf\n.\nalias\n(\n'my_name'\n)\n.\nselect\n(\n'my_name.*'\n)\n\n\n\n\n\n\nsubtract\n\u00b6\n\n\nDataframe\n.\nsubtract\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nReturns a new Dataframe containing rows in this Dataframe but not in another Dataframe.\nThis is equivalent to \nEXCEPT\n in SQL.\n\n\nArguments\n\n\n\n\nother (Dataframe)\n: another Dataframe to compute the except\n\n\n\n\nExample\n\n\ndf\n.\nwhere\n(\ndf\n.\nwax\n \n<\n \n2.8\n)\n.\nsubtract\n(\ndf\n.\nwhere\n(\ndf\n.\nwax\n \n<\n \n2\n))\n.\nselect\n(\n'customer'\n,\n \n'wax'\n)\n.\nshow\n()\n\n    \n...\n\n            \ncustomer\n  \nwax\n\n    \n0\n      \nSERVMERCH\n  \n2.7\n\n    \n1\n        \nTVGUIDE\n  \n2.5\n\n    \n2\n         \nMODMAT\n  \n2.7\n\n    \n3\n           \nAMES\n  \n2.4\n\n    \n4\n  \ncolorfulimage\n  \n2.5\n\n\n\n\n\n\nid\n\u00b6\n\n\nReturn the unique ID of this :class:\nDataframe\n\n\nagg\n\u00b6\n\n\nDataframe\n.\nagg\n(\nself\n,\n \n*\nexprs\n)\n\n\n\n\n\n\nCompute aggregates and returns the result as a DataFrame.\n\n\nThis is a convenient method in which the aggregations are computed on\nall rows. In other words, \nself.agg(*exprs)\n is equivalent to \nself.groupby().agg(*exprs)\n\n\nIf exprs is a single dict mapping from string to string,\nthen the key is the column to perform aggregation on, and the value is the aggregate function.\n\n\nThe available aggregate functions are \navg\n, \nmax\n, \nmin\n, \nsum\n, \ncount\n.\n\n\nAlternatively, exprs can also be a list of aggregate \nColumn\n expressions.\n\n\nArguments\n\n\n\n\nexprs (dict, list)\n: a dict mapping from column name (string) to aggregate functions (string),\n    or a list of \nColumn\n\n\n\n\nExample\n\n\nimport\n \npycebes\n \nas\n \ncb\n\n\n\n# count number of non-NA values in column `hardener`\n\n\ndf\n.\nagg\n(\ncb\n.\ncount\n(\ndf\n.\nhardener\n))\n.\nshow\n()\n\n        \ncount\n(\nhardener\n)\n\n    \n0\n              \n533\n\n\n\n# count number of non-NA values in all columns:\n\n\ndf\n.\nagg\n(\n*\n[\ncb\n.\ncount\n(\nc\n)\n.\nalias\n(\nc\n)\n \nfor\n \nc\n \nin\n \ndf\n.\ncolumns\n])\n\n\n\n\n\n\nSee \nGroupedDataframe\n for more examples\n\n\ncube\n\u00b6\n\n\nDataframe\n.\ncube\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nCreate a multi-dimensional cube for the current \nDataframe\n using the specified columns,\nso we can run aggregation on them.\n\n\nSee \nGroupedDataframe\n for all the available aggregate functions.\n\n\nArguments\n\n\n\n\ncolumns\n: list of column names or \nColumn\n objects\n\n\n\n\nReturns\n\n\nGroupedDataframe\n: object providing aggregation functions\n\n\nExample\n\n\ndf\n.\ncube\n(\ndf\n.\ncustomer\n,\n \n'proof_on_ctd_ink'\n)\n.\ncount\n()\n.\nshow\n()\n\n    \n...\n\n          \ncustomer\n \nproof_on_ctd_ink\n  \ncount\n\n    \n0\n  \nNTLWILDLIFE\n              \nYES\n      \n1\n\n    \n1\n     \nHANHOUSE\n              \nYES\n      \n2\n\n    \n2\n   \nYIELDHOUSE\n              \nYES\n      \n1\n\n    \n3\n      \ntoysrus\n                       \n3\n\n    \n4\n          \nCVS\n                       \n2\n\n\n\ndf\n.\ncube\n(\ndf\n.\ncustomer\n,\n \n'proof_on_ctd_ink'\n)\n.\nagg\n({\n'hardener'\n:\n \n'max'\n,\n \n'wax'\n:\n \n'avg'\n})\n.\nshow\n()\n\n    \n...\n\n          \ncustomer\n \nproof_on_ctd_ink\n  \nmax\n(\nhardener\n)\n  \navg\n(\nwax\n)\n\n    \n0\n  \nNTLWILDLIFE\n              \nYES\n            \n0.6\n      \n3.00\n\n    \n1\n     \nHANHOUSE\n              \nYES\n            \n1.0\n      \n2.25\n\n    \n2\n   \nYIELDHOUSE\n              \nYES\n            \n0.5\n      \n3.00\n\n    \n3\n      \ntoysrus\n                             \n2.1\n      \n2.40\n\n    \n4\n          \nCVS\n                             \n1.0\n      \n2.30\n\n\n\n\n\n\ndrop\n\u00b6\n\n\nDataframe\n.\ndrop\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nDrop columns in this \nDataframe\n\n\nArguments\n\n\n\n\ncolumns\n: column names or \nColumn\n objects\n\n\n\n\nReturns\n\n\nDataframe\n: a new \nDataframe\n with the given columns dropped.\n    If \ncolumns\n is empty, this \nDataframe\n is returned\n\n\nExample\n\n\ndf1\n.\nshow\n()\n\n    \n...\n\n      \ncustomer\n \ncylinder_number\n  \nhardener\n\n    \n0\n  \nTVGUIDE\n            \nX126\n       \n1.0\n\n    \n1\n  \nTVGUIDE\n            \nX266\n       \n0.7\n\n    \n2\n   \nMODMAT\n              \nB7\n       \n0.9\n\n    \n3\n   \nMASSEY\n            \nT133\n       \n1.3\n\n    \n4\n    \nKMART\n             \nJ34\n       \n0.6\n\n\n\ndf1\n.\ndrop\n(\ndf1\n.\ncustomer\n,\n \n'hardener'\n)\n.\nshow\n()\n\n    \n...\n\n      \ncylinder_number\n\n    \n0\n            \nX126\n\n    \n1\n            \nX266\n\n    \n2\n              \nB7\n\n    \n3\n            \nT133\n\n    \n4\n             \nJ34\n\n\n\n\n\n\nbroadcast\n\u00b6\n\n\nMarks a Dataframe as small enough for use in broadcast joins.\n\n\nwith_column_renamed\n\u00b6\n\n\nDataframe\n.\nwith_column_renamed\n(\nself\n,\n \nexisting_name\n,\n \nnew_name\n)\n\n\n\n\n\n\nReturns a new \nDataframe\n with a column renamed.\n\n\nArguments\n\n\n\n\nexisting_name (str)\n:\n\n\nnew_name (str)\n:\n\n\n\n\njoin\n\u00b6\n\n\nDataframe\n.\njoin\n(\nself\n,\n \nother\n,\n \nexpr\n,\n \njoin_type\n=\n'inner'\n)\n\n\n\n\n\n\nJoin with another \nDataframe\n, using the given join expression.\n\n\nArguments\n\n\n\n\nother (Dataframe)\n: right side of the join\n\n\nexpr (Column)\n: Join expression, as a \nColumn\n\n\njoin_type (str): One of\n: \ninner\n, \nouter\n, \nleft_outer\n, \nright_outer\n, \nleftsemi\n, \nleftanti\n, \ncross\n\n\n\n\nExample\n\n\ndf1\n.\njoin\n(\ndf2\n,\n \ndf1\n.\ndf1Key\n \n==\n \ndf2\n.\ndf2Key\n,\n \njoin_type\n=\n'outer'\n)\n\n\n\n# for self-joins, you should rename the columns so that they are accessible after the join\n\n\ndf1\n \n=\n \ndf\n.\nwhere\n(\ndf\n.\nwax\n \n>\n \n2\n)\n.\nselect\n(\ndf\n[\nc\n]\n.\nalias\n(\n'df1_{}'\n.\nformat\n(\nc\n))\n \nfor\n \nc\n \nin\n \ndf\n.\ncolumns\n)\n\n\ndf2\n \n=\n \ndf\n.\nwhere\n(\ndf\n.\nwax\n \n<\n \n2.2\n)\n.\nselect\n(\ndf\n[\nc\n]\n.\nalias\n(\n'df2_{}'\n.\nformat\n(\nc\n))\n \nfor\n \nc\n \nin\n \ndf\n.\ncolumns\n)\n\n\n\ndf_join\n \n=\n \ndf1\n.\njoin\n(\ndf2\n,\n \ndf1\n.\ndf1_customer\n \n==\n \ndf2\n.\ndf2_customer\n)\n\n\n\n\n\n\ncolumns\n\u00b6\n\n\nReturn a list of column names in this \nDataframe\n\n\nsample\n\u00b6\n\n\nDataframe\n.\nsample\n(\nself\n,\n \nprob\n=\n0.1\n,\n \nreplacement\n=\nTrue\n,\n \nseed\n=\n42\n)\n\n\n\n\n\n\nTake a sample from this \nDataframe\n with or without replacement at the given probability.\n\n\nNote that this function is probabilistic: it samples each row with a probability of \nprob\n,\ntherefore if the original \nDataframe\n has \nN\n rows, the result of this function\nwill \nNOT\n have exactly \nN * prob\n rows.\n\n\nArguments\n\n\n\n\nprob (float)\n: The probability to sample each row in the \nDataframe\n\n\nreplacement (bool)\n: Whether to sample with replacement\n\n\nseed (int)\n: random seed\n\n\n\n\nReturns\n\n\nDataframe\n: a sample\n\n\nlimit\n\u00b6\n\n\nDataframe\n.\nlimit\n(\nself\n,\n \nn\n=\n100\n)\n\n\n\n\n\n\nReturns a new \nDataframe\n by taking the first \nn\n rows.\n\n\nalias\n\u00b6\n\n\nDataframe\n.\nalias\n(\nself\n,\n \nalias\n=\n'new_name'\n)\n\n\n\n\n\n\nReturns a new Dataframe with an alias set\n\n\nshape\n\u00b6\n\n\nReturn a 2-tuple with number of rows and columns\n\n\nExample\n\n\n    \ndf\n.\nshape\n\n        \n(\n540\n,\n \n40\n)\n\n\n\n\n\n\ndropna\n\u00b6\n\n\nDataframe\n.\ndropna\n(\nself\n,\n \nhow\n=\n'any'\n,\n \nthresh\n=\nNone\n,\n \ncolumns\n=\nNone\n)\n\n\n\n\n\n\nReturn object with labels on given axis omitted where some of the data are missing\n\n\nArguments\n\n\n\n\nhow (str): strategy to drop rows. Accepted values are\n:\n\n\n* \nany\n: if any NA values are present, drop that label\n\n\n* \nall\n: if all values are NA, drop that label\n\n\nthresh (int)\n: drop rows containing less than \nthresh\n non-null and non-NaN values\n    If \nthresh\n is specified, \nhow\n will be ignored.\n\n\ncolumns (list)\n: a list of columns to include. Default is None, meaning all columns are included\n\n\n\n\nReturns\n\n\nDataframe\n: a new \nDataframe\n with NA values dropped\n\n\nExample\n\n\ndf\n.\ndropna\n()\n\n\ndf\n.\ndropna\n(\nhow\n=\n'all'\n)\n\n\ndf\n.\ndropna\n(\nthresh\n=\n38\n)\n\n\ndf\n.\ndropna\n(\nthresh\n=\n2\n,\n \ncolumns\n=\n[\ndf\n.\nwax\n,\n \n'proof_on_ctd_ink'\n,\n \ndf\n.\nhardener\n])\n\n\n\n\n\n\nshow\n\u00b6\n\n\nDataframe\n.\nshow\n(\nself\n,\n \nn\n=\n5\n)\n\n\n\n\n\n\nConvenient function to show basic information and sample rows from this Dataframe\n\n\nArguments\n\n\n\n\nn (int)\n: Number of rows to show\n\n\n\n\nExample\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n,\n \n'cylinder_number'\n)\n.\ncount\n()\n.\nshow\n()\n\n    \nID\n:\n \n4\nae73a7c\n-\n27e5\n-\n43\nb9\n-\nb69f\n-\n43\nc606e38ee0\n\n    \nShape\n:\n \n(\n490\n,\n \n3\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n        \ncustomer\n \ncylinder_number\n  \ncount\n\n    \n0\n     \nMODMAT\n            \nF108\n      \n1\n\n    \n1\n     \nMODMAT\n              \nM4\n      \n1\n\n    \n2\n  \nCASLIVING\n             \nM74\n      \n1\n\n    \n3\n    \nECKERDS\n            \nX381\n      \n1\n\n    \n4\n     \nTARGET\n             \nR43\n      \n1\n\n\n\n\n\n\nfillna\n\u00b6\n\n\nDataframe\n.\nfillna\n(\nself\n,\n \nvalue\n=\nNone\n,\n \ncolumns\n=\nNone\n)\n\n\n\n\n\n\nFill NA/NaN values using the specified value.\n\n\nArguments\n\n\n\n\nvalue\n: the value used to fill the holes. Can be a \nfloat\n, \nstring\n or a \ndict\n.\n    When \nvalue\n is a dict, it is a map from column names to a value used to fill the holes in that column.\n    In this case the value can only be of type \nint\n, \nfloat\n, \nstring\n or \nbool\n.\n    The values will be casted to the column data type.\n\n\ncolumns (list)\n: List of columns to consider. If None, all columns are considered\n\n\n\n\nReturns:\n\n\nDataframe: a new Dataframe with holes filled\n\n\nsort\n\u00b6\n\n\nDataframe\n.\nsort\n(\nself\n,\n \n*\nargs\n)\n\n\n\n\n\n\nSort this \nDataframe\n based on the given list of expressions.\n\n\nArguments\n\n\n\n\nargs\n: a list of arguments to specify how to sort the Dataframe, where each element\n    is either a \nColumn\n object or a column name.\n    When the element is a column name, it will be sorted in ascending order.\n\n\n\n\nReturns\n\n\nDataframe\n: a new, sorted \nDataframe\n\n\nExample\n\n\ndf2\n \n=\n \ndf1\n.\nsort\n()\n\n\ndf2\n \n=\n \ndf1\n.\nsort\n(\ndf1\n[\n'timestamp'\n]\n.\nasc\n,\n \ndf1\n[\n'customer'\n]\n.\ndesc\n)\n\n\ndf2\n \n=\n \ndf1\n.\nsort\n(\n'timestamp'\n,\n \ndf1\n.\ncustomer\n.\ndesc\n)\n\n\n\n# raise ValueError\n\n\ndf1\n.\nsort\n(\n'non_exist'\n)\n\n\n\n\n\n\nrollup\n\u00b6\n\n\nDataframe\n.\nrollup\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nCreate a multi-dimensional rollup for the current \nDataframe\n using the specified columns,\nso we can run aggregation on them.\n\n\nSee \nGroupedDataframe\n for all the available aggregate functions.\n\n\nArguments\n\n\n\n\ncolumns\n: list of column names or \nColumn\n objects\n\n\n\n\nReturns\n\n\nGroupedDataframe\n: object providing aggregation functions\n\n\nExample\n\n\ndf\n.\nrollup\n(\ndf\n.\ncustomer\n,\n \n'proof_on_ctd_ink'\n)\n.\ncount\n()\n.\nshow\n()\n\n    \n...\n\n          \ncustomer\n \nproof_on_ctd_ink\n  \ncount\n\n    \n0\n  \nNTLWILDLIFE\n              \nYES\n      \n1\n\n    \n1\n     \nHANHOUSE\n              \nYES\n      \n2\n\n    \n2\n   \nYIELDHOUSE\n              \nYES\n      \n1\n\n    \n3\n      \ntoysrus\n                       \n3\n\n    \n4\n          \nCVS\n                       \n2\n\n\n\ndf\n.\nrollup\n(\ndf\n.\ncustomer\n,\n \n'proof_on_ctd_ink'\n)\n.\nagg\n({\n'hardener'\n:\n \n'max'\n,\n \n'wax'\n:\n \n'avg'\n})\n.\nshow\n()\n\n    \n...\n\n          \ncustomer\n \nproof_on_ctd_ink\n  \nmax\n(\nhardener\n)\n  \navg\n(\nwax\n)\n\n    \n0\n  \nNTLWILDLIFE\n              \nYES\n            \n0.6\n      \n3.00\n\n    \n1\n     \nHANHOUSE\n              \nYES\n            \n1.0\n      \n2.25\n\n    \n2\n   \nYIELDHOUSE\n              \nYES\n            \n0.5\n      \n3.00\n\n    \n3\n      \ntoysrus\n                             \n2.1\n      \n2.40\n\n    \n4\n          \nCVS\n                             \n1.0\n      \n2.30\n\n\n\n\n\n\nwhere\n\u00b6\n\n\nDataframe\n.\nwhere\n(\nself\n,\n \ncondition\n)\n\n\n\n\n\n\nFilters rows using the given condition.\n\n\nArguments\n\n\n\n\ncondition (Column)\n: the condition as a Column\n\n\n\n\nExample\n\n\ndf\n.\nwhere\n((\ndf\n.\nhardener\n \n>=\n \n1\n)\n \n&\n \n(\ndf\n.\nwax\n \n<\n \n2.8\n))\n.\nselect\n(\n'wax'\n,\n \ndf\n.\nhardener\n,\n \ndf\n.\ncustomer\n)\n.\nshow\n()\n\n    \n...\n\n       \nwax\n  \nhardener\n     \ncustomer\n\n    \n0\n  \n2.5\n       \n1.0\n      \nTVGUIDE\n\n    \n1\n  \n2.5\n       \n1.3\n       \nMASSEY\n\n    \n2\n  \n2.5\n       \n1.1\n        \nROSES\n\n    \n3\n  \n2.5\n       \n1.0\n  \nHANOVRHOUSE\n\n    \n4\n  \n2.0\n       \n1.0\n   \nGUIDEPOSTS\n\n\n\n\n\n\nwith_storage_type\n\u00b6\n\n\nDataframe\n.\nwith_storage_type\n(\nself\n,\n \ncolumn\n,\n \nstorage_type\n=<\nStorageTypes\n.\nINTEGER\n:\n \n(\n'integer'\n,\n \n<\nclass\n \n'\nint\n'>)>)\n\n\n\n\n\n\nCast the given column of this Dataframe into the given storage type.\nNo-op if the new storage type is the same with the current one.\n\n\nArguments\n\n\n\n\ncolumn\n: a string or a Column object\n\n\nstorage_type (StorageTypes)\n: the new storage type to convert to\n\n\n\n\nReturns\n\n\nDataframe\n: this Dataframe or a new Dataframe\n\n\nintersect\n\u00b6\n\n\nDataframe\n.\nintersect\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nReturns a new Dataframe containing rows only in both this Dataframe and another Dataframe.\n\n\nArguments\n\n\n\n\nother (Dataframe)\n: another Dataframe to compute the intersection\n\n\n\n\nExample\n\n\ndf\n.\nwhere\n(\ndf\n.\nwax\n \n>\n \n2\n)\n.\nintersect\n(\ndf\n.\nwhere\n(\ndf\n.\nwax\n \n<\n \n2.8\n))\n.\nselect\n(\n'customer'\n,\n \n'wax'\n)\n.\nshow\n()\n\n    \n...\n\n            \ncustomer\n  \nwax\n\n    \n0\n      \nSERVMERCH\n  \n2.7\n\n    \n1\n        \nTVGUIDE\n  \n2.5\n\n    \n2\n         \nMODMAT\n  \n2.7\n\n    \n3\n           \nAMES\n  \n2.4\n\n    \n4\n  \ncolorfulimage\n  \n2.5\n\n\n\n\n\n\nwith_column\n\u00b6\n\n\nDataframe\n.\nwith_column\n(\nself\n,\n \ncol_name\n,\n \ncol\n)\n\n\n\n\n\n\nReturns a new \nDataframe\n by adding a column or replacing\nthe existing column that has the same name (case-insensitive).\n\n\nArguments\n\n\n\n\ncol_name (str)\n: new column name\n\n\ncol (Column)\n: \nColumn\n object describing the new column\n\n\n\n\nwith_variable_type\n\u00b6\n\n\nDataframe\n.\nwith_variable_type\n(\nself\n,\n \ncolumn\n,\n \nvariable_type\n=<\nVariableTypes\n.\nDISCRETE\n:\n \n'Discrete'\n>\n)\n\n\n\n\n\n\nManually set the variable type of the given column.\nRaise error if the new variable type is not compatible with the storage type of the column.\n\n\nArguments\n\n\n\n\ncolumn\n: a string or a Column object\n\n\nvariable_type (VariableTypes)\n: the new variable type\n\n\n\n\nReturns\n\n\nDataframe\n: this Dataframe or a new Dataframe\n\n\ntake\n\u00b6\n\n\nDataframe\n.\ntake\n(\nself\n,\n \nn\n=\n10\n)\n\n\n\n\n\n\nTake a sample of this \nDataframe\n\n\nArguments\n\n\n\n\nn (int)\n: maximum number of rows to be taken\n\n\n\n\nReturns\n\n\nDataSample\n: sample of maximum size \nn\n\n\ngroupby\n\u00b6\n\n\nDataframe\n.\ngroupby\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nGroups the \nDataframe\n using the specified columns, so that we can run aggregation on them.\n\n\nSee \nGroupedDataframe\n for all the available aggregate functions.\n\n\nArguments\n\n\n\n\ncolumns\n: list of column names or \nColumn\n objects\n\n\n\n\nReturns\n\n\nGroupedDataframe\n: object providing aggregation functions\n\n\nExample\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n,\n \n'cylinder_number'\n)\n.\ncount\n()\n.\nshow\n()\n\n    \n...\n\n        \ncustomer\n \ncylinder_number\n  \ncount\n\n    \n0\n     \nMODMAT\n            \nF108\n      \n1\n\n    \n1\n     \nMODMAT\n              \nM4\n      \n1\n\n    \n2\n  \nCASLIVING\n             \nM74\n      \n1\n\n    \n3\n    \nECKERDS\n            \nX381\n      \n1\n\n    \n4\n     \nTARGET\n             \nR43\n      \n1\n\n\n\ndf\n.\ngroupby\n()\n.\ncount\n()\n.\nshow\n()\n\n    \n...\n\n       \ncount\n\n    \n0\n    \n540\n\n\n\n\n\n\nGroupedDataframe\n\u00b6\n\n\nGroupedDataframe\n(\nself\n,\n \ndf\n,\n \nagg_columns\n=\n(),\n \nagg_type\n=\n'GroupBy'\n,\n \npivot_column\n=\nNone\n,\n \npivot_values\n=\n())\n\n\n\n\n\n\nRepresent a grouped Dataframe, providing aggregation functions.\n\n\nGROUPBY\n\u00b6\n\n\nstr(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.\nstr\n() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\nmean\n\u00b6\n\n\nGroupedDataframe\n.\nmean\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nComputes the average value for each numeric column for each group.\n\n\nArguments\n\n\n\n\ncolumns\n: list of column names (string). Non-numeric columns are ignored.\n\n\n\n\nExample\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\nmean\n(\n'hardener'\n,\n \n'wax'\n)\n.\nshow\n()\n\n    \n...\n\n           \ncustomer\n  \navg\n(\nhardener\n)\n  \navg\n(\nwax\n)\n\n          \nWOOLWORTH\n       \n0.811111\n  \n1.744444\n\n           \nHOMESHOP\n       \n1.300000\n  \n2.500000\n\n       \nhomeshopping\n            \nNaN\n  \n2.500000\n\n             \nGLOBAL\n       \n1.100000\n  \n3.000000\n\n                \nJCP\n       \n0.975000\n  \n2.437500\n\n\n\n\n\n\nsum\n\u00b6\n\n\nGroupedDataframe\n.\nsum\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nComputes the sum for each numeric column for each group.\n\n\nArguments\n\n\n\n\ncolumns\n: list of column names (string). Non-numeric columns are ignored.\n\n\n\n\nExample\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\nsum\n(\n'hardener'\n,\n \n'wax'\n)\n.\nshow\n()\n\n    \n...\n\n           \ncustomer\n  \nsum\n(\nhardener\n)\n  \nsum\n(\nwax\n)\n\n          \nWOOLWORTH\n            \n7.3\n     \n15.70\n\n           \nHOMESHOP\n            \n2.6\n      \n5.00\n\n       \nhomeshopping\n            \nNaN\n      \n2.50\n\n             \nGLOBAL\n            \n1.1\n      \n3.00\n\n                \nJCP\n            \n3.9\n      \n9.75\n\n\n\n\n\n\npivot\n\u00b6\n\n\nGroupedDataframe\n.\npivot\n(\nself\n,\n \ncolumn\n,\n \nvalues\n=\nNone\n)\n\n\n\n\n\n\nPivots a column of the \nDataframe\n and perform the specified aggregation.\n\n\nArguments\n\n\n\n\ncolumn (str)\n: Name of the column to pivot.\n\n\nvalues\n: List of values that will be translated to columns in the output \nDataframe\n\n    If unspecified, Cebes will need to compute the unique values of the given column before\n    the pivot, therefore it will be less efficient.\n\n\n\n\nReturns\n\n\nGroupedDataframe\n: another \nGroupedDataframe\n object, on which you can call aggregation functions\n\n\nExample\n\n\n# for each pair of (`customer`, `proof_on_ctd_ink`), count the number of entries\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\npivot\n(\n'proof_on_ctd_ink'\n)\n.\ncount\n()\n.\nshow\n()\n\n    \n...\n\n           \ncustomer\n        \nNO\n  \nYES\n\n          \nWOOLWORTH\n  \n1.0\n  \n1.0\n  \n7.0\n\n           \nHOMESHOP\n  \nNaN\n  \nNaN\n  \n2.0\n\n       \nhomeshopping\n  \n1.0\n  \nNaN\n  \nNaN\n\n             \nGLOBAL\n  \nNaN\n  \nNaN\n  \n1.0\n\n                \nJCP\n  \nNaN\n  \nNaN\n  \n4.0\n\n\n\n# for each pair of (`customer`, `proof_on_ctd_ink`), get the max value of `hardener`\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\npivot\n(\n'proof_on_ctd_ink'\n)\n.\nmax\n(\n'hardener'\n)\n.\nshow\n()\n\n    \n...\n\n          \ncustomer\n        \nNO\n  \nYES\n\n         \nWOOLWORTH\n  \n1.0\n  \n0.0\n  \n1.3\n\n          \nHOMESHOP\n  \nNaN\n  \nNaN\n  \n1.8\n\n      \nhomeshopping\n  \nNaN\n  \nNaN\n  \nNaN\n\n            \nGLOBAL\n  \nNaN\n  \nNaN\n  \n1.1\n\n               \nJCP\n  \nNaN\n  \nNaN\n  \n1.7\n\n\n\n# specify the pivot values, more efficient\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\npivot\n(\n'proof_on_ctd_ink'\n,\n \nvalues\n=\n[\n'YES'\n,\n \n'NO'\n])\n.\ncount\n()\n.\nshow\n()\n\n    \n...\n\n           \ncustomer\n  \nYES\n   \nNO\n\n          \nWOOLWORTH\n  \n7.0\n  \n1.0\n\n           \nHOMESHOP\n  \n2.0\n  \nNaN\n\n       \nhomeshopping\n  \nNaN\n  \nNaN\n\n             \nGLOBAL\n  \n1.0\n  \nNaN\n\n                \nJCP\n  \n4.0\n  \nNaN\n\n\n\n\n\n\nmin\n\u00b6\n\n\nGroupedDataframe\n.\nmin\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nComputes the min value for each numeric column for each group.\n\n\nArguments\n\n\n\n\ncolumns\n: list of column names (string). Non-numeric columns are ignored.\n\n\n\n\nExample\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\nmin\n(\n'hardener'\n,\n \n'wax'\n)\n.\nshow\n()\n\n    \n...\n\n          \ncustomer\n  \nmin\n(\nhardener\n)\n  \nmin\n(\nwax\n)\n\n         \nWOOLWORTH\n            \n0.0\n      \n0.00\n\n          \nHOMESHOP\n            \n0.8\n      \n2.50\n\n      \nhomeshopping\n            \nNaN\n      \n2.50\n\n            \nGLOBAL\n            \n1.1\n      \n3.00\n\n               \nJCP\n            \n0.6\n      \n1.75\n\n\n\n\n\n\nCUBE\n\u00b6\n\n\nstr(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.\nstr\n() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\nagg\n\u00b6\n\n\nGroupedDataframe\n.\nagg\n(\nself\n,\n \n*\nexprs\n)\n\n\n\n\n\n\nCompute aggregates and returns the result as a DataFrame.\n\n\nIf exprs is a single dict mapping from string to string,\nthen the key is the column to perform aggregation on, and the value is the aggregate function.\n\n\nThe available aggregate functions are \navg\n, \nmax\n, \nmin\n, \nsum\n, \ncount\n.\n\n\nAlternatively, exprs can also be a list of aggregate \nColumn\n expressions.\n\n\nArguments\n\n\n\n\nexprs (dict, list)\n: a dict mapping from column name (string) to aggregate functions (string),\n\n\nor a list of :class\n:\nColumn\n\n\n\n\nExample\n\n\nimport\n \npycebes\n \nas\n \ncb\n\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\nagg\n(\ncb\n.\nmax\n(\ndf\n.\ntimestamp\n),\n \ncb\n.\nmin\n(\n'job_number'\n),\n\n                            \ncb\n.\ncount_distinct\n(\n'job_number'\n)\n.\nalias\n(\n'cnt'\n))\n.\nshow\n()\n\n    \n...\n\n           \ncustomer\n  \nmax\n(\ntimestamp\n)\n  \nmin\n(\njob_number\n)\n  \ncnt\n\n          \nWOOLWORTH\n        \n19920405\n            \n34227\n    \n5\n\n           \nHOMESHOP\n        \n19910129\n            \n38064\n    \n1\n\n       \nhomeshopping\n        \n19920712\n            \n36568\n    \n1\n\n             \nGLOBAL\n        \n19900621\n            \n36846\n    \n1\n\n                \nJCP\n        \n19910322\n            \n34781\n    \n2\n\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\nagg\n({\n'timestamp'\n:\n \n'max'\n,\n \n'job_number'\n:\n \n'min'\n,\n \n'job_number'\n:\n \n'count'\n})\n.\nshow\n()\n\n    \n...\n\n           \ncustomer\n  \nmax\n(\ntimestamp\n)\n  \ncount\n(\njob_number\n)\n\n          \nWOOLWORTH\n        \n19920405\n                  \n9\n\n           \nHOMESHOP\n        \n19910129\n                  \n2\n\n       \nhomeshopping\n        \n19920712\n                  \n1\n\n             \nGLOBAL\n        \n19900621\n                  \n1\n\n                \nJCP\n        \n19910322\n                  \n4\n\n\n\n\n\n\nmean\n\u00b6\n\n\nGroupedDataframe\n.\nmean\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nComputes the average value for each numeric column for each group.\n\n\nArguments\n\n\n\n\ncolumns\n: list of column names (string). Non-numeric columns are ignored.\n\n\n\n\nExample\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\nmean\n(\n'hardener'\n,\n \n'wax'\n)\n.\nshow\n()\n\n    \n...\n\n           \ncustomer\n  \navg\n(\nhardener\n)\n  \navg\n(\nwax\n)\n\n          \nWOOLWORTH\n       \n0.811111\n  \n1.744444\n\n           \nHOMESHOP\n       \n1.300000\n  \n2.500000\n\n       \nhomeshopping\n            \nNaN\n  \n2.500000\n\n             \nGLOBAL\n       \n1.100000\n  \n3.000000\n\n                \nJCP\n       \n0.975000\n  \n2.437500\n\n\n\n\n\n\nmax\n\u00b6\n\n\nGroupedDataframe\n.\nmax\n(\nself\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nComputes the max value for each numeric column for each group.\n\n\nArguments\n\n\n\n\ncolumns\n: list of column names (string). Non-numeric columns are ignored.\n\n\n\n\nExample\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\nmax\n(\n'hardener'\n,\n \n'wax'\n)\n.\nshow\n()\n\n    \n...\n\n           \ncustomer\n  \nmax\n(\nhardener\n)\n  \nmax\n(\nwax\n)\n\n          \nWOOLWORTH\n            \n1.3\n       \n2.6\n\n           \nHOMESHOP\n            \n1.8\n       \n2.5\n\n       \nhomeshopping\n            \nNaN\n       \n2.5\n\n             \nGLOBAL\n            \n1.1\n       \n3.0\n\n                \nJCP\n            \n1.7\n       \n3.0\n\n\n\n\n\n\nROLLUP\n\u00b6\n\n\nstr(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.\nstr\n() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\ncount\n\u00b6\n\n\nGroupedDataframe\n.\ncount\n(\nself\n)\n\n\n\n\n\n\nCounts the number of records for each group.\n\n\nExample\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n)\n.\ncount\n()\n.\nshow\n()\n\n    \n...\n\n          \ncustomer\n  \ncount\n\n         \nWOOLWORTH\n      \n9\n\n          \nHOMESHOP\n      \n2\n\n      \nhomeshopping\n      \n1\n\n            \nGLOBAL\n      \n1\n\n               \nJCP\n      \n4\n\n\n\ndf\n.\ngroupby\n(\ndf\n.\ncustomer\n,\n \n'proof_on_ctd_ink'\n)\n.\ncount\n()\n.\nshow\n()\n\n    \n...\n\n           \ncustomer\n \nproof_on_ctd_ink\n  \ncount\n\n         \nGUIDEPOSTS\n              \nYES\n      \n5\n\n           \nhomeshop\n                       \n4\n\n      \ncolorfulimage\n                       \n1\n\n          \nCASLIVING\n              \nYES\n      \n2\n\n              \nABBEY\n              \nYES\n      \n4",
            "title": "Dataframe API Reference"
        },
        {
            "location": "/dataframe_reference/#dataframe",
            "text": "",
            "title": "Dataframe"
        },
        {
            "location": "/dataframe_reference/#union",
            "text": "",
            "title": "union"
        },
        {
            "location": "/dataframe_reference/#schema",
            "text": "",
            "title": "schema"
        },
        {
            "location": "/dataframe_reference/#drop_duplicates",
            "text": "",
            "title": "drop_duplicates"
        },
        {
            "location": "/dataframe_reference/#from_json",
            "text": "",
            "title": "from_json"
        },
        {
            "location": "/dataframe_reference/#select",
            "text": "",
            "title": "select"
        },
        {
            "location": "/dataframe_reference/#subtract",
            "text": "",
            "title": "subtract"
        },
        {
            "location": "/dataframe_reference/#id",
            "text": "",
            "title": "id"
        },
        {
            "location": "/dataframe_reference/#agg",
            "text": "",
            "title": "agg"
        },
        {
            "location": "/dataframe_reference/#cube",
            "text": "",
            "title": "cube"
        },
        {
            "location": "/dataframe_reference/#drop",
            "text": "",
            "title": "drop"
        },
        {
            "location": "/dataframe_reference/#broadcast",
            "text": "",
            "title": "broadcast"
        },
        {
            "location": "/dataframe_reference/#with_column_renamed",
            "text": "",
            "title": "with_column_renamed"
        },
        {
            "location": "/dataframe_reference/#join",
            "text": "",
            "title": "join"
        },
        {
            "location": "/dataframe_reference/#columns",
            "text": "",
            "title": "columns"
        },
        {
            "location": "/dataframe_reference/#sample",
            "text": "",
            "title": "sample"
        },
        {
            "location": "/dataframe_reference/#limit",
            "text": "",
            "title": "limit"
        },
        {
            "location": "/dataframe_reference/#alias",
            "text": "",
            "title": "alias"
        },
        {
            "location": "/dataframe_reference/#shape",
            "text": "",
            "title": "shape"
        },
        {
            "location": "/dataframe_reference/#dropna",
            "text": "",
            "title": "dropna"
        },
        {
            "location": "/dataframe_reference/#show",
            "text": "",
            "title": "show"
        },
        {
            "location": "/dataframe_reference/#fillna",
            "text": "",
            "title": "fillna"
        },
        {
            "location": "/dataframe_reference/#sort",
            "text": "",
            "title": "sort"
        },
        {
            "location": "/dataframe_reference/#rollup",
            "text": "",
            "title": "rollup"
        },
        {
            "location": "/dataframe_reference/#where",
            "text": "",
            "title": "where"
        },
        {
            "location": "/dataframe_reference/#with_storage_type",
            "text": "",
            "title": "with_storage_type"
        },
        {
            "location": "/dataframe_reference/#intersect",
            "text": "",
            "title": "intersect"
        },
        {
            "location": "/dataframe_reference/#with_column",
            "text": "",
            "title": "with_column"
        },
        {
            "location": "/dataframe_reference/#with_variable_type",
            "text": "",
            "title": "with_variable_type"
        },
        {
            "location": "/dataframe_reference/#take",
            "text": "",
            "title": "take"
        },
        {
            "location": "/dataframe_reference/#groupby",
            "text": "",
            "title": "groupby"
        },
        {
            "location": "/dataframe_reference/#groupeddataframe",
            "text": "",
            "title": "GroupedDataframe"
        },
        {
            "location": "/dataframe_reference/#groupby_1",
            "text": "",
            "title": "GROUPBY"
        },
        {
            "location": "/dataframe_reference/#mean",
            "text": "",
            "title": "mean"
        },
        {
            "location": "/dataframe_reference/#sum",
            "text": "",
            "title": "sum"
        },
        {
            "location": "/dataframe_reference/#pivot",
            "text": "",
            "title": "pivot"
        },
        {
            "location": "/dataframe_reference/#min",
            "text": "",
            "title": "min"
        },
        {
            "location": "/dataframe_reference/#cube_1",
            "text": "",
            "title": "CUBE"
        },
        {
            "location": "/dataframe_reference/#agg_1",
            "text": "",
            "title": "agg"
        },
        {
            "location": "/dataframe_reference/#mean_1",
            "text": "",
            "title": "mean"
        },
        {
            "location": "/dataframe_reference/#max",
            "text": "",
            "title": "max"
        },
        {
            "location": "/dataframe_reference/#rollup_1",
            "text": "",
            "title": "ROLLUP"
        },
        {
            "location": "/dataframe_reference/#count",
            "text": "",
            "title": "count"
        },
        {
            "location": "/dataframe_functions/",
            "text": "pycebes.core.functions\n\u00b6\n\n\njson_tuple\n\u00b6\n\n\njson_tuple\n(\ncolumn\n,\n \n*\nfields\n)\n\n\n\n\n\n\nCreates a new row for a json column according to the given field names\n\n\ntanh\n\u00b6\n\n\ntanh\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the hyperbolic tangent of the given value\n\n\nweekofyear\n\u00b6\n\n\nweekofyear\n(\ncolumn\n)\n\n\n\n\n\n\nExtracts the week number as an integer from a given date/timestamp/string\n\n\nnanvl\n\u00b6\n\n\nnanvl\n(\ncolumn1\n,\n \ncolumn2\n)\n\n\n\n\n\n\nReturns col1 if it is not NaN, or col2 if col1 is NaN.\nBoth inputs should be floating point columns (DoubleType or FloatType).\n\n\nunix_timestamp\n\u00b6\n\n\nunix_timestamp\n(\ncolumn\n=\nNone\n,\n \npattern\n=\n'yyyy-MM-dd HH:mm:ss'\n)\n\n\n\n\n\n\nConvert time string in \ncolumn\n with given \npattern\n\n(see [http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html])\nto Unix time stamp (in seconds), return null if fail.\n\n\nIf \ncolumn=None\n, the current Unix timestamp (computed by :func:\ncurrent_timestamp\n) will be used\n\n\nbase64\n\u00b6\n\n\nbase64\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the BASE64 encoding of a binary column and returns it as a string column.\nThis is the reverse of unbase64\n\n\nrepeat\n\u00b6\n\n\nrepeat\n(\ncolumn\n,\n \nn\n=\n1\n)\n\n\n\n\n\n\nRepeats a string column n times, and returns it as a new string column\n\n\nexpm1\n\u00b6\n\n\nexpm1\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the exponential of the given value minus one\n\n\nformat_string\n\u00b6\n\n\nformat_string\n(\nfmt\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nFormats the arguments in printf-style and returns the result as a string column\n\n\nexplode\n\u00b6\n\n\nexplode\n(\ncolumn\n)\n\n\n\n\n\n\nCreates a new row for each element in the given array or map column\n\n\nmd5\n\u00b6\n\n\nmd5\n(\ncolumn\n)\n\n\n\n\n\n\nCalculates the MD5 digest of a binary column and returns the value\nas a 32 character hex string\n\n\nposexplode\n\u00b6\n\n\nposexplode\n(\ncolumn\n)\n\n\n\n\n\n\nCreates a new row for each element with position in the given array or map column\n\n\nget_json_object\n\u00b6\n\n\nget_json_object\n(\ncolumn\n,\n \npath\n=\n''\n)\n\n\n\n\n\n\nExtracts json object from a json string based on json path specified, and returns json string\nof the extracted json object. It will return null if the input json string is invalid.\n\n\nvar_samp\n\u00b6\n\n\nvar_samp\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the unbiased variance of the values in a group\n\n\nSee \nvar_pop\n for computing the population variance.\n\n\ndate_add\n\u00b6\n\n\ndate_add\n(\ncolumn\n,\n \ndays\n=\n1\n)\n\n\n\n\n\n\nReturns the date that is \ndays\n days after the date in \ncolumn\n\n\nlog\n\u00b6\n\n\nlog\n(\ncolumn\n,\n \nbase\n=\nNone\n)\n\n\n\n\n\n\nComputes the natural logarithm of the given value if \nbase\n is None.\nIf \nbase\n is not None, compute the logarithm with the given base of the column.\n\n\ncbrt\n\u00b6\n\n\ncbrt\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the cube-root of the given value\n\n\nsubstring_index\n\u00b6\n\n\nsubstring_index\n(\ncolumn\n,\n \ndelim\n=\n' '\n,\n \ncnt\n=\n1\n)\n\n\n\n\n\n\nReturns the substring from string \ncolumn\n before \ncnt\n occurrences of the delimiter \ndelim\n.\nIf \ncnt\n is positive, everything the left of the final delimiter (counting from left) is\nreturned. If \ncnt\n is negative, every to the right of the final delimiter (counting from the\nright) is returned. substring_index performs a case-sensitive match when searching for \ndelim\n.\n\n\nltrim\n\u00b6\n\n\nltrim\n(\ncolumn\n)\n\n\n\n\n\n\nTrim the spaces from left end for the specified string value\n\n\nregexp_extract\n\u00b6\n\n\nregexp_extract\n(\ncolumn\n,\n \nregexp\n=\n''\n,\n \ngroup_idx\n=\n0\n)\n\n\n\n\n\n\nExtract a specific group matched by a Java regex, from the specified string column.\nIf the regex did not match, or the specified group did not match, an empty string is returned.\n\n\nstruct\n\u00b6\n\n\nstruct\n(\n*\ncolumns\n)\n\n\n\n\n\n\nCreates a new struct column.\nIf the input column is a column in a \nDataframe\n, or a derived column expression\nthat is named (i.e. aliased), its name would be remained as the StructField's name,\notherwise, the newly generated StructField's name would be auto generated as col${index + 1},\ni.e. col1, col2, col3, ...\n\n\nbin\n\u00b6\n\n\nbin\n(\ncolumn\n)\n\n\n\n\n\n\nAn expression that returns the string representation of the binary value of the given long column.\nFor example, bin(\"12\") returns \"1100\".\n\n\nascii\n\u00b6\n\n\nascii\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the numeric value of the first character of the string column, and returns the\nresult as an int column\n\n\nsum\n\u00b6\n\n\nsum\n(\ncolumn\n,\n \nis_distinct\n=\nFalse\n)\n\n\n\n\n\n\nReturns the sum of all values in the expression\n\n\nArguments\n\n\n\n\ncolumn (Column)\n: the column to compute the sum\n\n\nis_distinct (bool)\n: whether to only compute the sum of distinct values.\n        False by default, meaning computing the sum of all values\n\n\n\n\nfirst\n\u00b6\n\n\nfirst\n(\ncolumn\n,\n \nignore_nulls\n=\nFalse\n)\n\n\n\n\n\n\nReturns the first value in a group.\n\n\nThe function by default returns the first values it sees. It will return the first non-null\nvalue it sees when \nignore_nulls\n is set to true. If all values are null, then null is returned.\n\n\ncrc32\n\u00b6\n\n\ncrc32\n(\ncolumn\n)\n\n\n\n\n\n\nCalculates the cyclic redundancy check value  (CRC32) of a binary column and\nreturns the value as a bigint\n\n\nhour\n\u00b6\n\n\nhour\n(\ncolumn\n)\n\n\n\n\n\n\nExtracts the hours as an integer from a given date/timestamp/string\n\n\ncount\n\u00b6\n\n\ncount\n(\ncolumn\n)\n\n\n\n\n\n\nreturns the number of items in a group\n\n\nupper\n\u00b6\n\n\nupper\n(\ncolumn\n)\n\n\n\n\n\n\nConverts a string column to upper case\n\n\ncovar_pop\n\u00b6\n\n\ncovar_pop\n(\ncolumn1\n,\n \ncolumn2\n)\n\n\n\n\n\n\nReturns the population covariance for two columns\n\n\nunhex\n\u00b6\n\n\nunhex\n(\ncolumn\n)\n\n\n\n\n\n\nInverse of hex. Interprets each pair of characters as a hexadecimal number\nand converts to the byte representation of number.\n\n\nis_null\n\u00b6\n\n\nis_null\n(\ncolumn\n)\n\n\n\n\n\n\nReturn true iff the column is null\n\n\nmax\n\u00b6\n\n\nmax\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the kurtosis of the values in a group\n\n\nsha1\n\u00b6\n\n\nsha1\n(\ncolumn\n)\n\n\n\n\n\n\nCalculates the SHA-1 digest of a binary column and returns the value\nas a 40 character hex string\n\n\nsort_array\n\u00b6\n\n\nsort_array\n(\ncolumn\n,\n \nasc\n=\nTrue\n)\n\n\n\n\n\n\nSorts the input array for the given column in ascending / descending order,\naccording to the natural ordering of the array elements.\n\n\nlast_day\n\u00b6\n\n\nlast_day\n(\ncolumn\n)\n\n\n\n\n\n\nGiven a date column, returns the last day of the month which the given date belongs to.\nFor example, input \"2015-07-27\" returns \"2015-07-31\" since July 31 is the last day of the\nmonth in July 2015.\n\n\nreverse\n\u00b6\n\n\nreverse\n(\ncolumn\n)\n\n\n\n\n\n\nReverses the string column and returns it as a new string column\n\n\ninstr\n\u00b6\n\n\ninstr\n(\ncolumn\n,\n \nsubstr\n=\n''\n)\n\n\n\n\n\n\nLocate the position of the first occurrence of substr column in the given string.\nReturns null if either of the arguments are null.\n\n\n\n\nThe position is not zero based, but 1 based index, returns 0 if substr\n   could not be found in \ncolumn\n\n\n\n\nrtrim\n\u00b6\n\n\nrtrim\n(\ncolumn\n)\n\n\n\n\n\n\nTrim the spaces from right end for the specified string value\n\n\nsignum\n\u00b6\n\n\nsignum\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the signum of the given value\n\n\nround\n\u00b6\n\n\nround\n(\ncolumn\n,\n \nscale\n=\n0\n)\n\n\n\n\n\n\nRound the value of \ncolumn\n to \nscale\n decimal places if \nscale >= 0\n\nor at integral part when \nscale < 0\n\n\nnegate\n\u00b6\n\n\nnegate\n(\ncolumn\n)\n\n\n\n\n\n\nUnary minus, i.e. negate the expression.\n\n\nExample\n\n\n# Select the amount column and negates all values.\n\n\ndf\n.\nselect\n(\n-\ndf\n.\namount\n)\n\n\ndf\n.\nselect\n(\nfunctions\n.\nnegate\n(\ndf\n.\namount\n))\n\n\ndf\n.\nselect\n(\nfunctions\n.\nnegate\n(\n\"amount\"\n))\n\n\n\n\n\n\nconcat\n\u00b6\n\n\nconcat\n(\n*\ncolumns\n)\n\n\n\n\n\n\nConcatenates multiple input string columns together into a single string column\n\n\narray_contains\n\u00b6\n\n\narray_contains\n(\ncolumn\n,\n \nvalue\n)\n\n\n\n\n\n\nReturns true if the array contains \nvalue\n\n\nshift_right_unsigned\n\u00b6\n\n\nshift_right_unsigned\n(\ncolumn\n,\n \nnum_bits\n=\n1\n)\n\n\n\n\n\n\nUnsigned shift the given value \nnum_bits\n right. If the given value is a long value, this function\nwill return a long value else it will return an integer value.\n\n\ncurrent_date\n\u00b6\n\n\ncurrent_date\n()\n\n\n\n\n\n\nReturns the current date as a date column\n\n\ncollect_list\n\u00b6\n\n\ncollect_list\n(\ncolumn\n)\n\n\n\n\n\n\nReturns a list of objects with duplicates\n\n\ntrim\n\u00b6\n\n\ntrim\n(\ncolumn\n)\n\n\n\n\n\n\nTrim the spaces from both ends for the specified string column\n\n\nlog10\n\u00b6\n\n\nlog10\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the logarithm of the given value in base 10\n\n\ninitcap\n\u00b6\n\n\ninitcap\n(\ncolumn\n)\n\n\n\n\n\n\nReturns a new string column by converting the first letter of each word to uppercase.\nWords are delimited by whitespace.\nFor example, \"hello world\" will become \"Hello World\".\n\n\nto_utc_timestamp\n\u00b6\n\n\nto_utc_timestamp\n(\ncolumn\n,\n \ntz\n=\n''\n)\n\n\n\n\n\n\nAssumes given timestamp is in given timezone and converts to UTC\n\n\ncount_distinct\n\u00b6\n\n\ncount_distinct\n(\n*\nargs\n)\n\n\n\n\n\n\nreturns the number of distinct items in a group\n\n\ncreate_map\n\u00b6\n\n\ncreate_map\n(\n*\ncolumns\n)\n\n\n\n\n\n\nCreates a new map column. The input columns must be grouped as key-value pairs, e.g.\n(key1, value1, key2, value2, ...). The key columns must all have the same data type, and can't\nbe null. The value columns must all have the same data type.\n\n\nsubstring\n\u00b6\n\n\nsubstring\n(\ncolumn\n,\n \npos\n=\n0\n,\n \nl\n=\n1\n)\n\n\n\n\n\n\nSubstring starts at \npos\n and is of length \nl\n when str is String type or\nreturns the slice of byte array that starts at \npos\n in byte and is of length \nl\n\nwhen \ncolumn\n is Binary type\n\n\nArguments\n\n\n\n\ncolumn\n: a \nColumn\n object, or a column name\n\n\npos\n: starting position, can be an integer, or a \nColumn\n with its expression giving an integer value\n\n\nl\n: length of the substring, can be an integer, or a \nColumn\n with its expression giving an integer value\n\n\n\n\nmin\n\u00b6\n\n\nmin\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the kurtosis of the values in a group\n\n\nrpad\n\u00b6\n\n\nrpad\n(\ncolumn\n,\n \npad_length\n=\n1\n,\n \npad\n=\n''\n)\n\n\n\n\n\n\nLeft-pad the string column\n\n\nfrom_unixtime\n\u00b6\n\n\nfrom_unixtime\n(\ncolumn\n,\n \nfmt\n=\n'yyyy-MM-dd HH:mm:ss'\n)\n\n\n\n\n\n\nConverts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\nrepresenting the timestamp of that moment in the current system time zone in the given\nformat.\n\n\nlocate\n\u00b6\n\n\nlocate\n(\nsubstr\n,\n \ncolumn\n,\n \npos\n=\nNone\n)\n\n\n\n\n\n\nLocate the position of the first occurrence of substr.\nIf \npos\n is not None, only search after position \npos\n.\n\n\n\n\nThe position is not zero based, but 1 based index, returns 0 if substr\n   could not be found in str.\n\n\n\n\nwhen\n\u00b6\n\n\nwhen\n(\ncondition\n,\n \nvalue\n)\n\n\n\n\n\n\nEvaluates a list of conditions and returns one of multiple possible result expressions.\nIf otherwise is not defined at the end, null is returned for unmatched conditions.\n\n\nArguments\n\n\n\n\ncondition (Column)\n: the condition\n\n\nvalue\n: value to take when condition is true\n\n\n\n\nExample\n\n\n# encoding gender string column into integer\n\n\npeople\n.\nselect\n(\nfunctions\n.\nwhen\n(\npeople\n.\ngender\n \n==\n \n'male'\n,\n \n0\n)\n\n    \n.\nwhen\n(\npeople\n.\ngender\n \n==\n \n'female'\n,\n \n1\n)\n\n    \n.\notherwise\n(\n2\n))\n\n\n\n\n\n\nlog1p\n\u00b6\n\n\nlog1p\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the natural logarithm of the given value plus one\n\n\nshift_right\n\u00b6\n\n\nshift_right\n(\ncolumn\n,\n \nnum_bits\n=\n1\n)\n\n\n\n\n\n\nShift the given value \nnum_bits\n right. If the given value is a long value, this function\nwill return a long value else it will return an integer value.\n\n\ninput_file_name\n\u00b6\n\n\ninput_file_name\n()\n\n\n\n\n\n\nSpark-specific: Creates a string column for the file name of the current Spark task\n\n\nexpr\n\u00b6\n\n\nexpr\n(\nexpr_str\n=\n''\n)\n\n\n\n\n\n\nParses the expression string into the column that it represents\n\n\nExample\n\n\n# get the number of words of each length\n\n\ndf\n.\ngroup_by\n(\nfunctions\n.\nexpr\n(\n\"length(word)\"\n))\n.\ncount\n()\n\n\n\n\n\n\ncovar_samp\n\u00b6\n\n\ncovar_samp\n(\ncolumn1\n,\n \ncolumn2\n)\n\n\n\n\n\n\nReturns the sample covariance for two columns\n\n\ndayofyear\n\u00b6\n\n\ndayofyear\n(\ncolumn\n)\n\n\n\n\n\n\nExtracts the day of the year as an integer from a given date/timestamp/string\n\n\ndate_format\n\u00b6\n\n\ndate_format\n(\ncolumn\n,\n \nfmt\n=\n''\n)\n\n\n\n\n\n\nConverts a date/timestamp/string to a value of string in the format specified by the date\nformat given by the second argument.\n\n\nA pattern could be for instance \ndd.MM.yyyy\n and could return a string like '18.03.1993'. All\npattern letters of [[java.text.SimpleDateFormat]] can be used.\n\n\n\n\nUse when ever possible specialized functions like :func:\nyear\n. These benefit from a\n    specialized implementation.\n\n\n\n\nshift_left\n\u00b6\n\n\nshift_left\n(\ncolumn\n,\n \nnum_bits\n=\n1\n)\n\n\n\n\n\n\nShift the given value \nnum_bits\n left. If the given value is a long value, this function\nwill return a long value else it will return an integer value.\n\n\nmonotonically_increasing_id\n\u00b6\n\n\nmonotonically_increasing_id\n()\n\n\n\n\n\n\nSpark-specific: A column expression that generates monotonically increasing 64-bit integers.\n\n\nThe generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\nThe current implementation puts the partition ID in the upper 31 bits, and the record number\nwithin each partition in the lower 33 bits. The assumption is that the data frame has\nless than 1 billion partitions, and each partition has less than 8 billion records.\n\n\nAs an example, consider a \nDataframe\n with two partitions, each with 3 records.\nThis expression would return the following IDs:\n\n\n0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n\n\nunbase64\n\u00b6\n\n\nunbase64\n(\ncolumn\n)\n\n\n\n\n\n\nDecodes a BASE64 encoded string column and returns it as a binary column.\nThis is the reverse of base64.\n\n\ntrunc\n\u00b6\n\n\ntrunc\n(\ncolumn\n,\n \nfmt\n=\n'year'\n)\n\n\n\n\n\n\nReturns date truncated to the unit specified by the format.\n\n\nArguments\n\n\n\n\ncolumn\n: the source column, can be a \nColumn\n object or a string (column name)\n\n\nfmt\n: 'year', 'yyyy', 'yy' for truncate by year,\n        or 'month', 'mon', 'mm' for truncate by month\n\n\n\n\nspark_partition_id\n\u00b6\n\n\nspark_partition_id\n()\n\n\n\n\n\n\nSpark-specific: Partition ID of the Spark task.\n\n\n\n\nThis is un-deterministic because it depends on data partitioning and task scheduling.\n\n\n\n\nsplit\n\u00b6\n\n\nsplit\n(\ncolumn\n,\n \npattern\n=\n''\n)\n\n\n\n\n\n\nSplits str around pattern (pattern is a regular expression)\n\n\n\n\npattern is a string representation of the regular expression\n\n\n\n\ngrouping_id\n\u00b6\n\n\ngrouping_id\n(\n*\ncolumns\n)\n\n\n\n\n\n\nreturns the level of grouping, equals to\n\n\n(grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n\n\n\n\nthe list of columns should match with grouping columns exactly, or empty (means all the\n   grouping columns).\n\n\n\n\nsize\n\u00b6\n\n\nsize\n(\ncolumn\n)\n\n\n\n\n\n\nReturns length of array or map\n\n\nrint\n\u00b6\n\n\nrint\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the double value that is closest in value to the argument and\nis equal to a mathematical integer.\n\n\ncos\n\u00b6\n\n\ncos\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the cosine of the given value\n\n\ncurrent_timestamp\n\u00b6\n\n\ncurrent_timestamp\n()\n\n\n\n\n\n\nReturns the current timestamp as a timestamp column\n\n\napprox_count_distinct\n\u00b6\n\n\napprox_count_distinct\n(\ncolumn\n,\n \nrsd\n=\n0.05\n)\n\n\n\n\n\n\nReturns the approximate number of distinct items in a group.\n\n\nArguments:\n\n\ncolumn (Column): the column to compute\nrsd: maximum estimation error allowed (default = 0.05)\n\n\nstddev_samp\n\u00b6\n\n\nstddev_samp\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the sample standard deviation of the expression in a group.\n\n\nsha2\n\u00b6\n\n\nsha2\n(\ncolumn\n,\n \nnum_bits\n=\n0\n)\n\n\n\n\n\n\nCalculates the SHA-2 family of hash functions of a binary column and\nreturns the value as a hex string\n\n\nArguments\n\n\n\n\ncolumn\n: column to compute SHA-2 on. Can be a \nColumn\n or a string (a column name)\n\n\nnum_bits\n: one of 224, 256, 384, or 512\n\n\n\n\nbround\n\u00b6\n\n\nbround\n(\ncolumn\n,\n \nscale\n=\n0\n)\n\n\n\n\n\n\nRound the value of \ncolumn\n to \nscale\n decimal places with HALF_EVEN round mode if \nscale >= 0\n\nor at integral part when \nscale < 0\n\n\nhypot\n\u00b6\n\n\nhypot\n(\nx\n,\n \ny\n)\n\n\n\n\n\n\nComputes \nsqrt(x^2 + y^2)\n without intermediate overflow or underflow\nSee \natan2\n to compute the angle.\n\n\nArguments\n\n\n\n\nx\n: can be a \nColumn\n, a string (column name), or a double value\n\n\ny\n: can be a \nColumn\n, a string (column name), or a double value\n\n\n\n\ngreatest\n\u00b6\n\n\ngreatest\n(\n*\ncolumns\n)\n\n\n\n\n\n\nReturns the greatest value of the list of values, skipping null values.\nThis function takes at least 2 parameters. It will return null iff all parameters are null.\n\n\npmod\n\u00b6\n\n\npmod\n(\ndividend\n,\n \ndivisor\n)\n\n\n\n\n\n\nReturns the positive value of dividend mod divisor.\n\ndividend\n and \ndivisor\n can be \nColumn\n or strings (column names)\n\n\nconv\n\u00b6\n\n\nconv\n(\ncolumn\n,\n \nfrom_base\n=\n10\n,\n \nto_base\n=\n2\n)\n\n\n\n\n\n\nConvert a number in a string column from one base to another\n\n\nhash\n\u00b6\n\n\nhash\n(\ncolumn\n)\n\n\n\n\n\n\nCalculates the hash code of given columns, and returns the result as an int column\n\n\nlength\n\u00b6\n\n\nlength\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the length of a given string or binary column\n\n\nskewness\n\u00b6\n\n\nskewness\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the kurtosis of the values in a group\n\n\ndecode\n\u00b6\n\n\ndecode\n(\ncolumn\n,\n \ncharset\n=\n'US-ASCII'\n)\n\n\n\n\n\n\nComputes the first argument into a string from a binary using the provided character set\n(one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n\n\nIf either argument is null, the result will also be null.\n\n\narray\n\u00b6\n\n\narray\n(\n*\ncolumns\n)\n\n\n\n\n\n\nCreates a new array column. The input columns must all have the same data type.\n\n\nfloor\n\u00b6\n\n\nfloor\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the floor of the given value\n\n\ndatediff\n\u00b6\n\n\ndatediff\n(\nend_column\n,\n \nstart_column\n)\n\n\n\n\n\n\nReturns the number of days from \nstart\n to \nend\n\n\nacos\n\u00b6\n\n\nacos\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the cosine inverse of the given value; the returned angle is in the range 0.0 through pi.\n\n\navg\n\u00b6\n\n\navg\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the average of the values in a group\n\n\nfrom_utc_timestamp\n\u00b6\n\n\nfrom_utc_timestamp\n(\ncolumn\n,\n \ntz\n=\n''\n)\n\n\n\n\n\n\nAssumes given timestamp is UTC and converts to given timezone\n\n\nfactorial\n\u00b6\n\n\nfactorial\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the factorial of the given value\n\n\ndayofmonth\n\u00b6\n\n\ndayofmonth\n(\ncolumn\n)\n\n\n\n\n\n\nExtracts the day of the month as an integer from a given date/timestamp/string\n\n\navg\n\u00b6\n\n\navg\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the average of the values in a group\n\n\nrandn\n\u00b6\n\n\nrandn\n(\nseed\n=\nNone\n)\n\n\n\n\n\n\nGenerate a column with i.i.d. samples from the standard normal distribution.\n\nseed\n will be automatically randomly generated by \nrandom.randint(0, 1000)\n if not specified.\n\n\n\n\nThis is un-deterministic when data partitions are not fixed.\n\n\n\n\nsoundex\n\u00b6\n\n\nsoundex\n(\ncolumn\n)\n\n\n\n\n\n\nReturn the soundex code for the specified expression\n\n\nto_date\n\u00b6\n\n\nto_date\n(\ncolumn\n)\n\n\n\n\n\n\nConverts the column into DateType\n\n\nbitwise_not\n\u00b6\n\n\nbitwise_not\n(\ncolumn\n)\n\n\n\n\n\n\nComputes bitwise NOT.\n\n\nstddev_pop\n\u00b6\n\n\nstddev_pop\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the population standard deviation of the expression in a group.\n\n\nconcat_ws\n\u00b6\n\n\nconcat_ws\n(\nsep\n=\n' '\n,\n \n*\ncolumns\n)\n\n\n\n\n\n\nConcatenates multiple input string columns together into a single string column,\nusing the given separator\n\n\nabs\n\u00b6\n\n\nabs\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the absolute value\n\n\nto_radians\n\u00b6\n\n\nto_radians\n(\ncolumn\n)\n\n\n\n\n\n\nConverts an angle measured in degrees to an approximately equivalent angle measured in radians\n\n\npow\n\u00b6\n\n\npow\n(\nleft\n,\n \nright\n)\n\n\n\n\n\n\nReturns the value of the first argument raised to the power of the second argument\n\n\nArguments\n\n\n\n\nleft\n: can be a \nColumn\n, a string (column name), or a double value\n\n\nright\n: can be a \nColumn\n, a string (column name), or a double value\n\n\n\n\nsqrt\n\u00b6\n\n\nsqrt\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the square root of the specified float value.\n\n\nregexp_replace\n\u00b6\n\n\nregexp_replace\n(\ncolumn\n,\n \npattern\n=\n''\n,\n \nreplacement\n=\n''\n)\n\n\n\n\n\n\nReplace all substrings of the specified string value that match regexp with rep.\n\n\ndate_sub\n\u00b6\n\n\ndate_sub\n(\ncolumn\n,\n \ndays\n=\n1\n)\n\n\n\n\n\n\nReturns the date that is \ndays\n days before the date in \ncolumn\n\n\nlower\n\u00b6\n\n\nlower\n(\ncolumn\n)\n\n\n\n\n\n\nConverts a string column to lower case\n\n\nlevenshtein\n\u00b6\n\n\nlevenshtein\n(\nleft\n,\n \nright\n)\n\n\n\n\n\n\nComputes the Levenshtein distance of the two given string columns\n\n\nvar_samp\n\u00b6\n\n\nvar_samp\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the unbiased variance of the values in a group\n\n\nSee \nvar_pop\n for computing the population variance.\n\n\nhex\n\u00b6\n\n\nhex\n(\ncolumn\n)\n\n\n\n\n\n\nComputes hex value of the given column\n\n\nsinh\n\u00b6\n\n\nsinh\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the hyperbolic sine of the given value\n\n\nto_degrees\n\u00b6\n\n\nto_degrees\n(\ncolumn\n)\n\n\n\n\n\n\nConverts an angle measured in radians to an approximately equivalent angle measured in degrees\n\n\ngrouping\n\u00b6\n\n\ngrouping\n(\ncolumn\n)\n\n\n\n\n\n\nindicates whether a specified column in a GROUP BY list is aggregated\nor not, returns 1 for aggregated or 0 for not aggregated in the result set.\n\n\nstddev_samp\n\u00b6\n\n\nstddev_samp\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the sample standard deviation of the expression in a group.\n\n\ntranslate\n\u00b6\n\n\ntranslate\n(\ncolumn\n,\n \nmatching_str\n=\n''\n,\n \nreplace_str\n=\n''\n)\n\n\n\n\n\n\nTranslate any character in the \ncolumn\n by a character in \nreplace_str\n.\nThe characters in \nreplace_str\n correspond to the characters in \nmatching_str\n.\nThe translate will happen when any character in the string matches the character\nin the \nmatching_str\n.\n\n\nformat_number\n\u00b6\n\n\nformat_number\n(\ncolumn\n,\n \nd\n=\n2\n)\n\n\n\n\n\n\nFormats numeric column x to a format like '#,###,##``.##', rounded to d decimal places,\nand returns the result as a string column.\n\n\n* If d is 0, the result has no decimal point or fractional part.\n* If d < 0, the result will be null.\n\n\n\n\n\nlpad\n\u00b6\n\n\nlpad\n(\ncolumn\n,\n \npad_length\n=\n1\n,\n \npad\n=\n''\n)\n\n\n\n\n\n\nLeft-pad the string column\n\n\nsin\n\u00b6\n\n\nsin\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the sine of the given value\n\n\ncoalesce\n\u00b6\n\n\ncoalesce\n(\n*\ncolumns\n)\n\n\n\n\n\n\nReturns the first column that is not null, or null if all inputs are null.\n\n\nFor example, \ncoalesce(a, b, c)\n will return a if a is not null,\nor b if a is null and b is not null, or c if both a and b are null but c is not null.\n\n\nleast\n\u00b6\n\n\nleast\n(\n*\ncolumns\n)\n\n\n\n\n\n\nReturns the least value of the list of values, skipping null values.\nThis function takes at least 2 parameters. It will return null iff all parameters are null.\n\n\nasin\n\u00b6\n\n\nasin\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the sine inverse of the given value; the returned angle is in the range -pi/2 through pi/2.\n\n\nminute\n\u00b6\n\n\nminute\n(\ncolumn\n)\n\n\n\n\n\n\nExtracts the minutes as an integer from a given date/timestamp/string\n\n\nquarter\n\u00b6\n\n\nquarter\n(\ncolumn\n)\n\n\n\n\n\n\nExtracts the quarter as an integer from a given date/timestamp/string\n\n\ntan\n\u00b6\n\n\ntan\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the tangent of the given value\n\n\nlast\n\u00b6\n\n\nlast\n(\ncolumn\n,\n \nignore_nulls\n=\nFalse\n)\n\n\n\n\n\n\nReturns the last value in a group.\n\n\nThe function by default returns the last values it sees. It will return the last non-null\nvalue it sees when \nignore_nulls\n is set to true. If all values are null, then null is returned.\n\n\nwindow\n\u00b6\n\n\nwindow\n(\ncolumn\n,\n \nwindow_duration\n=\n''\n,\n \nslide_duration\n=\nNone\n,\n \nstart_time\n=\n'0 second'\n)\n\n\n\n\n\n\nBucketize rows into one or more time windows given a timestamp specifying column. Window\nstarts are inclusive but the window ends are exclusive, e.g. \n12:05\n will be in the window\n\n[12:05,12:10)\n but not in \n[12:00,12:05)\n.\n\n\nWindows can support microsecond precision. Windows in the order of months are not supported.\n\n\nThe following example takes the average stock price for a one minute window every 10 seconds\nstarting 5 seconds after the hour:\n\n\nExample\n\n\ndf\n \n=\n \n...\n  \n# schema: timestamp: TimestampType, stockId: StringType, price: DoubleType\n\n\ndf\n.\ngroup_by\n(\nfunctions\n.\nwindow\n(\ndf\n.\ntimestamp\n,\n \n\"1 minute\"\n,\n \n\"10 seconds\"\n,\n \n\"5 seconds\"\n),\n \ndf\n.\nstockId\n)\n\n    \n.\nagg\n(\nmean\n(\n\"price\"\n))\n\n\n\n\n\n\nThe windows will look like:\n\n\n    \n09\n:\n00\n:\n05\n-\n09\n:\n01\n:\n05\n\n    \n09\n:\n00\n:\n15\n-\n09\n:\n01\n:\n15\n\n    \n09\n:\n00\n:\n25\n-\n09\n:\n01\n:\n25\n \n...\n\n\n\n\n\n\nFor a streaming query, you may use the function \ncurrent_timestamp\n to generate windows on\nprocessing time.\n\n\nArguments\n\n\n\n\ncolumn\n: The column or the expression to use as the timestamp for windowing by time.\n               The time column must be of TimestampType.\n\n\nwindow_duration\n: A string specifying the width of the window, e.g. \n10 minutes\n,\n                \n1 second\n. Check \nCalendarIntervalType\n for\n                valid duration identifiers. Note that the duration is a fixed length of\n                time, and does not vary over time according to a calendar. For example,\n                \n1 day\n always means \n86,400,000 milliseconds\n, not a calendar day.\n\n\nslide_duration\n: A string specifying the sliding interval of the window, e.g. \n1 minute\n.\n                A new window will be generated every \nslide_duration\n. Must be less than\n                or equal to the \nwindow_duration\n.\n                This duration is likewise absolute, and does not vary according to a calendar.\n                If unspecified, \nslide_duration\n will be equal to \nwindow_duration\n\n\n__start_time (str): The offset with respect to \n1970-01-01 00:00__:00 UTC\n with which to start\n                    window intervals. For example, in order to have hourly tumbling windows\n\n\n__that start 15 minutes past the hour, e.g. \n12:15 - 13:15\n, \n13:15 - 14__:15\n...\n                    provide \nstart_time\n as \n15 minutes\n.\n\n\n\n\nis_nan\n\u00b6\n\n\nis_nan\n(\ncolumn\n)\n\n\n\n\n\n\nReturn true iff the column is NaN\n\n\nmonth\n\u00b6\n\n\nmonth\n(\ncolumn\n)\n\n\n\n\n\n\nExtracts the month as an integer from a given date/timestamp/string\n\n\nencode\n\u00b6\n\n\nencode\n(\ncolumn\n,\n \ncharset\n=\n'US-ASCII'\n)\n\n\n\n\n\n\nComputes the first argument into a binary from a string using the provided character set\n(one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n\n\nIf either argument is null, the result will also be null.\n\n\ncollect_set\n\u00b6\n\n\ncollect_set\n(\ncolumn\n)\n\n\n\n\n\n\nReturns a set of objects with duplicate elements eliminated\n\n\nlog2\n\u00b6\n\n\nlog2\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the logarithm of the given column in base 2\n\n\natan2\n\u00b6\n\n\natan2\n(\nx\n,\n \ny\n)\n\n\n\n\n\n\nReturns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta)\n\n\nArguments\n\n\n\n\nx\n: the x-component, can be a \nColumn\n, a string (column name), or a float value\n\n\ny\n: the y-component, can be a \nColumn\n, a string (column name), or a float value\n\n\n\n\nnext_day\n\u00b6\n\n\nnext_day\n(\ncolumn\n,\n \nday_of_week\n=\n'mon'\n)\n\n\n\n\n\n\nGiven a date column, returns the first date which is later than the value of the date column\nthat is on the specified day of the week.\n\n\nFor example, \nnext_day('2015-07-27', \"Sunday\")\n returns 2015-08-02 because that is the first\nSunday after 2015-07-27.\n\n\nDay of the week parameter is case insensitive, and accepts:\n\n\n\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n\n\n\n\n\natan\n\u00b6\n\n\natan\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the tangent inverse of the given value\n\n\ncol\n\u00b6\n\n\ncol\n(\ncol_name\n=\n'column'\n)\n\n\n\n\n\n\nReturns a \nColumn\n based on the given column name.\n\n\nmonths_between\n\u00b6\n\n\nmonths_between\n(\ncolumn1\n,\n \ncolumn2\n)\n\n\n\n\n\n\nReturns number of months between dates \ncolumn1\n and \ncolumn2\n\n\ncosh\n\u00b6\n\n\ncosh\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the hyperbolic cosine of the given value\n\n\nadd_months\n\u00b6\n\n\nadd_months\n(\ncolumn\n,\n \nnum_months\n=\n1\n)\n\n\n\n\n\n\nReturns the date that is \nnum_months\n after the date in \ncolumn\n\n\nceil\n\u00b6\n\n\nceil\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the ceiling of the given value.\n\n\nsecond\n\u00b6\n\n\nsecond\n(\ncolumn\n)\n\n\n\n\n\n\nExtracts the seconds as an integer from a given date/timestamp/string\n\n\nvar_pop\n\u00b6\n\n\nvar_pop\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the population variance of the values in a group\n\n\nSee \nvar_samp\n for computing the unbiased variance.\n\n\ncorr\n\u00b6\n\n\ncorr\n(\ncolumn1\n,\n \ncolumn2\n)\n\n\n\n\n\n\nReturns the Pearson Correlation Coefficient for two columns\n\n\nexp\n\u00b6\n\n\nexp\n(\ncolumn\n)\n\n\n\n\n\n\nComputes the exponential of the given value\n\n\nyear\n\u00b6\n\n\nyear\n(\ncolumn\n)\n\n\n\n\n\n\nExtracts the year as an integer from a given date/timestamp/string\n\n\nrand\n\u00b6\n\n\nrand\n(\nseed\n=\nNone\n)\n\n\n\n\n\n\nGenerate a random column with i.i.d. samples from U[0.0, 1.0].\n\nseed\n will be automatically randomly generated by \nrandom.randint(0, 1000)\n if not specified.\n\n\n\n\nThis is un-deterministic when data partitions are not fixed.\n\n\n\n\nkurtosis\n\u00b6\n\n\nkurtosis\n(\ncolumn\n)\n\n\n\n\n\n\nReturns the kurtosis of the values in a group\n\n\nColumn\n\u00b6\n\n\nColumn\n(\nself\n,\n \nexpr\n)\n\n\n\n\n\n\notherwise\n\u00b6\n\n\nColumn\n.\notherwise\n(\nself\n,\n \nvalue\n)\n\n\n\n\n\n\nEvaluates a list of conditions and returns one of multiple possible result expressions.\nIf otherwise() is not defined at the end, \nnull\n is returned for unmatched conditions.\n\n\nExample\n\n\n# encoding gender string column into integer.\n\n\npeople\n.\nselect\n(\nfunctions\n.\nwhen\n(\npeople\n.\ngender\n \n==\n \n\"male\"\n,\n \n0\n)\n\n    \n.\nwhen\n(\npeople\n.\ngender\n \n==\n \n\"female\"\n,\n \n1\n)\n\n    \n.\notherwise\n(\n2\n))\n\n\n\n\n\n\nalias\n\u00b6\n\n\nColumn\n.\nalias\n(\nself\n,\n \n*\nalias\n)\n\n\n\n\n\n\nGives the column an alias, or multiple aliases in case it is used on an expression\nthat returns more than one column (such as \nexplode\n).\n\n\nExample\n\n\n# Renames colA to colB in select output.\n\n\ndf\n.\nselect\n(\ndf\n.\ncolA\n.\nalias\n(\n'colB'\n))\n\n\n\n# multiple alias\n\n\ndf\n.\nselect\n(\nfunctions\n.\nexplode\n(\ndf\n.\nmyMap\n)\n.\nalias\n(\n'key'\n,\n \n'value'\n))\n\n\n\n\n\n\nequal_null_safe\n\u00b6\n\n\nColumn\n.\nequal_null_safe\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nEquality test that is safe for null values.\nReturns same result with EQUAL(=) operator for non-null operands,\nbut returns \ntrue\n if both are \nNULL\n, \nfalse\n if one of the them is \nNULL\n.\n\n\ncontains\n\u00b6\n\n\nColumn\n.\ncontains\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nContains the other element\n\n\ncast\n\u00b6\n\n\nColumn\n.\ncast\n(\nself\n,\n \nto\n)\n\n\n\n\n\n\nCasts the column to a different data type.\n\n\nArguments\n\n\n\n\nto (StorageType)\n: the StorageType to cast to\n\n\n\n\nExample\n\n\n# Casts colA to [[IntegerType]].\n\n\ndf\n.\nselect\n(\ndf\n.\ncolA\n.\ncast\n(\nIntegerType\n))\n\n\ndf\n.\nselect\n(\ndf\n.\ncolA\n.\ncast\n(\n\"int\"\n))\n\n\n\n\n\n\ndesc\n\u00b6\n\n\nReturns an ordering used in sorting.\n\n\nExample\n\n\ndf\n.\nsort\n(\ndf\n.\ncol1\n.\ndesc\n)\n\n\n\n\n\n\nto_json\n\u00b6\n\n\nColumn\n.\nto_json\n(\nself\n)\n\n\n\n\n\n\nSerialize the Column into JSON format\n\n\nReturns\n\n\nJSON representation of this Column\n\n\nname\n\u00b6\n\n\nColumn\n.\nname\n(\nself\n,\n \n*\nalias\n)\n\n\n\n\n\n\nGives the column a name. Same as :func:\nalias\n\n\nasc\n\u00b6\n\n\nReturns an ordering used in sorting.\n\n\nExample\n\n\ndf\n.\nsort\n(\ndf\n.\ncol1\n.\nasc\n)\n\n\n\n\n\n\nbitwise_not\n\u00b6\n\n\nCompute bitwise NOT of this expression\n\n\nExample\n\n\ndf\n.\nselect\n(\ndf\n.\ncolA\n.\nbitwise_not\n)\n\n\n\n\n\n\nsubstr\n\u00b6\n\n\nColumn\n.\nsubstr\n(\nself\n,\n \nstart_pos\n,\n \nlength\n)\n\n\n\n\n\n\nAn expression that returns a substring.\n\n\nThis is not zero-based, but 1-based index. The first character in str has index 1.\n\n\nstart_pos\n and \nlength\n are handled specially:\n\n\n\n\n\"Content\".substr(1, 3)\n gives \n\"Con\"\n\n\n\"Content\".substr(-100, 2)\n gives \n\"\"\n\n\n\"Content\".substr(-100, 102)\n gives \n\"Content\"\n\n\n\"Content\".substr(2, 100)\n gives \n\"ontent\"\n\n\n\n\nArguments\n\n\n\n\nstart_pos\n: starting position, can be an integer, or a \nColumn\n that gives an integer\n\n\nlength\n: length of the sub-string, can be an integer, or a \nColumn\n that gives an integer\n\n\n\n\nstarts_with\n\u00b6\n\n\nColumn\n.\nstarts_with\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nString starts with.\n\n\nArguments\n\n\n\n\nother\n: string to test, can be a string, or a \nColumn\n  that gives a string\n\n\n\n\nisin\n\u00b6\n\n\nColumn\n.\nisin\n(\nself\n,\n \nvalues\n)\n\n\n\n\n\n\nA boolean expression that is evaluated to true if the value of this expression is contained\nby the evaluated values of the arguments.\n\n\nis_null\n\u00b6\n\n\nColumn\n.\nis_null\n(\nself\n)\n\n\n\n\n\n\nTrue if the current expression is null.\n\n\nbetween\n\u00b6\n\n\nColumn\n.\nbetween\n(\nself\n,\n \nlower_bound\n,\n \nupper_bound\n)\n\n\n\n\n\n\nTrue if the current column is between the lower bound and upper bound, inclusive.\n\n\nbitwise_or\n\u00b6\n\n\nColumn\n.\nbitwise_or\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nCompute bitwise OR of this expression with another expression.\n\n\nExample\n\n\n    \ndf\n.\nselect\n(\ndf\n.\ncolA\n.\nbitwise_or\n(\ndf\n.\ncolB\n))\n\n\n\n\n\n\nget_field\n\u00b6\n\n\nColumn\n.\nget_field\n(\nself\n,\n \nfield_name\n=\n'key'\n)\n\n\n\n\n\n\nAn expression that gets a field by name in a StructType column.\n\n\nget_item\n\u00b6\n\n\nColumn\n.\nget_item\n(\nself\n,\n \nkey\n)\n\n\n\n\n\n\nAn expression that gets an item at position \nordinal\n out of an array,\nor gets a value by key \nkey\n in a MapType column.\n\n\nis_nan\n\u00b6\n\n\nColumn\n.\nis_nan\n(\nself\n)\n\n\n\n\n\n\nTrue if the current expression is NaN.\n\n\nrlike\n\u00b6\n\n\nColumn\n.\nrlike\n(\nself\n,\n \nliteral\n)\n\n\n\n\n\n\nSQL RLIKE expression (LIKE with Regex). Result will be a BooleanType column\n\n\nArguments\n\n\n\n\nliteral\n: a string\n\n\n\n\nlike\n\u00b6\n\n\nColumn\n.\nlike\n(\nself\n,\n \nliteral\n)\n\n\n\n\n\n\nSQL like expression. Result will be a BooleanType column\n\n\nArguments\n\n\n\n\nliteral\n: a string\n\n\n\n\nbitwise_xor\n\u00b6\n\n\nColumn\n.\nbitwise_xor\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nCompute bitwise XOR of this expression with another expression.\n\n\nExample\n\n\ndf\n.\nselect\n(\ndf\n.\ncolA\n.\nbitwise_xor\n(\ndf\n.\ncolB\n))\n\n\n\n\n\n\nwhen\n\u00b6\n\n\nColumn\n.\nwhen\n(\nself\n,\n \ncondition\n,\n \nvalue\n)\n\n\n\n\n\n\nEvaluates a list of conditions and returns one of multiple possible result expressions.\nIf otherwise() is not defined at the end, \nnull\n is returned for unmatched conditions.\n\n\nArguments\n\n\n\n\ncondition\n: the condition to be checked\n\n\nvalue (Column)\n: the output value if condition is true\n\n\n\n\nExample\n\n\n# encoding gender string column into integer.\n\n\npeople\n.\nselect\n(\nfunctions\n.\nwhen\n(\npeople\n.\ngender\n \n==\n \n\"male\"\n,\n \n0\n)\n\n    \n.\nwhen\n(\npeople\n.\ngender\n \n==\n \n\"female\"\n,\n \n1\n)\n\n    \n.\notherwise\n(\n2\n))\n\n\n\n\n\n\nends_with\n\u00b6\n\n\nColumn\n.\nends_with\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nString ends with.\n\n\nArguments\n\n\n\n\nother\n: string to test, can be a string, or a \nColumn\n  that gives a string\n\n\n\n\nbitwise_and\n\u00b6\n\n\nColumn\n.\nbitwise_and\n(\nself\n,\n \nother\n)\n\n\n\n\n\n\nCompute bitwise AND of this expression with another expression.\n\n\nExample\n\n\ndf\n.\nselect\n(\ndf\n.\ncolA\n.\nbitwise_and\n(\ndf\n.\ncolB\n))\n\n\n\n\n\n\nis_not_null\n\u00b6\n\n\nColumn\n.\nis_not_null\n(\nself\n)\n\n\n\n\n\n\nTrue if the current expression is not null.",
            "title": "Functional API Reference"
        },
        {
            "location": "/dataframe_functions/#pycebescorefunctions",
            "text": "",
            "title": "pycebes.core.functions"
        },
        {
            "location": "/dataframe_functions/#json_tuple",
            "text": "",
            "title": "json_tuple"
        },
        {
            "location": "/dataframe_functions/#tanh",
            "text": "",
            "title": "tanh"
        },
        {
            "location": "/dataframe_functions/#weekofyear",
            "text": "",
            "title": "weekofyear"
        },
        {
            "location": "/dataframe_functions/#nanvl",
            "text": "",
            "title": "nanvl"
        },
        {
            "location": "/dataframe_functions/#unix_timestamp",
            "text": "",
            "title": "unix_timestamp"
        },
        {
            "location": "/dataframe_functions/#base64",
            "text": "",
            "title": "base64"
        },
        {
            "location": "/dataframe_functions/#repeat",
            "text": "",
            "title": "repeat"
        },
        {
            "location": "/dataframe_functions/#expm1",
            "text": "",
            "title": "expm1"
        },
        {
            "location": "/dataframe_functions/#format_string",
            "text": "",
            "title": "format_string"
        },
        {
            "location": "/dataframe_functions/#explode",
            "text": "",
            "title": "explode"
        },
        {
            "location": "/dataframe_functions/#md5",
            "text": "",
            "title": "md5"
        },
        {
            "location": "/dataframe_functions/#posexplode",
            "text": "",
            "title": "posexplode"
        },
        {
            "location": "/dataframe_functions/#get_json_object",
            "text": "",
            "title": "get_json_object"
        },
        {
            "location": "/dataframe_functions/#var_samp",
            "text": "",
            "title": "var_samp"
        },
        {
            "location": "/dataframe_functions/#date_add",
            "text": "",
            "title": "date_add"
        },
        {
            "location": "/dataframe_functions/#log",
            "text": "",
            "title": "log"
        },
        {
            "location": "/dataframe_functions/#cbrt",
            "text": "",
            "title": "cbrt"
        },
        {
            "location": "/dataframe_functions/#substring_index",
            "text": "",
            "title": "substring_index"
        },
        {
            "location": "/dataframe_functions/#ltrim",
            "text": "",
            "title": "ltrim"
        },
        {
            "location": "/dataframe_functions/#regexp_extract",
            "text": "",
            "title": "regexp_extract"
        },
        {
            "location": "/dataframe_functions/#struct",
            "text": "",
            "title": "struct"
        },
        {
            "location": "/dataframe_functions/#bin",
            "text": "",
            "title": "bin"
        },
        {
            "location": "/dataframe_functions/#ascii",
            "text": "",
            "title": "ascii"
        },
        {
            "location": "/dataframe_functions/#sum",
            "text": "",
            "title": "sum"
        },
        {
            "location": "/dataframe_functions/#first",
            "text": "",
            "title": "first"
        },
        {
            "location": "/dataframe_functions/#crc32",
            "text": "",
            "title": "crc32"
        },
        {
            "location": "/dataframe_functions/#hour",
            "text": "",
            "title": "hour"
        },
        {
            "location": "/dataframe_functions/#count",
            "text": "",
            "title": "count"
        },
        {
            "location": "/dataframe_functions/#upper",
            "text": "",
            "title": "upper"
        },
        {
            "location": "/dataframe_functions/#covar_pop",
            "text": "",
            "title": "covar_pop"
        },
        {
            "location": "/dataframe_functions/#unhex",
            "text": "",
            "title": "unhex"
        },
        {
            "location": "/dataframe_functions/#is_null",
            "text": "",
            "title": "is_null"
        },
        {
            "location": "/dataframe_functions/#max",
            "text": "",
            "title": "max"
        },
        {
            "location": "/dataframe_functions/#sha1",
            "text": "",
            "title": "sha1"
        },
        {
            "location": "/dataframe_functions/#sort_array",
            "text": "",
            "title": "sort_array"
        },
        {
            "location": "/dataframe_functions/#last_day",
            "text": "",
            "title": "last_day"
        },
        {
            "location": "/dataframe_functions/#reverse",
            "text": "",
            "title": "reverse"
        },
        {
            "location": "/dataframe_functions/#instr",
            "text": "",
            "title": "instr"
        },
        {
            "location": "/dataframe_functions/#rtrim",
            "text": "",
            "title": "rtrim"
        },
        {
            "location": "/dataframe_functions/#signum",
            "text": "",
            "title": "signum"
        },
        {
            "location": "/dataframe_functions/#round",
            "text": "",
            "title": "round"
        },
        {
            "location": "/dataframe_functions/#negate",
            "text": "",
            "title": "negate"
        },
        {
            "location": "/dataframe_functions/#concat",
            "text": "",
            "title": "concat"
        },
        {
            "location": "/dataframe_functions/#array_contains",
            "text": "",
            "title": "array_contains"
        },
        {
            "location": "/dataframe_functions/#shift_right_unsigned",
            "text": "",
            "title": "shift_right_unsigned"
        },
        {
            "location": "/dataframe_functions/#current_date",
            "text": "",
            "title": "current_date"
        },
        {
            "location": "/dataframe_functions/#collect_list",
            "text": "",
            "title": "collect_list"
        },
        {
            "location": "/dataframe_functions/#trim",
            "text": "",
            "title": "trim"
        },
        {
            "location": "/dataframe_functions/#log10",
            "text": "",
            "title": "log10"
        },
        {
            "location": "/dataframe_functions/#initcap",
            "text": "",
            "title": "initcap"
        },
        {
            "location": "/dataframe_functions/#to_utc_timestamp",
            "text": "",
            "title": "to_utc_timestamp"
        },
        {
            "location": "/dataframe_functions/#count_distinct",
            "text": "",
            "title": "count_distinct"
        },
        {
            "location": "/dataframe_functions/#create_map",
            "text": "",
            "title": "create_map"
        },
        {
            "location": "/dataframe_functions/#substring",
            "text": "",
            "title": "substring"
        },
        {
            "location": "/dataframe_functions/#min",
            "text": "",
            "title": "min"
        },
        {
            "location": "/dataframe_functions/#rpad",
            "text": "",
            "title": "rpad"
        },
        {
            "location": "/dataframe_functions/#from_unixtime",
            "text": "",
            "title": "from_unixtime"
        },
        {
            "location": "/dataframe_functions/#locate",
            "text": "",
            "title": "locate"
        },
        {
            "location": "/dataframe_functions/#when",
            "text": "",
            "title": "when"
        },
        {
            "location": "/dataframe_functions/#log1p",
            "text": "",
            "title": "log1p"
        },
        {
            "location": "/dataframe_functions/#shift_right",
            "text": "",
            "title": "shift_right"
        },
        {
            "location": "/dataframe_functions/#input_file_name",
            "text": "",
            "title": "input_file_name"
        },
        {
            "location": "/dataframe_functions/#expr",
            "text": "",
            "title": "expr"
        },
        {
            "location": "/dataframe_functions/#covar_samp",
            "text": "",
            "title": "covar_samp"
        },
        {
            "location": "/dataframe_functions/#dayofyear",
            "text": "",
            "title": "dayofyear"
        },
        {
            "location": "/dataframe_functions/#date_format",
            "text": "",
            "title": "date_format"
        },
        {
            "location": "/dataframe_functions/#shift_left",
            "text": "",
            "title": "shift_left"
        },
        {
            "location": "/dataframe_functions/#monotonically_increasing_id",
            "text": "",
            "title": "monotonically_increasing_id"
        },
        {
            "location": "/dataframe_functions/#unbase64",
            "text": "",
            "title": "unbase64"
        },
        {
            "location": "/dataframe_functions/#trunc",
            "text": "",
            "title": "trunc"
        },
        {
            "location": "/dataframe_functions/#spark_partition_id",
            "text": "",
            "title": "spark_partition_id"
        },
        {
            "location": "/dataframe_functions/#split",
            "text": "",
            "title": "split"
        },
        {
            "location": "/dataframe_functions/#grouping_id",
            "text": "",
            "title": "grouping_id"
        },
        {
            "location": "/dataframe_functions/#size",
            "text": "",
            "title": "size"
        },
        {
            "location": "/dataframe_functions/#rint",
            "text": "",
            "title": "rint"
        },
        {
            "location": "/dataframe_functions/#cos",
            "text": "",
            "title": "cos"
        },
        {
            "location": "/dataframe_functions/#current_timestamp",
            "text": "",
            "title": "current_timestamp"
        },
        {
            "location": "/dataframe_functions/#approx_count_distinct",
            "text": "",
            "title": "approx_count_distinct"
        },
        {
            "location": "/dataframe_functions/#stddev_samp",
            "text": "",
            "title": "stddev_samp"
        },
        {
            "location": "/dataframe_functions/#sha2",
            "text": "",
            "title": "sha2"
        },
        {
            "location": "/dataframe_functions/#bround",
            "text": "",
            "title": "bround"
        },
        {
            "location": "/dataframe_functions/#hypot",
            "text": "",
            "title": "hypot"
        },
        {
            "location": "/dataframe_functions/#greatest",
            "text": "",
            "title": "greatest"
        },
        {
            "location": "/dataframe_functions/#pmod",
            "text": "",
            "title": "pmod"
        },
        {
            "location": "/dataframe_functions/#conv",
            "text": "",
            "title": "conv"
        },
        {
            "location": "/dataframe_functions/#hash",
            "text": "",
            "title": "hash"
        },
        {
            "location": "/dataframe_functions/#length",
            "text": "",
            "title": "length"
        },
        {
            "location": "/dataframe_functions/#skewness",
            "text": "",
            "title": "skewness"
        },
        {
            "location": "/dataframe_functions/#decode",
            "text": "",
            "title": "decode"
        },
        {
            "location": "/dataframe_functions/#array",
            "text": "",
            "title": "array"
        },
        {
            "location": "/dataframe_functions/#floor",
            "text": "",
            "title": "floor"
        },
        {
            "location": "/dataframe_functions/#datediff",
            "text": "",
            "title": "datediff"
        },
        {
            "location": "/dataframe_functions/#acos",
            "text": "",
            "title": "acos"
        },
        {
            "location": "/dataframe_functions/#avg",
            "text": "",
            "title": "avg"
        },
        {
            "location": "/dataframe_functions/#from_utc_timestamp",
            "text": "",
            "title": "from_utc_timestamp"
        },
        {
            "location": "/dataframe_functions/#factorial",
            "text": "",
            "title": "factorial"
        },
        {
            "location": "/dataframe_functions/#dayofmonth",
            "text": "",
            "title": "dayofmonth"
        },
        {
            "location": "/dataframe_functions/#avg_1",
            "text": "",
            "title": "avg"
        },
        {
            "location": "/dataframe_functions/#randn",
            "text": "",
            "title": "randn"
        },
        {
            "location": "/dataframe_functions/#soundex",
            "text": "",
            "title": "soundex"
        },
        {
            "location": "/dataframe_functions/#to_date",
            "text": "",
            "title": "to_date"
        },
        {
            "location": "/dataframe_functions/#bitwise_not",
            "text": "",
            "title": "bitwise_not"
        },
        {
            "location": "/dataframe_functions/#stddev_pop",
            "text": "",
            "title": "stddev_pop"
        },
        {
            "location": "/dataframe_functions/#concat_ws",
            "text": "",
            "title": "concat_ws"
        },
        {
            "location": "/dataframe_functions/#abs",
            "text": "",
            "title": "abs"
        },
        {
            "location": "/dataframe_functions/#to_radians",
            "text": "",
            "title": "to_radians"
        },
        {
            "location": "/dataframe_functions/#pow",
            "text": "",
            "title": "pow"
        },
        {
            "location": "/dataframe_functions/#sqrt",
            "text": "",
            "title": "sqrt"
        },
        {
            "location": "/dataframe_functions/#regexp_replace",
            "text": "",
            "title": "regexp_replace"
        },
        {
            "location": "/dataframe_functions/#date_sub",
            "text": "",
            "title": "date_sub"
        },
        {
            "location": "/dataframe_functions/#lower",
            "text": "",
            "title": "lower"
        },
        {
            "location": "/dataframe_functions/#levenshtein",
            "text": "",
            "title": "levenshtein"
        },
        {
            "location": "/dataframe_functions/#var_samp_1",
            "text": "",
            "title": "var_samp"
        },
        {
            "location": "/dataframe_functions/#hex",
            "text": "",
            "title": "hex"
        },
        {
            "location": "/dataframe_functions/#sinh",
            "text": "",
            "title": "sinh"
        },
        {
            "location": "/dataframe_functions/#to_degrees",
            "text": "",
            "title": "to_degrees"
        },
        {
            "location": "/dataframe_functions/#grouping",
            "text": "",
            "title": "grouping"
        },
        {
            "location": "/dataframe_functions/#stddev_samp_1",
            "text": "",
            "title": "stddev_samp"
        },
        {
            "location": "/dataframe_functions/#translate",
            "text": "",
            "title": "translate"
        },
        {
            "location": "/dataframe_functions/#format_number",
            "text": "",
            "title": "format_number"
        },
        {
            "location": "/dataframe_functions/#lpad",
            "text": "",
            "title": "lpad"
        },
        {
            "location": "/dataframe_functions/#sin",
            "text": "",
            "title": "sin"
        },
        {
            "location": "/dataframe_functions/#coalesce",
            "text": "",
            "title": "coalesce"
        },
        {
            "location": "/dataframe_functions/#least",
            "text": "",
            "title": "least"
        },
        {
            "location": "/dataframe_functions/#asin",
            "text": "",
            "title": "asin"
        },
        {
            "location": "/dataframe_functions/#minute",
            "text": "",
            "title": "minute"
        },
        {
            "location": "/dataframe_functions/#quarter",
            "text": "",
            "title": "quarter"
        },
        {
            "location": "/dataframe_functions/#tan",
            "text": "",
            "title": "tan"
        },
        {
            "location": "/dataframe_functions/#last",
            "text": "",
            "title": "last"
        },
        {
            "location": "/dataframe_functions/#window",
            "text": "",
            "title": "window"
        },
        {
            "location": "/dataframe_functions/#is_nan",
            "text": "",
            "title": "is_nan"
        },
        {
            "location": "/dataframe_functions/#month",
            "text": "",
            "title": "month"
        },
        {
            "location": "/dataframe_functions/#encode",
            "text": "",
            "title": "encode"
        },
        {
            "location": "/dataframe_functions/#collect_set",
            "text": "",
            "title": "collect_set"
        },
        {
            "location": "/dataframe_functions/#log2",
            "text": "",
            "title": "log2"
        },
        {
            "location": "/dataframe_functions/#atan2",
            "text": "",
            "title": "atan2"
        },
        {
            "location": "/dataframe_functions/#next_day",
            "text": "",
            "title": "next_day"
        },
        {
            "location": "/dataframe_functions/#atan",
            "text": "",
            "title": "atan"
        },
        {
            "location": "/dataframe_functions/#col",
            "text": "",
            "title": "col"
        },
        {
            "location": "/dataframe_functions/#months_between",
            "text": "",
            "title": "months_between"
        },
        {
            "location": "/dataframe_functions/#cosh",
            "text": "",
            "title": "cosh"
        },
        {
            "location": "/dataframe_functions/#add_months",
            "text": "",
            "title": "add_months"
        },
        {
            "location": "/dataframe_functions/#ceil",
            "text": "",
            "title": "ceil"
        },
        {
            "location": "/dataframe_functions/#second",
            "text": "",
            "title": "second"
        },
        {
            "location": "/dataframe_functions/#var_pop",
            "text": "",
            "title": "var_pop"
        },
        {
            "location": "/dataframe_functions/#corr",
            "text": "",
            "title": "corr"
        },
        {
            "location": "/dataframe_functions/#exp",
            "text": "",
            "title": "exp"
        },
        {
            "location": "/dataframe_functions/#year",
            "text": "",
            "title": "year"
        },
        {
            "location": "/dataframe_functions/#rand",
            "text": "",
            "title": "rand"
        },
        {
            "location": "/dataframe_functions/#kurtosis",
            "text": "",
            "title": "kurtosis"
        },
        {
            "location": "/dataframe_functions/#column",
            "text": "",
            "title": "Column"
        },
        {
            "location": "/dataframe_functions/#otherwise",
            "text": "",
            "title": "otherwise"
        },
        {
            "location": "/dataframe_functions/#alias",
            "text": "",
            "title": "alias"
        },
        {
            "location": "/dataframe_functions/#equal_null_safe",
            "text": "",
            "title": "equal_null_safe"
        },
        {
            "location": "/dataframe_functions/#contains",
            "text": "",
            "title": "contains"
        },
        {
            "location": "/dataframe_functions/#cast",
            "text": "",
            "title": "cast"
        },
        {
            "location": "/dataframe_functions/#desc",
            "text": "",
            "title": "desc"
        },
        {
            "location": "/dataframe_functions/#to_json",
            "text": "",
            "title": "to_json"
        },
        {
            "location": "/dataframe_functions/#name",
            "text": "",
            "title": "name"
        },
        {
            "location": "/dataframe_functions/#asc",
            "text": "",
            "title": "asc"
        },
        {
            "location": "/dataframe_functions/#bitwise_not_1",
            "text": "",
            "title": "bitwise_not"
        },
        {
            "location": "/dataframe_functions/#substr",
            "text": "",
            "title": "substr"
        },
        {
            "location": "/dataframe_functions/#starts_with",
            "text": "",
            "title": "starts_with"
        },
        {
            "location": "/dataframe_functions/#isin",
            "text": "",
            "title": "isin"
        },
        {
            "location": "/dataframe_functions/#is_null_1",
            "text": "",
            "title": "is_null"
        },
        {
            "location": "/dataframe_functions/#between",
            "text": "",
            "title": "between"
        },
        {
            "location": "/dataframe_functions/#bitwise_or",
            "text": "",
            "title": "bitwise_or"
        },
        {
            "location": "/dataframe_functions/#get_field",
            "text": "",
            "title": "get_field"
        },
        {
            "location": "/dataframe_functions/#get_item",
            "text": "",
            "title": "get_item"
        },
        {
            "location": "/dataframe_functions/#is_nan_1",
            "text": "",
            "title": "is_nan"
        },
        {
            "location": "/dataframe_functions/#rlike",
            "text": "",
            "title": "rlike"
        },
        {
            "location": "/dataframe_functions/#like",
            "text": "",
            "title": "like"
        },
        {
            "location": "/dataframe_functions/#bitwise_xor",
            "text": "",
            "title": "bitwise_xor"
        },
        {
            "location": "/dataframe_functions/#when_1",
            "text": "",
            "title": "when"
        },
        {
            "location": "/dataframe_functions/#ends_with",
            "text": "",
            "title": "ends_with"
        },
        {
            "location": "/dataframe_functions/#bitwise_and",
            "text": "",
            "title": "bitwise_and"
        },
        {
            "location": "/dataframe_functions/#is_not_null",
            "text": "",
            "title": "is_not_null"
        },
        {
            "location": "/pipelines_concepts/",
            "text": "Cebes Pipelines is an opinionated way to construct, train and deploy Machine Learning models.\nCebes Pipelines generalizes and unifies the APIs in a coherent design, which can then be extended\nto support other compute engines than Spark.\n\n\nDefinitions\n\u00b6\n\n\n\n\nPipeline\n is a directed acyclic graph whose edges carry and transfer data, and whose vertices \nare \nstages\n that do specific, pre-defined tasks. \n\n\nStage\n is a single processing unit implemented in Cebes. There are different types of stages \ndoing different type of tasks. A stage has multiple input and output \nslots\n. Once executed, a stage\ncomputes the value of its output slots from the values received in its input slots.\n\n\nSlot\n is a mechanism for stages to receive inputs and communicate to each other. \nThere are input slots and output slots. All slots are strongly-typed and only \ntransfer data of the correct type.\n\n\nSlots can be \nstateful\n or \nstateless\n, which controls how their values are re-computed once \nthe input changes.\n\n\nValues of all output slots are cached and not recomputed as long as input slots do not change.\nWhen the input changes, they are recomputed depending on whether they are stateful or stateless:\n\n\nStateless output slots are recomputed anytime any input slot changes\n\n\nStateful output slots are only recomputed when stateful input slots change, or explicitly\nrequired by the user.\n\n\n\n\n\n\n\n\n\n\n\n\nEstimator\n is a special kind of stage for training Machine Learning models. Estimators often \nhas:\n\n\n\n\n1 \nstateless\n input slot for the dataset, named \ninputDf\n\n\n0 to many \nstateful\n input slots for the parameters, \ne.g.\n \nlearningRate\n, \nregularization\n, etc...\n\n\n1 \nstateful\n output slot for the trained \nModel\n\n\n1 \nstateless\n output slot for the transformed dataset, named \noutputDf\n\n\n0 to many \nstateful\n output slots for other auxiliary information produced during training\n\n\n\n\n\n\n\n\nPlaceholder\n is a special kind of stage that hold a piece of data that can be changed dynamically,\nespecially when pipeline is executed.\n\n\n\n\n\n\nModel\n is the trained Machine Learning model, and often stay in a Pipeline.\n\n\nTo use pipelines, users first construct it by specifying stages and how they connect to each other.\nThey then execute the pipeline with \nPipeline.run()\n. At runtime, they can choose which output slots\nwhose value will be retrieved, and overwrite the value of any input slot.\n\n\n\n\n\n\nPipeline tags\n\u00b6\n\n\nSimilar to \nDataframe tags\n, pipelines can be tagged too. \nIn general, tags in Cebes are more powerful than just mere string identifiers. In this section,\nwe will see how tags provide a convenient way to do version management and publishing for pipelines.\n\n\nIn Cebes, a pipeline tag is a string of the following syntax:\n\n\n[repo_host[:repo_port]/]path[:version]\n\n\n\n\n\n\n\nrepo_host\n and \nrepo_port\n is the address of the repository. These are optional. You only need\nthe repository when you want to push or pull pipelines. If repository is not specified in the tag,\nCebes server will use its default value, which is configurable in the \nCEBES_DEFAULT_REPOSITORY_*\n\nenviroment variables when \nCebes server is installed\n.\n\n\npath\n is a repository path that you give to the pipeline. This is required, must be a valid ASCII\nstring, and can have multiple segments separated by \n/\n.\n\n\nversion\n is an optional string that can be used to uniquely identify a pipeline in a given path.\nIf not specify, the default version is \ndefault\n.\n\n\n\n\nFor example, \nbob/anomaly-detection:default\n, \nbob/anomaly-detection:v2\n, \nrepo.company.net/bob/anomaly-detection\n,\n\nlocalhost:35000/bob/anomaly-detection:v5\n are all valid tags.\n\n\nSee \nthis section\n for using Session API to manage tags in Cebes server, and \n\nthis page\n for pushing and pulling pipelines to/from repositories via tags.\n\n\n\n\nWhere to from here?\n\u00b6\n\n\nCheck \nthis page\n for a sample Pipeline and learn how to use Pipeline APIs.\nCebes provides a rich set of stages, check programming guide for more information.\n\n\nOnce constructed, pipelines can be kept in Cebes server to run on your in-house datasets. They can also\nbe \npublished to a pipeline repository\n so that they are available for \n\nonline serving\n.",
            "title": "Key concepts"
        },
        {
            "location": "/pipelines_concepts/#definitions",
            "text": "Pipeline  is a directed acyclic graph whose edges carry and transfer data, and whose vertices \nare  stages  that do specific, pre-defined tasks.   Stage  is a single processing unit implemented in Cebes. There are different types of stages \ndoing different type of tasks. A stage has multiple input and output  slots . Once executed, a stage\ncomputes the value of its output slots from the values received in its input slots.  Slot  is a mechanism for stages to receive inputs and communicate to each other. \nThere are input slots and output slots. All slots are strongly-typed and only \ntransfer data of the correct type.  Slots can be  stateful  or  stateless , which controls how their values are re-computed once \nthe input changes.  Values of all output slots are cached and not recomputed as long as input slots do not change.\nWhen the input changes, they are recomputed depending on whether they are stateful or stateless:  Stateless output slots are recomputed anytime any input slot changes  Stateful output slots are only recomputed when stateful input slots change, or explicitly\nrequired by the user.       Estimator  is a special kind of stage for training Machine Learning models. Estimators often \nhas:   1  stateless  input slot for the dataset, named  inputDf  0 to many  stateful  input slots for the parameters,  e.g.   learningRate ,  regularization , etc...  1  stateful  output slot for the trained  Model  1  stateless  output slot for the transformed dataset, named  outputDf  0 to many  stateful  output slots for other auxiliary information produced during training     Placeholder  is a special kind of stage that hold a piece of data that can be changed dynamically,\nespecially when pipeline is executed.    Model  is the trained Machine Learning model, and often stay in a Pipeline.  To use pipelines, users first construct it by specifying stages and how they connect to each other.\nThey then execute the pipeline with  Pipeline.run() . At runtime, they can choose which output slots\nwhose value will be retrieved, and overwrite the value of any input slot.",
            "title": "Definitions"
        },
        {
            "location": "/pipelines_concepts/#pipeline-tags",
            "text": "Similar to  Dataframe tags , pipelines can be tagged too. \nIn general, tags in Cebes are more powerful than just mere string identifiers. In this section,\nwe will see how tags provide a convenient way to do version management and publishing for pipelines.  In Cebes, a pipeline tag is a string of the following syntax:  [repo_host[:repo_port]/]path[:version]   repo_host  and  repo_port  is the address of the repository. These are optional. You only need\nthe repository when you want to push or pull pipelines. If repository is not specified in the tag,\nCebes server will use its default value, which is configurable in the  CEBES_DEFAULT_REPOSITORY_* \nenviroment variables when  Cebes server is installed .  path  is a repository path that you give to the pipeline. This is required, must be a valid ASCII\nstring, and can have multiple segments separated by  / .  version  is an optional string that can be used to uniquely identify a pipeline in a given path.\nIf not specify, the default version is  default .   For example,  bob/anomaly-detection:default ,  bob/anomaly-detection:v2 ,  repo.company.net/bob/anomaly-detection , localhost:35000/bob/anomaly-detection:v5  are all valid tags.  See  this section  for using Session API to manage tags in Cebes server, and  this page  for pushing and pulling pipelines to/from repositories via tags.",
            "title": "Pipeline tags"
        },
        {
            "location": "/pipelines_concepts/#where-to-from-here",
            "text": "Check  this page  for a sample Pipeline and learn how to use Pipeline APIs.\nCebes provides a rich set of stages, check programming guide for more information.  Once constructed, pipelines can be kept in Cebes server to run on your in-house datasets. They can also\nbe  published to a pipeline repository  so that they are available for  online serving .",
            "title": "Where to from here?"
        },
        {
            "location": "/pipelines_api/",
            "text": "Construct a Pipeline\n\u00b6\n\n\nAs an example, here is a script to construct a Pipeline containing a Linear Regression model:\n\n\n>>>\n \nwith\n \ncb\n.\nPipeline\n()\n \nas\n \nppl\n:\n\n\n...\n:\n     \ninp\n \n=\n \ncb\n.\nplaceholder\n(\ncb\n.\nPlaceholderTypes\n.\nDATAFRAME\n)\n\n\n...\n:\n     \nassembler\n \n=\n \ncb\n.\nvector_assembler\n(\ninp\n,\n \ninput_cols\n=\n[\n'viscosity'\n,\n \n'proof_cut'\n],\n \n\n...\n:\n                                     \noutput_col\n=\n'features'\n)\n\n\n...\n:\n     \nlr\n \n=\n \ncb\n.\nlinear_regression\n(\nassembler\n.\noutput_df\n,\n \n\n...\n:\n                               \nfeatures_col\n=\n'features'\n,\n \nlabel_col\n=\n'caliper'\n,\n\n\n...\n:\n                               \nprediction_col\n=\n'caliper_predict'\n,\n \n\n...\n:\n                               \nreg_param\n=\n0.0\n)\n\n\n...\n:\n                                      \n\n\n\n\n\nThe pipeline is constructed in the \nwith cb.Pipeline()\n block, which includes a Dataframe placeholder, \na vector assembler and a linear regression stage. The pipeline above can be visualized as follows:\n\n\n\n\nCompared to the code, there are some good magics going on to reduce the verbosity:\n\n\n\n\nThe output slot of the placeholder is omitted, because it only has 1 output slot, which becomes\nits \ndefault\n output slot.\n\n\nThe stages are created using functions provided in the main namespace of \npycebes\n like \n\ncb.vector_assembler\n, \ncb.linear_regression\n, ...\n\n\n\n\nOnce constructed, you can check the list of stages in the pipeline:\n\n\n>>>\n \nppl\n\n    \nPipeline\n(\ndataframeplaceholder_0\n,\n \nvectorassembler_0\n,\n \nlinearregression_0\n)\n\n\n\n>>>\n \nppl\n.\nstages\n\n    \n{\n'dataframeplaceholder_0'\n:\n \nDataframePlaceholder\n(\ninput_val\n=\nNone\n,\nname\n=\n'dataframeplaceholder_0'\n),\n\n     \n'linearregression_0'\n:\n \nLinearRegression\n(\n...\n),\n\n     \n'vectorassembler_0'\n:\n \nVectorAssembler\n(\n...\n)}\n\n\n\n\n\n\nppl.stages\n is a dict mapping the stage name to the actual \nStage\n object. Stage names can be \nspecified in the \nname\n argument in all functions that create pipeline stages. When unspecified \nlike above, Cebes will automatically pick names for the stages, based on the type of the stage.\n\n\nNames are useful because you can use them to access the stage objects:\n\n\n>>>\n \nppl\n[\n'linearregression_0'\n]\n\n    \nLinearRegression\n(\ninput_df\n=\nSlotDescriptor\n(\nname\n=\n'output_df'\n,\nis_input\n=\nFalse\n),\n\n        \nsolver\n=\n'auto'\n,\nweight_col\n=\nNone\n,\ntolerance\n=\n1e-06\n,\nstandardization\n=\nTrue\n,\nreg_param\n=\n0.0\n,\n\n        \nmax_iter\n=\n10\n,\nfit_intercept\n=\nTrue\n,\nelastic_net_param\n=\n0.0\n,\naggregation_depth\n=\n2\n,\n\n        \nfeatures_col\n=\n'features'\n,\nlabel_col\n=\n'caliper'\n,\nprediction_col\n=\n'caliper_predict'\n,\n\n        \nname\n=\n'linearregression_0'\n)\n\n\n\n\n\n\nYou don't need to construct the whole pipeline in a single \nwith\n block. For example, \nlet's add an evaluator stage like so:\n\n\n>>>\n \nwith\n \nppl\n:\n\n\n...\n:\n     \nrmse\n \n=\n \ncb\n.\nrmse\n(\nlr\n.\noutput_df\n,\n \nlabel_col\n=\n'caliper'\n,\n \nprediction_col\n=\n'caliper_predict'\n,\n\n\n...\n:\n                    \nname\n=\n'evaluator'\n)\n\n\n\n>>>\n \nppl\n.\nstages\n\n    \n{\n'dataframeplaceholder_0'\n:\n \nDataframePlaceholder\n(\ninput_val\n=\nNone\n,\nname\n=\n'dataframeplaceholder_0'\n),\n\n     \n'evaluator'\n:\n \nRegressionEvaluator\n(\n...\n),\n\n     \n'linearregression_0'\n:\n \nLinearRegression\n(\n...\n),\n\n     \n'vectorassembler_0'\n:\n \nVectorAssembler\n(\n...\n)}\n\n\n\n\n\n\nYou always need to use a \nwith\n block to add new stages into a pipeline. Without the context manager,\nCebes doesn't know which pipeline is being referred to.\n\n\nRun a Pipeline for training\n\u00b6\n\n\nLet's first load the sample dataset and perform some preprocessing so that it can be used \nin the pipeline above.\n\n\nNote that those preprocessing step actually can also be done using Pipeline API. We use \nDataframe API here for brevity.\n\n\n>>>\n \ndf\n \n=\n \ncb\n.\nget_default_session\n()\n.\nload_test_datasets\n()[\n'cylinder_bands'\n]\n\n\n>>>\n \ndf2\n \n=\n \ndf\n.\ndrop\n(\n*\n(\nset\n(\ndf\n.\ncolumns\n)\n \n-\n \n{\n'viscosity'\n,\n \n'proof_cut'\n,\n \n'caliper'\n}))\n\n\n>>>\n \ndf2\n \n=\n \ndf2\n.\ndropna\n(\ncolumns\n=\n[\n'viscosity'\n,\n \n'proof_cut'\n,\n \n'caliper'\n])\n\n\n\n>>>\n \ndf2\n.\nshow\n()\n\n    \nID\n:\n \n25\na139f8\n-\n4\na27\n-\n442\nf\n-\n8\na95\n-\nd8b0cd528c75\n\n    \nShape\n:\n \n(\n466\n,\n \n3\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \nproof_cut\n  \nviscosity\n  \ncaliper\n\n    \n0\n       \n55.0\n         \n46\n    \n0.200\n\n    \n1\n       \n55.0\n         \n46\n    \n0.300\n\n    \n2\n       \n62.0\n         \n40\n    \n0.433\n\n    \n3\n       \n52.0\n         \n40\n    \n0.300\n\n    \n4\n       \n50.0\n         \n46\n    \n0.300\n\n\n\n\n\n\nA Pipeline is executed using its \nrun()\n method, in which we need to provide value to the \nplaceholder, and specify the list of output slots we want to receive the result.\n\n\nIn this case, we are feeding \ndf\n into the \ninp\n placeholder, and retrieved the final \nDataframe, the trained model, the Dataframe after featurization and the RMSE score:\n\n\n>>>\n \npredicted_df\n,\n \nlr_model\n,\n \nassembled_df\n,\n \nrmse_val\n \n=\n \nppl\n.\nrun\n(\n\n\n...\n:\n    \n[\nlr\n.\noutput_df\n,\n \nlr\n.\nmodel\n,\n \nassembler\n.\noutput_df\n,\n \nrmse\n.\nmetric_value\n],\n \nfeeds\n=\n{\ninp\n:\n \ndf2\n})\n\n    \nRequest\n \nID\n:\n \n08228\ndc8\n-\n2\nbce\n-\n45\na1\n-\na13e\n-\n353726e45646\n\n\n\n>>>\n \npredicted_df\n.\nshow\n()\n\n    \nID\n:\n \n294\nab9a2\n-\n0\nb84\n-\n4\ncf9\n-\n97\nb4\n-\nde4cd4be9901\n\n    \nShape\n:\n \n(\n466\n,\n \n5\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \nproof_cut\n  \nviscosity\n  \ncaliper\n      \nfeatures\n  \ncaliper_predict\n\n    \n0\n       \n55.0\n         \n46\n    \n0.200\n  \n[\n46.0\n,\n \n55.0\n]\n         \n0.273678\n\n    \n1\n       \n55.0\n         \n46\n    \n0.300\n  \n[\n46.0\n,\n \n55.0\n]\n         \n0.273678\n\n    \n2\n       \n62.0\n         \n40\n    \n0.433\n  \n[\n40.0\n,\n \n62.0\n]\n         \n0.266757\n\n    \n3\n       \n52.0\n         \n40\n    \n0.300\n  \n[\n40.0\n,\n \n52.0\n]\n         \n0.261072\n\n    \n4\n       \n50.0\n         \n46\n    \n0.300\n  \n[\n46.0\n,\n \n50.0\n]\n         \n0.270836\n\n\n\n>>>\n \nlr_model\n\n    \nModel\n(\nid\n=\n'5ed5e8bd-579d-4ae0-a932-718da9064c36'\n,\n\n          \nmodel_class\n=\n'io.cebes.spark.pipeline.ml.regression.LinearRegressionModel'\n)\n\n\n\n>>>\n \nassembled_df\n.\nshow\n()\n\n    \nID\n:\n \n541814\nc7\n-\n738\nb\n-\n436\nb\n-\n8479\n-\nf385ad4edc49\n\n    \nShape\n:\n \n(\n466\n,\n \n4\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \nproof_cut\n  \nviscosity\n  \ncaliper\n      \nfeatures\n\n    \n0\n       \n55.0\n         \n46\n    \n0.200\n  \n[\n46.0\n,\n \n55.0\n]\n\n    \n1\n       \n55.0\n         \n46\n    \n0.300\n  \n[\n46.0\n,\n \n55.0\n]\n\n    \n2\n       \n62.0\n         \n40\n    \n0.433\n  \n[\n40.0\n,\n \n62.0\n]\n\n    \n3\n       \n52.0\n         \n40\n    \n0.300\n  \n[\n40.0\n,\n \n52.0\n]\n\n    \n4\n       \n50.0\n         \n46\n    \n0.300\n  \n[\n46.0\n,\n \n50.0\n]\n\n\n\n>>>\n \nrmse_val\n\n    \n0.06898695718445916\n\n\n\n\n\n\nIf the placeholder is not fed, an exception will be thrown:\n\n\n>>>\n \npredicted_df\n,\n \nlr_model\n,\n \nassembled_df\n,\n \nrmse_val\n \n=\n \nppl\n.\nrun\n(\n\n\n...\n:\n    \n[\ns\n.\noutput_df\n,\n \ns\n.\nmodel\n,\n \nassembler\n.\noutput_df\n,\n \nrmse\n.\nmetric_value\n])\n\n\n    \nServerException\n:\n \n(\n'DataframePlaceholder(name=dataframeplaceholder_0): Input slot inputVal is undefined'\n,\n \n...\n)\n\n\n\n\n\n\nBy callling \nrun()\n, the model will be estimated, then used to compute the final \nDataframe, the RMSE score is also computed.\n\n\nInference\n\u00b6\n\n\nOnce trained, the pipeline can now be used for inference, using exactly the same API:\n\n\n>>>\n \ndf3\n \n=\n \ndf2\n.\nwhere\n(\ndf2\n.\nproof_cut\n \n>\n \n50\n)\n\n\n\n>>>\n \ndf3_predicted\n,\n \ndf3_rmse\n \n=\n \nppl\n.\nrun\n([\ns\n.\noutput_df\n,\n \nppl\n[\n'evaluator'\n]],\n \nfeeds\n=\n{\ninp\n:\n \ndf3\n})\n\n    \nRequest\n \nID\n:\n \n4\nc8524a3\n-\n0112\n-\n479\nc\n-\n8\nad0\n-\ne021dbac2566\n\n\n\n>>>\n \ndf3_predicted\n.\nshow\n()\n\n    \nID\n:\n \nf03d6c2a\n-\n7518\n-\n453e-81\nbf\n-\nc56a59e94021\n\n    \nShape\n:\n \n(\n100\n,\n \n5\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \nproof_cut\n  \nviscosity\n  \ncaliper\n      \nfeatures\n  \ncaliper_predict\n\n    \n0\n       \n55.0\n         \n46\n    \n0.200\n  \n[\n46.0\n,\n \n55.0\n]\n         \n0.280963\n\n    \n1\n       \n55.0\n         \n46\n    \n0.300\n  \n[\n46.0\n,\n \n55.0\n]\n         \n0.280963\n\n    \n2\n       \n62.0\n         \n40\n    \n0.433\n  \n[\n40.0\n,\n \n62.0\n]\n         \n0.252337\n\n    \n3\n       \n52.0\n         \n40\n    \n0.300\n  \n[\n40.0\n,\n \n52.0\n]\n         \n0.265102\n\n    \n4\n       \n65.0\n         \n43\n    \n0.333\n  \n[\n43.0\n,\n \n65.0\n]\n         \n0.258353\n\n\n\n>>>\n \ndf3_rmse\n\n    \n0.07246338467430166\n\n\n\n\n\n\nUnder the hood, Cebes does this in a non-trivial way. The trained model is actually stored\nin a \nstateful\n output slot. In the second \nrun()\n call, only the input Dataframe changes,\nwhich is a \nstateless\n input slot, hence the model will not be re-trained.\nThe model is only re-trained if you change the parameters in the \nLinearRegression\n stage,\nwhich can be done using the \nfeeds\n argument in the \nrun()\n call. Although the parameters\nwere specified at construction time, you can always overwrite them using the \nfeeds\n mechanism.\n\n\nRun a pipeline asynchronously\n\u00b6\n\n\nIn the above, you execute \nppl.run()\n and waits for the result to come back. Since the \ndataset is small, this can be done in a few seconds. For bigger datasets, training might \ntake days or hours. In this case, waiting for the result like above is not practical.\nCebes provides functions to get the result of pipeline execution asynchronously.\n\n\nT.B.A\n\n\nTag a Pipeline\n\u00b6\n\n\nLet's tag the pipeline so we can reuse it later:\n\n\n>>>\n \nsess\n \n=\n \ncb\n.\nget_default_session\n()\n\n\n>>>\n \nsess\n.\npipeline\n.\ntag\n(\nppl\n,\n \n'linear_regression'\n)\n\n\n>>>\n \nsess\n.\npipeline\n.\nlist\n()\n\n    \nUUID\n                                  \nTag\n                        \nCreated\n                       \n# of stages\n\n    \n------------------------------------\n  \n-------------------------\n  \n--------------------------\n  \n-------------\n\n    \n7\nd364b33\n-\ncc0b\n-\n4\nc96\n-\n9\necb\n-\ne2e48fa8d94f\n  \nlinear_regression\n:\ndefault\n  \n2017\n-\n12\n-\n31\n \n21\n:\n13\n:\n49.207000\n              \n4\n\n\n\n\n\n\nSee \nthis section\n for more on managing pipelines.\n\n\nUsing Models\n\u00b6\n\n\nThe resulting model can be use to transform \narbitrary\n input Dataframe, as long as it has the correct schema:\n\n\n>>>\n \nlr_model\n.\ntransform\n(\nassembled_df\n)\n.\nshow\n()\n\n    \nID\n:\n \nc1aaddeb\n-\n3646\n-\n4934\n-\n8\nf75\n-\n02\n9\na20fe542c\n\n    \nShape\n:\n \n(\n466\n,\n \n5\n)\n\n    \nSample\n \n5\n \nrows\n:\n\n       \nproof_cut\n  \nviscosity\n  \ncaliper\n      \nfeatures\n  \ncaliper_predict\n\n    \n0\n       \n55.0\n         \n46\n    \n0.200\n  \n[\n46.0\n,\n \n55.0\n]\n         \n0.273678\n\n    \n1\n       \n55.0\n         \n46\n    \n0.300\n  \n[\n46.0\n,\n \n55.0\n]\n         \n0.273678\n\n    \n2\n       \n62.0\n         \n40\n    \n0.433\n  \n[\n40.0\n,\n \n62.0\n]\n         \n0.266757\n\n    \n3\n       \n52.0\n         \n40\n    \n0.300\n  \n[\n40.0\n,\n \n52.0\n]\n         \n0.261072\n\n    \n4\n       \n50.0\n         \n46\n    \n0.300\n  \n[\n46.0\n,\n \n50.0\n]\n         \n0.270836\n\n\n\n>>>\n \nlr_model\n.\ntransform\n(\ndf2\n)\n.\nshow\n()\n\n    \nServerException\n:\n \n(\n'Field \"features\" does not exist.'\n,\n \n...\n)\n\n\n\n\n\n\ndf2\n is not valid because it has not been vectorized by VectorAssembler, and therefore does not have the \n\nfeatures\n column. This example shows why it is often more useful to use a whole pipeline, instead of \nthe individual models.\n\n\nSimilar to Pipelines, you can also tag a Model:\n\n\n>>>\n \nsess\n.\nmodel\n.\ntag\n(\nlr_model\n,\n \n'linear_regression'\n)\n\n    \nModel\n(\nid\n=\n'5ed5e8bd-579d-4ae0-a932-718da9064c36'\n,\nmodel_class\n=\n'io.cebes.spark.pipeline.ml.regression.LinearRegressionModel'\n)\n\n\n\n>>>\n \nsess\n.\nmodel\n.\nlist\n()\n\n    \nUUID\n                                  \nTag\n                        \nCreated\n                     \nModel\n \nclass\n\n    \n------------------------------------  -------------------------  --------------------------  -----------------------------------------------------------\n\n    \n5\ned5e8bd\n-\n579\nd\n-\n4\nae0\n-\na932\n-\n718\nda9064c36\n  \nlinear_regression\n:\ndefault\n  \n2018\n-\n01\n-\n06\n \n01\n:\n14\n:\n59.294000\n  \nio\n.\ncebes\n.\nspark\n.\npipeline\n.\nml\n.\nregression\n.\nLinearRegressionModel",
            "title": "Working with Cebes Pipelines"
        },
        {
            "location": "/pipelines_api/#construct-a-pipeline",
            "text": "As an example, here is a script to construct a Pipeline containing a Linear Regression model:  >>>   with   cb . Pipeline ()   as   ppl :  ... :       inp   =   cb . placeholder ( cb . PlaceholderTypes . DATAFRAME )  ... :       assembler   =   cb . vector_assembler ( inp ,   input_cols = [ 'viscosity' ,   'proof_cut' ],   ... :                                       output_col = 'features' )  ... :       lr   =   cb . linear_regression ( assembler . output_df ,   ... :                                 features_col = 'features' ,   label_col = 'caliper' ,  ... :                                 prediction_col = 'caliper_predict' ,   ... :                                 reg_param = 0.0 )  ... :                                         The pipeline is constructed in the  with cb.Pipeline()  block, which includes a Dataframe placeholder, \na vector assembler and a linear regression stage. The pipeline above can be visualized as follows:   Compared to the code, there are some good magics going on to reduce the verbosity:   The output slot of the placeholder is omitted, because it only has 1 output slot, which becomes\nits  default  output slot.  The stages are created using functions provided in the main namespace of  pycebes  like  cb.vector_assembler ,  cb.linear_regression , ...   Once constructed, you can check the list of stages in the pipeline:  >>>   ppl \n     Pipeline ( dataframeplaceholder_0 ,   vectorassembler_0 ,   linearregression_0 )  >>>   ppl . stages \n     { 'dataframeplaceholder_0' :   DataframePlaceholder ( input_val = None , name = 'dataframeplaceholder_0' ), \n      'linearregression_0' :   LinearRegression ( ... ), \n      'vectorassembler_0' :   VectorAssembler ( ... )}   ppl.stages  is a dict mapping the stage name to the actual  Stage  object. Stage names can be \nspecified in the  name  argument in all functions that create pipeline stages. When unspecified \nlike above, Cebes will automatically pick names for the stages, based on the type of the stage.  Names are useful because you can use them to access the stage objects:  >>>   ppl [ 'linearregression_0' ] \n     LinearRegression ( input_df = SlotDescriptor ( name = 'output_df' , is_input = False ), \n         solver = 'auto' , weight_col = None , tolerance = 1e-06 , standardization = True , reg_param = 0.0 , \n         max_iter = 10 , fit_intercept = True , elastic_net_param = 0.0 , aggregation_depth = 2 , \n         features_col = 'features' , label_col = 'caliper' , prediction_col = 'caliper_predict' , \n         name = 'linearregression_0' )   You don't need to construct the whole pipeline in a single  with  block. For example, \nlet's add an evaluator stage like so:  >>>   with   ppl :  ... :       rmse   =   cb . rmse ( lr . output_df ,   label_col = 'caliper' ,   prediction_col = 'caliper_predict' ,  ... :                      name = 'evaluator' )  >>>   ppl . stages \n     { 'dataframeplaceholder_0' :   DataframePlaceholder ( input_val = None , name = 'dataframeplaceholder_0' ), \n      'evaluator' :   RegressionEvaluator ( ... ), \n      'linearregression_0' :   LinearRegression ( ... ), \n      'vectorassembler_0' :   VectorAssembler ( ... )}   You always need to use a  with  block to add new stages into a pipeline. Without the context manager,\nCebes doesn't know which pipeline is being referred to.",
            "title": "Construct a Pipeline"
        },
        {
            "location": "/pipelines_api/#run-a-pipeline-for-training",
            "text": "Let's first load the sample dataset and perform some preprocessing so that it can be used \nin the pipeline above.  Note that those preprocessing step actually can also be done using Pipeline API. We use \nDataframe API here for brevity.  >>>   df   =   cb . get_default_session () . load_test_datasets ()[ 'cylinder_bands' ]  >>>   df2   =   df . drop ( * ( set ( df . columns )   -   { 'viscosity' ,   'proof_cut' ,   'caliper' }))  >>>   df2   =   df2 . dropna ( columns = [ 'viscosity' ,   'proof_cut' ,   'caliper' ])  >>>   df2 . show () \n     ID :   25 a139f8 - 4 a27 - 442 f - 8 a95 - d8b0cd528c75 \n     Shape :   ( 466 ,   3 ) \n     Sample   5   rows : \n        proof_cut    viscosity    caliper \n     0         55.0           46      0.200 \n     1         55.0           46      0.300 \n     2         62.0           40      0.433 \n     3         52.0           40      0.300 \n     4         50.0           46      0.300   A Pipeline is executed using its  run()  method, in which we need to provide value to the \nplaceholder, and specify the list of output slots we want to receive the result.  In this case, we are feeding  df  into the  inp  placeholder, and retrieved the final \nDataframe, the trained model, the Dataframe after featurization and the RMSE score:  >>>   predicted_df ,   lr_model ,   assembled_df ,   rmse_val   =   ppl . run (  ... :      [ lr . output_df ,   lr . model ,   assembler . output_df ,   rmse . metric_value ],   feeds = { inp :   df2 }) \n     Request   ID :   08228 dc8 - 2 bce - 45 a1 - a13e - 353726e45646  >>>   predicted_df . show () \n     ID :   294 ab9a2 - 0 b84 - 4 cf9 - 97 b4 - de4cd4be9901 \n     Shape :   ( 466 ,   5 ) \n     Sample   5   rows : \n        proof_cut    viscosity    caliper        features    caliper_predict \n     0         55.0           46      0.200    [ 46.0 ,   55.0 ]           0.273678 \n     1         55.0           46      0.300    [ 46.0 ,   55.0 ]           0.273678 \n     2         62.0           40      0.433    [ 40.0 ,   62.0 ]           0.266757 \n     3         52.0           40      0.300    [ 40.0 ,   52.0 ]           0.261072 \n     4         50.0           46      0.300    [ 46.0 ,   50.0 ]           0.270836  >>>   lr_model \n     Model ( id = '5ed5e8bd-579d-4ae0-a932-718da9064c36' , \n           model_class = 'io.cebes.spark.pipeline.ml.regression.LinearRegressionModel' )  >>>   assembled_df . show () \n     ID :   541814 c7 - 738 b - 436 b - 8479 - f385ad4edc49 \n     Shape :   ( 466 ,   4 ) \n     Sample   5   rows : \n        proof_cut    viscosity    caliper        features \n     0         55.0           46      0.200    [ 46.0 ,   55.0 ] \n     1         55.0           46      0.300    [ 46.0 ,   55.0 ] \n     2         62.0           40      0.433    [ 40.0 ,   62.0 ] \n     3         52.0           40      0.300    [ 40.0 ,   52.0 ] \n     4         50.0           46      0.300    [ 46.0 ,   50.0 ]  >>>   rmse_val \n     0.06898695718445916   If the placeholder is not fed, an exception will be thrown:  >>>   predicted_df ,   lr_model ,   assembled_df ,   rmse_val   =   ppl . run (  ... :      [ s . output_df ,   s . model ,   assembler . output_df ,   rmse . metric_value ]) \n\n     ServerException :   ( 'DataframePlaceholder(name=dataframeplaceholder_0): Input slot inputVal is undefined' ,   ... )   By callling  run() , the model will be estimated, then used to compute the final \nDataframe, the RMSE score is also computed.",
            "title": "Run a Pipeline for training"
        },
        {
            "location": "/pipelines_api/#inference",
            "text": "Once trained, the pipeline can now be used for inference, using exactly the same API:  >>>   df3   =   df2 . where ( df2 . proof_cut   >   50 )  >>>   df3_predicted ,   df3_rmse   =   ppl . run ([ s . output_df ,   ppl [ 'evaluator' ]],   feeds = { inp :   df3 }) \n     Request   ID :   4 c8524a3 - 0112 - 479 c - 8 ad0 - e021dbac2566  >>>   df3_predicted . show () \n     ID :   f03d6c2a - 7518 - 453e-81 bf - c56a59e94021 \n     Shape :   ( 100 ,   5 ) \n     Sample   5   rows : \n        proof_cut    viscosity    caliper        features    caliper_predict \n     0         55.0           46      0.200    [ 46.0 ,   55.0 ]           0.280963 \n     1         55.0           46      0.300    [ 46.0 ,   55.0 ]           0.280963 \n     2         62.0           40      0.433    [ 40.0 ,   62.0 ]           0.252337 \n     3         52.0           40      0.300    [ 40.0 ,   52.0 ]           0.265102 \n     4         65.0           43      0.333    [ 43.0 ,   65.0 ]           0.258353  >>>   df3_rmse \n     0.07246338467430166   Under the hood, Cebes does this in a non-trivial way. The trained model is actually stored\nin a  stateful  output slot. In the second  run()  call, only the input Dataframe changes,\nwhich is a  stateless  input slot, hence the model will not be re-trained.\nThe model is only re-trained if you change the parameters in the  LinearRegression  stage,\nwhich can be done using the  feeds  argument in the  run()  call. Although the parameters\nwere specified at construction time, you can always overwrite them using the  feeds  mechanism.",
            "title": "Inference"
        },
        {
            "location": "/pipelines_api/#run-a-pipeline-asynchronously",
            "text": "In the above, you execute  ppl.run()  and waits for the result to come back. Since the \ndataset is small, this can be done in a few seconds. For bigger datasets, training might \ntake days or hours. In this case, waiting for the result like above is not practical.\nCebes provides functions to get the result of pipeline execution asynchronously.  T.B.A",
            "title": "Run a pipeline asynchronously"
        },
        {
            "location": "/pipelines_api/#tag-a-pipeline",
            "text": "Let's tag the pipeline so we can reuse it later:  >>>   sess   =   cb . get_default_session ()  >>>   sess . pipeline . tag ( ppl ,   'linear_regression' )  >>>   sess . pipeline . list () \n     UUID                                    Tag                          Created                         # of stages \n     ------------------------------------    -------------------------    --------------------------    ------------- \n     7 d364b33 - cc0b - 4 c96 - 9 ecb - e2e48fa8d94f    linear_regression : default    2017 - 12 - 31   21 : 13 : 49.207000                4   See  this section  for more on managing pipelines.",
            "title": "Tag a Pipeline"
        },
        {
            "location": "/pipelines_api/#using-models",
            "text": "The resulting model can be use to transform  arbitrary  input Dataframe, as long as it has the correct schema:  >>>   lr_model . transform ( assembled_df ) . show () \n     ID :   c1aaddeb - 3646 - 4934 - 8 f75 - 02 9 a20fe542c \n     Shape :   ( 466 ,   5 ) \n     Sample   5   rows : \n        proof_cut    viscosity    caliper        features    caliper_predict \n     0         55.0           46      0.200    [ 46.0 ,   55.0 ]           0.273678 \n     1         55.0           46      0.300    [ 46.0 ,   55.0 ]           0.273678 \n     2         62.0           40      0.433    [ 40.0 ,   62.0 ]           0.266757 \n     3         52.0           40      0.300    [ 40.0 ,   52.0 ]           0.261072 \n     4         50.0           46      0.300    [ 46.0 ,   50.0 ]           0.270836  >>>   lr_model . transform ( df2 ) . show () \n     ServerException :   ( 'Field \"features\" does not exist.' ,   ... )   df2  is not valid because it has not been vectorized by VectorAssembler, and therefore does not have the  features  column. This example shows why it is often more useful to use a whole pipeline, instead of \nthe individual models.  Similar to Pipelines, you can also tag a Model:  >>>   sess . model . tag ( lr_model ,   'linear_regression' ) \n     Model ( id = '5ed5e8bd-579d-4ae0-a932-718da9064c36' , model_class = 'io.cebes.spark.pipeline.ml.regression.LinearRegressionModel' )  >>>   sess . model . list () \n     UUID                                    Tag                          Created                       Model   class \n     ------------------------------------  -------------------------  --------------------------  ----------------------------------------------------------- \n     5 ed5e8bd - 579 d - 4 ae0 - a932 - 718 da9064c36    linear_regression : default    2018 - 01 - 06   01 : 14 : 59.294000    io . cebes . spark . pipeline . ml . regression . LinearRegressionModel",
            "title": "Using Models"
        },
        {
            "location": "/pipelines_repo/",
            "text": "Pipeline repository\n is a web service that stores \nCebes pipelines\n.\nIt is a separated component in the Cebes suite and is deployed independently from Cebes server.\nUsing \npycebes\n, users can instruct Cebes server to push a pipeline to repository, or pull a pipeline \nfrom a repository. Pipeline tags provide user a principled way to manage different versions of the same\npipeline.\n\n\nIf you are familiar with Docker, Cebes pipeline repository is analogous to docker repository.\n\n\nThe recommended setup is to have a repository deployed on a host, and your team share the same \nrepository. This repository can also be used to provide pipelines for \nserving in production\n.\n\n\nFor testing, you can \nstart a local repository\n, which runs on your local \nmachine in a Docker container. We are also working on a public repository, stay tuned!\n\n\n\n\nLogin to a Repository\n\u00b6\n\n\nSince repository is separated from Cebes server, you need to login to the repository by a username and password:\n\n\n>>>\n \nsession\n \n=\n \ncb\n.\nget_default_pipeline\n()\n\n\n\n>>>\n \nsession\n.\npipeline\n.\nlogin\n()\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nLogged\n \ninto\n \nrepository\n \n172.17\n.\n0.3\n:\n22000\n\n\n\n\n\n\nUsers can specify the repository hostname and port, along with username and password, as arguments to \nthe \nlogin\n function. If username is not None and password is None, Cebes will ask for the password\ninteractively. This is recommended, so that passwords do not get stored in the interpreter history.\n\n\n\n\nPush a pipeline to repository\n\u00b6\n\n\nFrom \npycebes\n, you can push a tagged pipeline by using the pipeline object or its tag:\n\n\n>>>\n \nsession\n.\npipeline\n.\nlist\n()\n     \n    \nUUID\n                                  \nTag\n                     \nCreated\n                       \n# of stages\n\n    \n------------------------------------\n  \n----------------------\n  \n--------------------------\n  \n-------------\n\n    \ncac82369\n-\na6cc\n-\n40\ndb\n-\na4ca\n-\n78\nd18e7ec30e\n  \ntest\n-\nlocal\n-\nppl\n:\ndefault\n  \n2018\n-\n01\n-\n13\n \n16\n:\n37\n:\n57.479000\n              \n3\n\n\n\n>>>\n \nppl\n \n=\n \nsession\n.\npipeline\n.\nget\n(\n'test-local-ppl'\n)\n\n\n\n>>>\n \nsession\n.\npipeline\n.\npush\n(\nppl\n)\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nUsing\n \nrepository\n \n172.17\n.\n0.3\n:\n22000\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nPushed\n \nsuccessfully\n:\n \n{\n\"name\"\n:\n\"default\"\n,\n\"lastUpdate\"\n:\n1515877964875\n,\n\"repositoryName\"\n:\n\"test-local-ppl\"\n}\n\n\n\n>>>\n \nsession\n.\npipeline\n.\npush\n(\n'test-local-ppl'\n)\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nUsing\n \nrepository\n \n172.17\n.\n0.3\n:\n22000\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nPushed\n \nsuccessfully\n:\n \n{\n\"name\"\n:\n\"default\"\n,\n\"lastUpdate\"\n:\n1515877972651\n,\n\"repositoryName\"\n:\n\"test-local-ppl\"\n}\n\n\n\n\n\n\nSee \nthis section\n for information on how Cebes determine which repository\nto push to. In this case, since the tag doesn't carry repository address, Cebes pushes it to a local repository.\n\n\n\n\nPull a pipeline from repository\n\u00b6\n\n\nSimilarly, a pipeline can be pulled from a repository into Cebes server using its tag:\n\n\n>>>\n  \nsession\n.\npipeline\n.\npull\n(\n'test-local-ppl'\n)\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nUsing\n \nrepository\n \n172.17\n.\n0.3\n:\n22000\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nPulled\n \nsuccessfully\n:\n \ntest\n-\nlocal\n-\nppl\n:\ndefault\n\n\n    \nPipeline\n(\ndataframeplaceholder_0\n,\n \nlinearregression_0\n,\n \nvectorassembler_0\n)\n\n\n\n\n\n\nThe \npull()\n function returns the pipeline object itself.\n\n\n\n\n\n\nStart a local repository\n\u00b6\n\n\nA local repository can be started using \nSession.start_repository_container\n and providing a port:\n\n\n>>>\n \nsession\n \n=\n \ncb\n.\nget_default_session\n()\n\n\n\n>>>\n \nsession\n.\nstart_repository_container\n(\nhost_port\n=\n35000\n)\n\n    \n[\nINFO\n]\n \nStarting\n \ncontainer\n \ncebes\n-\npipeline\n-\nrepository\n-\nrepo\n-\nx\n.\nx\n.\nx\n-\n0\n[\ncebes\n:\nx\n.\nx\n.\nx\n]\n \nwith\n \ndata\n \npath\n \nat\n \n$\nHOME\n/.\ncebes\n/\nrepo\n-\nx\n.\nx\n.\nx\n/\nrepository\n\n    \n[\nINFO\n]\n \nCebes\n \ncontainer\n \nstarted\n,\n \nlistening\n \nat\n \nlocalhost\n:\n35000\n\n    \n[\nINFO\n]\n \npycebes\n.\ncore\n.\nsession\n:\n \nPipeline\n \nrepository\n \nstarted\n \non\n \nport\n \n35000\n\n\n\n\n\n\nhost_port\n is the port on your machine that you will use to talk to the repository on \nlocalhost\n.\nWe recommend a high port between 30000 and 40000. If there is already a repository running, \n\nstart_repository_container()\n will simply re-use it.\n\n\nIf you started a local repository in a session, all pipeline pushes and pulls in that session \nwill try to use the local repository (which is at \nlocalhost:35000\n). Of course if the pipeline \ntag explicitly contains a different repository, it will be used instead.\n\n\nThe local repository can be stopped with:\n\n\n>>>\n \nsession\n.\nstop_repository_container\n()\n\n    \n[\nINFO\n]\n \nCebes\n \ncontainer\n \nstopped\n:\n \ncebes\n-\npipeline\n-\nrepository\n-\nrepo\n-\nx\n.\nx\n.\nx\n-\n0\n[\ncebes\n:\nx\n.\nx\n.\nx\n]\n \nat\n \nport\n \n35000\n\n\n\n\n\n\n\n\n\n\nHow Cebes server determines which repository to use\n\u00b6\n\n\nThe repository to which pipelines are pushed or pulled can be confusing. In general, Cebes decides \nthe repository to use in the following order:\n\n\n\n\nThe repository specified in the tag,\n\n\nThe local repository, if it is started in the session,\n\n\nThe server-wide default repository configured in Cebes server.",
            "title": "Working with repositories in pycebes"
        },
        {
            "location": "/pipelines_repo/#login-to-a-repository",
            "text": "Since repository is separated from Cebes server, you need to login to the repository by a username and password:  >>>   session   =   cb . get_default_pipeline ()  >>>   session . pipeline . login () \n     [ INFO ]   pycebes . core . session :   Logged   into   repository   172.17 . 0.3 : 22000   Users can specify the repository hostname and port, along with username and password, as arguments to \nthe  login  function. If username is not None and password is None, Cebes will ask for the password\ninteractively. This is recommended, so that passwords do not get stored in the interpreter history.",
            "title": "Login to a Repository"
        },
        {
            "location": "/pipelines_repo/#push-a-pipeline-to-repository",
            "text": "From  pycebes , you can push a tagged pipeline by using the pipeline object or its tag:  >>>   session . pipeline . list ()      \n     UUID                                    Tag                       Created                         # of stages \n     ------------------------------------    ----------------------    --------------------------    ------------- \n     cac82369 - a6cc - 40 db - a4ca - 78 d18e7ec30e    test - local - ppl : default    2018 - 01 - 13   16 : 37 : 57.479000                3  >>>   ppl   =   session . pipeline . get ( 'test-local-ppl' )  >>>   session . pipeline . push ( ppl ) \n     [ INFO ]   pycebes . core . session :   Using   repository   172.17 . 0.3 : 22000 \n     [ INFO ]   pycebes . core . session :   Pushed   successfully :   { \"name\" : \"default\" , \"lastUpdate\" : 1515877964875 , \"repositoryName\" : \"test-local-ppl\" }  >>>   session . pipeline . push ( 'test-local-ppl' ) \n     [ INFO ]   pycebes . core . session :   Using   repository   172.17 . 0.3 : 22000 \n     [ INFO ]   pycebes . core . session :   Pushed   successfully :   { \"name\" : \"default\" , \"lastUpdate\" : 1515877972651 , \"repositoryName\" : \"test-local-ppl\" }   See  this section  for information on how Cebes determine which repository\nto push to. In this case, since the tag doesn't carry repository address, Cebes pushes it to a local repository.",
            "title": "Push a pipeline to repository"
        },
        {
            "location": "/pipelines_repo/#pull-a-pipeline-from-repository",
            "text": "Similarly, a pipeline can be pulled from a repository into Cebes server using its tag:  >>>    session . pipeline . pull ( 'test-local-ppl' ) \n     [ INFO ]   pycebes . core . session :   Using   repository   172.17 . 0.3 : 22000 \n     [ INFO ]   pycebes . core . session :   Pulled   successfully :   test - local - ppl : default \n\n     Pipeline ( dataframeplaceholder_0 ,   linearregression_0 ,   vectorassembler_0 )   The  pull()  function returns the pipeline object itself.",
            "title": "Pull a pipeline from repository"
        },
        {
            "location": "/pipelines_repo/#start-a-local-repository",
            "text": "A local repository can be started using  Session.start_repository_container  and providing a port:  >>>   session   =   cb . get_default_session ()  >>>   session . start_repository_container ( host_port = 35000 ) \n     [ INFO ]   Starting   container   cebes - pipeline - repository - repo - x . x . x - 0 [ cebes : x . x . x ]   with   data   path   at   $ HOME /. cebes / repo - x . x . x / repository \n     [ INFO ]   Cebes   container   started ,   listening   at   localhost : 35000 \n     [ INFO ]   pycebes . core . session :   Pipeline   repository   started   on   port   35000   host_port  is the port on your machine that you will use to talk to the repository on  localhost .\nWe recommend a high port between 30000 and 40000. If there is already a repository running,  start_repository_container()  will simply re-use it.  If you started a local repository in a session, all pipeline pushes and pulls in that session \nwill try to use the local repository (which is at  localhost:35000 ). Of course if the pipeline \ntag explicitly contains a different repository, it will be used instead.  The local repository can be stopped with:  >>>   session . stop_repository_container () \n     [ INFO ]   Cebes   container   stopped :   cebes - pipeline - repository - repo - x . x . x - 0 [ cebes : x . x . x ]   at   port   35000",
            "title": "Start a local repository"
        },
        {
            "location": "/pipelines_repo/#how-cebes-server-determines-which-repository-to-use",
            "text": "The repository to which pipelines are pushed or pulled can be confusing. In general, Cebes decides \nthe repository to use in the following order:   The repository specified in the tag,  The local repository, if it is started in the session,  The server-wide default repository configured in Cebes server.",
            "title": "How Cebes server determines which repository to use"
        },
        {
            "location": "/pipelines_deploy/",
            "text": "This section describes how to deploy a Cebes repository. You only need this if you want \nto setup a private repository that can be shared between multiple users in your organization.\n\n\nUse a docker image\n\u00b6\n\n\nThe easiest way to deploy a Cebes repository is to use a Docker image. We provide images that \nhas Cebes repository with and without a MariaDB database for the ease of integration. \nThe image with MariaDB should work out-of-the-box, while for the image without MariaDB you will\nneed to provide some environment variables that specify the database to be used. Check out \n\nDocker hub\n for more information.\n\n\nIf you have a CI/CD system, deploying the docker image should be straightforward.\n\n\nDeploy manually\n\u00b6\n\n\n\n\nDownload \ncebes-pipeline-repository-assembly-VERSION.jar\n from \n\ngithub release page\n\n\nSpecify all environment variables as in the table below.\n\n\nStart the repository:\n$ java -Dcebes.logs.dir=/tmp/ -jar cebes-pipeline-repository-assembly-VERSION.jar\n\n\n\n\n\n\n\n\n\nThe repository is a normal web app that doesn't run on Spark, therefore configuration \nis fairly straightforward:\n\n\n\n\n\n\n\n\nEnvironment variable name\n\n\nConfiguration key\n\n\nDescription\n\n\nDefault value\n\n\n\n\n\n\n\n\n\n\nCEBES_MYSQL_URL\n\n\ncebes.mysql.url\n\n\nURL for MySQL database\n\n\n\n\n\n\n\n\nCEBES_MYSQL_DRIVER\n\n\ncebes.mysql.driver\n\n\nDriver for MySQL database\n\n\norg.mariadb.jdbc.Driver\n\n\n\n\n\n\nCEBES_MYSQL_USERNAME\n\n\ncebes.mysql.username\n\n\nUsername for MySQL database\n\n\n\n\n\n\n\n\nCEBES_MYSQL_PASSWORD\n\n\ncebes.mysql.password\n\n\nPassword for MySQL database\n\n\n\n\n\n\n\n\nCEBES_REPOSITORY_INTERFACE\n\n\ncebes.repository.interface\n\n\nThe interface on which the HTTP service will be listening\n\n\nlocalhost\n\n\n\n\n\n\nCEBES_REPOSITORY_PORT\n\n\ncebes.repository.port\n\n\nThe port on which the HTTP service will be listening, to be combined with REPOSITORY_INTERFACE\n\n\n22000\n\n\n\n\n\n\nCEBES_REPOSITORY_SERVER_SECRET\n\n\ncebes.repository.server.secret\n\n\nThe secret string to be used in authentication of the HTTP server\n\n\n...\n\n\n\n\n\n\nCEBES_REPOSITORY_PATH\n\n\ncebes.repository.path\n\n\nPath to local disk where we store the binary files of repositories\n\n\n/tmp\n\n\n\n\n\n\n\n\nAn example bash script can be found under \ndocker/repository/start_cebes_repository.sh\n on github.",
            "title": "Install pipeline repository"
        },
        {
            "location": "/pipelines_deploy/#use-a-docker-image",
            "text": "The easiest way to deploy a Cebes repository is to use a Docker image. We provide images that \nhas Cebes repository with and without a MariaDB database for the ease of integration. \nThe image with MariaDB should work out-of-the-box, while for the image without MariaDB you will\nneed to provide some environment variables that specify the database to be used. Check out  Docker hub  for more information.  If you have a CI/CD system, deploying the docker image should be straightforward.",
            "title": "Use a docker image"
        },
        {
            "location": "/pipelines_deploy/#deploy-manually",
            "text": "Download  cebes-pipeline-repository-assembly-VERSION.jar  from  github release page  Specify all environment variables as in the table below.  Start the repository: $ java -Dcebes.logs.dir=/tmp/ -jar cebes-pipeline-repository-assembly-VERSION.jar    The repository is a normal web app that doesn't run on Spark, therefore configuration \nis fairly straightforward:     Environment variable name  Configuration key  Description  Default value      CEBES_MYSQL_URL  cebes.mysql.url  URL for MySQL database     CEBES_MYSQL_DRIVER  cebes.mysql.driver  Driver for MySQL database  org.mariadb.jdbc.Driver    CEBES_MYSQL_USERNAME  cebes.mysql.username  Username for MySQL database     CEBES_MYSQL_PASSWORD  cebes.mysql.password  Password for MySQL database     CEBES_REPOSITORY_INTERFACE  cebes.repository.interface  The interface on which the HTTP service will be listening  localhost    CEBES_REPOSITORY_PORT  cebes.repository.port  The port on which the HTTP service will be listening, to be combined with REPOSITORY_INTERFACE  22000    CEBES_REPOSITORY_SERVER_SECRET  cebes.repository.server.secret  The secret string to be used in authentication of the HTTP server  ...    CEBES_REPOSITORY_PATH  cebes.repository.path  Path to local disk where we store the binary files of repositories  /tmp     An example bash script can be found under  docker/repository/start_cebes_repository.sh  on github.",
            "title": "Deploy manually"
        },
        {
            "location": "/serving/",
            "text": "Once trained, Cebes pipelines should be \npushed to a repository\n so that they are\nready to be used in production serving. By \nserving\n, we mean to use the trained pipeline to answer\nonline queries, usually through HTTP requests or message queues. Those requests are typically small \n(a few data samples), and the results can be retrieved synchronously or asynchronously. For running \nPipelines on big datasets, we recommend to keep the pipelines in Cebes server, where a beefy Spark \ncluster is used.\n\n\nThe following figure should explain the relationship between Cebes server, repository and the serving \ncomponent.\n\n\n\n\n\n\nDeploy Cebes serving with Docker\n\u00b6\n\n\nSimilar to other components, Cebes serving is also available as Docker images. These images package\nCebes serving running on Spark in local mode, and with/without MariaDB. The images that has MariaDB\nshould work out-of-the-box, while the images without MariaDB will need some additional configuration.\nCheck \ndocker hub\n for more information.\n\n\nTechnically, Cebes serving is a web app written in Scala that runs on Spark and the core Cebes engine.\nThis web app answers HTTP requests synchronously or asynchronously, and serve certain number of pipelines\nthat are specified in its \nconfiguration file\n.\n\n\nSince Cebes serving is designed to answer online queries of typically a few data samples, it runs on \nSpark in local mode. Scalability can be achieved by having multiple instances of Cebes serving behind \na load balancer. Many Cebes components were designed to be compatible with scalable deployment systems\nsuch as \nKubernetes\n and friends. \nTalk to us if you need help in designing and building such a system!\n\n\n\n\n\n\nConfigure Cebes serving\n\u00b6\n\n\nCebes serving is a generic web app that can serve any number of pipelines. When deploying Cebes serving,\nyou specify the pipelines you want it to serve in a configuration file, along with other properties.\nA sample configuration file is shown below:\n\n\n{\n\n  \n\"pipelines\"\n:\n \n[\n\n    \n{\n\n      \n\"servingName\"\n:\n \n\"anomaly-detect\"\n,\n\n      \n\"slotNamings\"\n:\n \n{\n\n        \n\"s1:inputDf\"\n:\n \n\"input-data\"\n,\n\n        \n\"s5:outputDf\"\n:\n \n\"result\"\n\n      \n},\n\n      \n\"pipelineTag\"\n:\n \n\"repo.company.net/bob/anomaly-detection:v2\"\n,\n\n      \n\"userName\"\n:\n \n\"\"\n,\n\n      \n\"password\"\n:\n \n\"\"\n\n    \n}\n\n  \n],\n\n  \n\"secured\"\n:\n \nfalse\n,\n\n  \n\"httpInterface\"\n:\n \n\"0.0.0.0\"\n,\n\n  \n\"httpPort\"\n:\n \n23000\n\n\n}\n\n\n\n\n\n\nIn this configuration file, you specify the list of served pipelines in the \npipelines\n list:\n\n\n\n\nservingName\n is an easy-to-remember name for the pipeline that the end-users can use when they\nsend requests for this pipeline\n\n\nslotNamings\n is an easy way to renaming slots in this pipeline. For example in this pipeline,\nthe \ninputDf\n slot of stage \ns1\n is renamed to \ninput-data\n, and slot \noutputDf\n of stage \ns5\n is \nrenamed to \nresult\n. When the end-users send requests, they can provide input data into \ninput-data\n \nand get the results from \nresult\n, without knowing the specific slots they are dealing with.\n\nslotNamings\n is therefore a tool for you, as system administrators, to fine-tune how the pipelines\nwill be served in production.\n\n\npipelineTag\n is the tag of the pipeline to be served. \nuserName\n and \npassword\n, if provided,\nare the credentials to be used to login to the repository to retrieve the pipeline.\n\n\n\n\n\n\nCebes serving API\n\u00b6\n\n\nOnce started, Cebes serving exposes two endpoints \n/inference\n and \n/inferencesync\n to receive\nHTTP requests, along with some other authentication endpoints if it is deployed in \nsecured\n mode.\n\n\nThe \n/inference\n and \n/inferencesync\n endpoints are for asynchronous and synchronous inference, respectively.\nA request sent to \n/inferencesync\n will block and return the inference results when it is ready.\nA request sent to \n/inference\n will return immediately a job ID that can be used to periodically \ncheck for the result.\n\n\nThe body of the inference request is a JSON object of the following fields:\n\n\n{\n\n  \n\"servingName\"\n:\n \n\"anomaly-detect\"\n,\n\n  \n\"inputs\"\n:\n \n{\n\n    \n\"input-data\"\n:\n \n{\n\n      \n\"data\"\n:\n \n[\n\n        \n{\n\n          \n\"viscosity\"\n:\n \n0.1\n,\n \n          \n\"proof_cut\"\n:\n \n2\n\n        \n},\n\n        \n{\n\n          \n\"viscosity\"\n:\n \n4.5\n,\n \n          \n\"proof_cut\"\n:\n \n100\n\n        \n}\n\n      \n]\n\n    \n}\n \n  \n},\n\n  \n\"outputs\"\n:\n \n[\n\n    \n\"result\"\n\n  \n],\n\n  \n\"maxDfSize\"\n:\n \n2000\n\n\n}\n\n\n\n\n\n\n\n\nservingName\n is the serving name to be requested\n\n\ninputs\n is a map from slot namings (see previous section on \nslotNamings\n) to data.\nUsers can feed any data, as long as their types, after JSON deserialization, is compatible\nto the type of the slot. Here we give the \ninput-data\n slot a sample dataframe of 2 rows,\nwhere \nviscosity\n and \nproof_cut\n are the 2 column names. This will be deserialized by \nCebes serving and feed into the \ninput-data\n slot.\n\n\noutputs\n is the list of slot namings to retrieved results. Users can request\nresults from any slot belonging to the served pipeline, regardless of their types.\n\n\nmaxDfSize\n is an optional configuration. If the result is a Dataframe and it has more than \nthis number of rows, then only \nmaxDfSize\n rows will be returned.\n\n\n\n\nA typical response for the above request is:\n\n\n{\n\n  \n\"outputs\"\n:\n \n{\n\n    \n\"result\"\n:\n \n{\n\n      \n\"schema\"\n:\n \n[\n...\n],\n\n      \n\"data\"\n:\n \n[\n\n        \n{\n\n          \n...\n\n        \n}\n\n      \n]\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThis is simply a map from output slots (that the end-users requested) to their corresponding values.\nSince the \nresult\n slot gives a Dataframe, the response will contain a schema of that dataframe,\nand its rows.\n\n\nSee the tests in Cebes codebase for more examples.\n\n\n\n\nServing via message queues\n\u00b6\n\n\nIn practice, it is more often to serve your requests coming in from a message queue, \ne.g.\n Apache Kafka.\nSupporting message queue in Cebes serving is in the work. Talk to us about your usecase if you are interested!",
            "title": "Serving pipelines"
        },
        {
            "location": "/serving/#deploy-cebes-serving-with-docker",
            "text": "Similar to other components, Cebes serving is also available as Docker images. These images package\nCebes serving running on Spark in local mode, and with/without MariaDB. The images that has MariaDB\nshould work out-of-the-box, while the images without MariaDB will need some additional configuration.\nCheck  docker hub  for more information.  Technically, Cebes serving is a web app written in Scala that runs on Spark and the core Cebes engine.\nThis web app answers HTTP requests synchronously or asynchronously, and serve certain number of pipelines\nthat are specified in its  configuration file .  Since Cebes serving is designed to answer online queries of typically a few data samples, it runs on \nSpark in local mode. Scalability can be achieved by having multiple instances of Cebes serving behind \na load balancer. Many Cebes components were designed to be compatible with scalable deployment systems\nsuch as  Kubernetes  and friends. \nTalk to us if you need help in designing and building such a system!",
            "title": "Deploy Cebes serving with Docker"
        },
        {
            "location": "/serving/#configure-cebes-serving",
            "text": "Cebes serving is a generic web app that can serve any number of pipelines. When deploying Cebes serving,\nyou specify the pipelines you want it to serve in a configuration file, along with other properties.\nA sample configuration file is shown below:  { \n   \"pipelines\" :   [ \n     { \n       \"servingName\" :   \"anomaly-detect\" , \n       \"slotNamings\" :   { \n         \"s1:inputDf\" :   \"input-data\" , \n         \"s5:outputDf\" :   \"result\" \n       }, \n       \"pipelineTag\" :   \"repo.company.net/bob/anomaly-detection:v2\" , \n       \"userName\" :   \"\" , \n       \"password\" :   \"\" \n     } \n   ], \n   \"secured\" :   false , \n   \"httpInterface\" :   \"0.0.0.0\" , \n   \"httpPort\" :   23000  }   In this configuration file, you specify the list of served pipelines in the  pipelines  list:   servingName  is an easy-to-remember name for the pipeline that the end-users can use when they\nsend requests for this pipeline  slotNamings  is an easy way to renaming slots in this pipeline. For example in this pipeline,\nthe  inputDf  slot of stage  s1  is renamed to  input-data , and slot  outputDf  of stage  s5  is \nrenamed to  result . When the end-users send requests, they can provide input data into  input-data  \nand get the results from  result , without knowing the specific slots they are dealing with. slotNamings  is therefore a tool for you, as system administrators, to fine-tune how the pipelines\nwill be served in production.  pipelineTag  is the tag of the pipeline to be served.  userName  and  password , if provided,\nare the credentials to be used to login to the repository to retrieve the pipeline.",
            "title": "Configure Cebes serving"
        },
        {
            "location": "/serving/#cebes-serving-api",
            "text": "Once started, Cebes serving exposes two endpoints  /inference  and  /inferencesync  to receive\nHTTP requests, along with some other authentication endpoints if it is deployed in  secured  mode.  The  /inference  and  /inferencesync  endpoints are for asynchronous and synchronous inference, respectively.\nA request sent to  /inferencesync  will block and return the inference results when it is ready.\nA request sent to  /inference  will return immediately a job ID that can be used to periodically \ncheck for the result.  The body of the inference request is a JSON object of the following fields:  { \n   \"servingName\" :   \"anomaly-detect\" , \n   \"inputs\" :   { \n     \"input-data\" :   { \n       \"data\" :   [ \n         { \n           \"viscosity\" :   0.1 ,  \n           \"proof_cut\" :   2 \n         }, \n         { \n           \"viscosity\" :   4.5 ,  \n           \"proof_cut\" :   100 \n         } \n       ] \n     }  \n   }, \n   \"outputs\" :   [ \n     \"result\" \n   ], \n   \"maxDfSize\" :   2000  }    servingName  is the serving name to be requested  inputs  is a map from slot namings (see previous section on  slotNamings ) to data.\nUsers can feed any data, as long as their types, after JSON deserialization, is compatible\nto the type of the slot. Here we give the  input-data  slot a sample dataframe of 2 rows,\nwhere  viscosity  and  proof_cut  are the 2 column names. This will be deserialized by \nCebes serving and feed into the  input-data  slot.  outputs  is the list of slot namings to retrieved results. Users can request\nresults from any slot belonging to the served pipeline, regardless of their types.  maxDfSize  is an optional configuration. If the result is a Dataframe and it has more than \nthis number of rows, then only  maxDfSize  rows will be returned.   A typical response for the above request is:  { \n   \"outputs\" :   { \n     \"result\" :   { \n       \"schema\" :   [ ... ], \n       \"data\" :   [ \n         { \n           ... \n         } \n       ] \n     } \n   }  }   This is simply a map from output slots (that the end-users requested) to their corresponding values.\nSince the  result  slot gives a Dataframe, the response will contain a schema of that dataframe,\nand its rows.  See the tests in Cebes codebase for more examples.",
            "title": "Cebes serving API"
        },
        {
            "location": "/serving/#serving-via-message-queues",
            "text": "In practice, it is more often to serve your requests coming in from a message queue,  e.g.  Apache Kafka.\nSupporting message queue in Cebes serving is in the work. Talk to us about your usecase if you are interested!",
            "title": "Serving via message queues"
        }
    ]
}